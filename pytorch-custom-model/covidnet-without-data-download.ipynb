{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"-   <a href=\"#summary\" id=\"toc-summary\">Summary</a>\n    -   <a href=\"#overview-and-explanation\"\n        id=\"toc-overview-and-explanation\">Overview and Explanation</a>\n-   <a href=\"#import-and-deterministic-setup\"\n    id=\"toc-import-and-deterministic-setup\">Import and Deterministic\n    Setup</a>\n-   <a href=\"#data-preprocessing\" id=\"toc-data-preprocessing\">Data\n    Preprocessing</a>\n    -   <a href=\"#custom-dataset-class-to-load-covidnet-data\"\n        id=\"toc-custom-dataset-class-to-load-covidnet-data\">Custom Dataset class\n        to load CovidNet data</a>\n    -   <a href=\"#spliting-dataset-into-train-val-test\"\n        id=\"toc-spliting-dataset-into-train-val-test\">Spliting dataset into\n        train, val, test</a>\n    -   <a href=\"#applying-transforms-to-dataset\"\n        id=\"toc-applying-transforms-to-dataset\">Applying transforms to\n        dataset</a>\n        -   <a href=\"#define-a-wrapper-dataset\"\n            id=\"toc-define-a-wrapper-dataset\">Define a wrapper dataset</a>\n        -   <a href=\"#defining-the-transforms\"\n            id=\"toc-defining-the-transforms\">Defining the transforms</a>\n        -   <a href=\"#creating-the-dataset-loader\"\n            id=\"toc-creating-the-dataset-loader\">Creating the Dataset Loader</a>\n-   <a href=\"#custom-model\" id=\"toc-custom-model\">Custom Model</a>\n    -   <a href=\"#references\" id=\"toc-references\">References</a>\n        -   <a href=\"#links\" id=\"toc-links\">Links</a>\n    -   <a href=\"#first-try\" id=\"toc-first-try\">First try</a>\n    -   <a href=\"#full-model\" id=\"toc-full-model\">Full Model</a>\n-   <a href=\"#training-validation\" id=\"toc-training-validation\">Training\n    &amp; Validation</a>\n-   <a href=\"#test-evaluation\" id=\"toc-test-evaluation\">Test\n    (Evaluation)</a>\n-   <a href=\"#playground\" id=\"toc-playground\">Playground</a>\n    -   <a href=\"#visualizing-models-using-pytorchviz\"\n        id=\"toc-visualizing-models-using-pytorchviz\">Visualizing Models using\n        PytorchViz</a>\n    -   <a href=\"#investigate-image-in-dataset\"\n        id=\"toc-investigate-image-in-dataset\">Investigate Image in Dataset</a>\n    -   <a href=\"#investigate-metadata.csv\"\n        id=\"toc-investigate-metadata.csv\">Investigate metadata.csv</a>\n    -   <a href=\"#investigating-split-dataset\"\n        id=\"toc-investigating-split-dataset\">Investigating split dataset</a>\n    -   <a href=\"#visualizing-dataloader\"\n        id=\"toc-visualizing-dataloader\">Visualizing Dataloader</a>\n    -   <a href=\"#trying-avalance\" id=\"toc-trying-avalance\">Trying Avalance</a>\n    -   <a href=\"#pretrained-model\" id=\"toc-pretrained-model\">Pretrained\n        Model</a>\n        -   <a href=\"#helper-functions\" id=\"toc-helper-functions\">Helper\n            Functions</a>\n        -   <a href=\"#define-function-to-initialize-deep-learning-models\"\n            id=\"toc-define-function-to-initialize-deep-learning-models\">Define\n            Function to Initialize Deep Learning Models</a>\n        -   <a href=\"#define-function-to-train-models\"\n            id=\"toc-define-function-to-train-models\">Define Function to Train\n            Models</a>\n        -   <a href=\"#define-functions-to-visualize-prediction\"\n            id=\"toc-define-functions-to-visualize-prediction\">Define Functions to\n            Visualize Prediction</a>\n        -   <a href=\"#run-all-models---init-models-training\"\n            id=\"toc-run-all-models---init-models-training\">Run all models - Init\n            Models + Training</a>\n    -   <a href=\"#out-of-memory-issue\" id=\"out-of-memory-issue\">Out of memory issue</a>","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Summary\n\n------------------------------------------------------------------------\n\n> Expand to see summary and details","metadata":{}},{"cell_type":"markdown","source":"## Overview and Explanation\n\n1.  This notebook reuses a lot of the [original transfer learning\n    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n    -   Here the focus is on building the new custom model using the\n        CovidNet-CT database.\n2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n    -   is not longer needed for notebook running in kaggle. Remained\n        here for references only\n    -   is where the dataset is being acquired.\n    -   Explanation of various phases in the [CovidNet-CT ML\n        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n        -   train phase is train phase\n        -   test phase is validation phase\n        -   infer phase is test phase\n3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n    -   refers to the way [CovidNet-CT preprocess its\n        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n        to use PyTorch\n    -   Two highlights\n        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n            used by the imagenet model\n        -   the image is cropped to the bounding box provided with the\n            dataset before resize to 512x512\n4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n    -   refers to [how CovidNet-CT\n        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n    -   This part is almost identical to the original transfer learning\n        model notebook.","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Import and Deterministic Setup\n\n------------------------------------------------------------------------\n\nAll modules will be imported here including modules used in the\n[Playground](#playground) section","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\n%env CUDA_LAUNCH_BLOCKING=1\nimport random\nimport numpy as np\nimport torch\n\nfrom os import path\nimport math\nimport sys\n\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\n\nimport torchvision\nfrom torchvision import models, transforms #, datasets\nimport matplotlib.pyplot as plt\n\nfrom torch import optim\nfrom torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Linear, CrossEntropyLoss\nfrom torch.optim import lr_scheduler\nimport time\nimport torch.nn.functional as F\n\nimport gc\nimport pandas as pd\nimport enum\nfrom enum import Enum\n\n# ensure reproducibility across different executions\n# https://pytorch.org/docs/stable/notes/randomness.html\n# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\nSEED = 18\ndef seed_everything(seed=18):\n    random.seed(seed)\n    %env PYTHONHASHSEED=$seed\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n#torch.set_deterministic(True)\ntorch.use_deterministic_algorithms(True)\n%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n        \n# https://www.kaggle.com/code/manabendrarout/vision-transformer-vit-pytorch-on-tpus-train/notebook\ndef is_tpu_avail():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        TPU_DETECTED = True\n    except:\n        pass\n\n    return TPU_DETECTED\n\n\ndef is_gpu_avail():\n    GPU_DETECTED = False\n    try:\n        GPU_DETECTED = torch.cuda.is_available()\n    except:\n        pass\n\n    return GPU_DETECTED","metadata":{"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","execution":{"iopub.status.busy":"2023-01-29T08:10:33.740570Z","iopub.execute_input":"2023-01-29T08:10:33.740915Z","iopub.status.idle":"2023-01-29T08:10:36.851748Z","shell.execute_reply.started":"2023-01-29T08:10:33.740816Z","shell.execute_reply":"2023-01-29T08:10:36.850643Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: CUDA_LAUNCH_BLOCKING=1\nenv: CUBLAS_WORKSPACE_CONFIG=:4096:8\nenv: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Data Preprocessing\n\n------------------------------------------------------------------------\n\n-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset class to load CovidNet data\n\n- Various references used: \n  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10","metadata":{}},{"cell_type":"code","source":"class CovidNetDataset(Dataset):\n    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n        # don't seem to need the csv file\n        # self.df = pd.read_csv(csv_path)\n        # _, self.class_to_idx  = self.find_classes(csv_path);\n\n        self.img_dir = img_dir\n        self.split_files = split_files\n        \n        self.size = 0\n        self.limit_size = limit_size\n        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n        self.stradify_removal_based_on_limit()\n        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, index):\n        # filename = self.df[index, \"FILENAME\"]\n        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n        # image = Image.open(os.path.join(self.img_dir, filename))\n\n        label = self.targets[index]\n        with open(self.imgs[index], \"rb\") as f:\n            image = Image.open(f)\n            image = image.crop(self.bboxes[index])\n            image = image.copy()\n\n        if self.transform is not None:\n            image = self.transform(image)\n    \n\n        return image, label\n\n    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n    def find_classes(self, csv_path=None):\n        \"\"\"Returns class name array and class_to_idx.\n        See :class:`CovidNetDataset` for details.\n        \"\"\"\n        # class_col = \"finding\"\n        # classes = sorted(self.df[class_col].unique())\n        # if not classes:\n        #     raise FileNotFoundError(f\"Couldn't find any class from '{class_col}' column in {csv_path}.\")\n\n        # hard code classes as the order are not alphabetic\n        classes = ['Normal', 'Pneumonia', 'COVID-19']\n\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n        return classes, class_to_idx\n\n    def get_all_split_file_data(self):\n        files, classes, bboxes = [], [], []\n        for split_file in self.split_files:\n            f, cls, bb = self.get_data_from_split_file(split_file)\n            files.extend(f)\n            classes.extend(cls)\n            bboxes.extend(bb)\n        return files, classes, bboxes\n\n    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n    def get_data_from_split_file(self, split_file):\n        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n        files, classes, bboxes = [], [], []\n        with open(split_file, 'r') as f:\n            for line in f.readlines():\n                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n                files.append(path.join(self.img_dir, fname))\n                classes.append(int(cls))\n                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n                self.size += 1\n        return files, classes, bboxes\n    \n    '''Try to do stratified removal based on limit count if it is specified'''\n    def stradify_removal_based_on_limit(self):\n        _, class_to_idx = self.find_classes()\n        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n            return\n        \n        total_remove_count = self.size - self.limit_size\n        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n        for idx, count in occurrence.items():\n            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n        \n        print(occurrence)\n        print(target_remove_count)\n        \n        for i in reversed(range(len(self.targets))):\n            idx = self.targets[i]\n            if target_remove_count[idx] > 0:\n                del self.targets[i]\n                del self.imgs[i]\n                del self.bboxes[i]\n                target_remove_count[idx] -= 1\n                self.size -= 1","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:36.856451Z","iopub.execute_input":"2023-01-29T08:10:36.856992Z","iopub.status.idle":"2023-01-29T08:10:36.885561Z","shell.execute_reply.started":"2023-01-29T08:10:36.856954Z","shell.execute_reply":"2023-01-29T08:10:36.884605Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Spliting dataset into train, val, test\n\n-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nCURR_DIR = \"/kaggle/working\"\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nMAX_COUNT = 10000\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_COUNT)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\n_, _, _, _, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\n# dataset size\ntrain_data_size = len(train_index)\nvalid_data_size = len(val_index)\ntest_data_size = len(test_index)\n\nprint(f\"First 10 train_index: {train_index[:10]}\")\nprint(f\"length of train_index: {train_data_size}\\n\")\nprint(f\"First 10 val_index: {val_index[:10]}\")\nprint(f\"length of val_index: {valid_data_size}\\n\")\nprint(f\"First 10 test_index: {test_index[:10]}\")\nprint(f\"length of test_index: {test_data_size}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:36.890342Z","iopub.execute_input":"2023-01-29T08:10:36.893232Z","iopub.status.idle":"2023-01-29T08:10:39.913130Z","shell.execute_reply.started":"2023-01-29T08:10:36.893194Z","shell.execute_reply":"2023-01-29T08:10:39.911922Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"{0: 71488, 1: 42943, 2: 310593}\n{0: 69806, 1: 41932, 2: 303285}\nLength of full dataset: 10001\nFirst 10 train_index: [6373 6939 9338  105 7573 4773 9125 4210 4499 1754]\nlength of train_index: 7999\n\nFirst 10 val_index: [6942 5767  536 6017 6502 1373 8798 3052 4665 9964]\nlength of val_index: 1001\n\nFirst 10 test_index: [9022 9017 9868 1724  795 4754 4568 1985 9612 9534]\nlength of test_index: 1001\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Applying transforms to dataset","metadata":{}},{"cell_type":"markdown","source":"### Define a wrapper dataset\n\n- This is to have the flexibility of applying different transforms to each of the splitted dataset \n- References \n    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)","metadata":{}},{"cell_type":"code","source":"class WrapperDataset:\n    def __init__(self, dataset, transform=None, target_transform=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        image, label = self.dataset[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:39.915614Z","iopub.execute_input":"2023-01-29T08:10:39.915975Z","iopub.status.idle":"2023-01-29T08:10:39.932035Z","shell.execute_reply.started":"2023-01-29T08:10:39.915940Z","shell.execute_reply":"2023-01-29T08:10:39.929810Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Defining the transforms\n\n- References for mean and std of images \n    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n    - Bounding box causing issue when batching as stacking donâ€™t work with\n    different size \n        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`","metadata":{}},{"cell_type":"code","source":"covidnet_std_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n])\n\ncovidnet_train_transform = transforms.Compose([\n    transforms.RandomChoice(transforms=[\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=.3, hue=.3),\n        transforms.RandomPerspective(distortion_scale=0.4),\n        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n    ])\n\nimage_transforms = {\n    'train': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ]),\n    'val': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'test': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'playground': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:39.933385Z","iopub.execute_input":"2023-01-29T08:10:39.933728Z","iopub.status.idle":"2023-01-29T08:10:39.951077Z","shell.execute_reply.started":"2023-01-29T08:10:39.933694Z","shell.execute_reply":"2023-01-29T08:10:39.949560Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Dataset Loader","metadata":{}},{"cell_type":"code","source":"seed_everything(SEED)\nBATCH_SIZE = 32\n\ntrain_sampler = SubsetRandomSampler(train_index)\nval_sampler = SubsetRandomSampler(val_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\ntest_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n\nclass_names, class_to_idx = full_dataset.find_classes()\nprint(class_names)\nprint(class_to_idx)\n\nif is_tpu_avail():\n    device = 'TPU'\nelif is_gpu_avail():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\nprint(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:39.953170Z","iopub.execute_input":"2023-01-29T08:10:39.953486Z","iopub.status.idle":"2023-01-29T08:10:40.084139Z","shell.execute_reply.started":"2023-01-29T08:10:39.953454Z","shell.execute_reply":"2023-01-29T08:10:40.082165Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"env: PYTHONHASHSEED=18\n['Normal', 'Pneumonia', 'COVID-19']\n{'Normal': 0, 'Pneumonia': 1, 'COVID-19': 2}\nUsing device: cuda\ntrain size:7999; validation size:1001; test size:1001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Custom Model\n\n------------------------------------------------------------------------\n\n- The design of this custom model is illustrated in a draw.io diagram \n    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"### Links\n\n-   [10 CNN Architecture\n    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n    -   [Visualizing pytorch\n        models](https://github.com/szagoruyko/pytorchviz)\n-   Main model building references\n    -   The [CT-3A github\n        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n        -   [tensorflow pretrained\n            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n        -   How to [convert tensorflow checkpoints into pytorch\n            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n            -   [pytorch\n                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n        -   [pytorch\n            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n            about combining two existing models\n    -   [Pytorch resnext50\n        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n    -   [pytorch beginner tutorial on building\n        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n-   Other model building references\n    -   [Custom\n        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n    -   [Resnest convolution block\n        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram","metadata":{}},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"# modified from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L46\nclass Conv2d_BN(Module):\n    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, groups=1, acti=True):\n        super().__init__() # same as super(Conv2d_BN, self).__init__()\n        if acti:\n            self.conv2d_bn = Sequential(\n                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n                BatchNorm2d(out_channels),\n                LeakyReLU(0.2, inplace=True)\n            )\n        else:\n            self.conv2d_bn = Sequential(\n                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n                BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        return self.conv2d_bn(x)\n\n    def out_channels(self):\n      return next(self.conv2d_bn.children()).out_channels\n\n\n# Taken from https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py#L63\nclass StemBlock(Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        # For simplicity Sequential module can be used here, explicitly name every layer for practise and readibility\n        self.conv1 = Conv2d_BN(in_channels, out_channels=64, kernel=7, stride=2, padding=3)\n        self.maxpool1 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n        self.conv2 = Conv2d_BN(in_channels=self.conv1.out_channels(), out_channels=80, kernel=1)\n        self.conv3 = Conv2d_BN(in_channels=self.conv2.out_channels(), out_channels=192, kernel=3, padding=1)\n        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n\n    def forward(self, x):\n        # N x 3 x 224 x 224\n        x = self.conv1(x)\n        # N x 64 x 112 x 112\n        x = self.maxpool1(x)\n        # N x 64 x 56 x 56\n        x = self.conv2(x)\n        # N x 80 x 56 x 56\n        x = self.conv3(x)\n        # N x 192 x 56 x 56\n        return self.maxpool2(x)\n        # N x 192 x 28 x 28\n\n    def out_channels(self):\n      # unable to access output size of MaxPool2d, use hard-coded formula instead\n      return math.floor(self.conv3.out_channels()/2)\n\n\n# https://stackoverflow.com/questions/4950155/objects-as-keys-in-python-dictionaries\nclass BlockType(Enum):\n    CONV = enum.auto()\n    IDENTITY = enum.auto()\n\n    def __eq__(self, other):\n        return self.name == other.name and self.value == other.value\n\n    def __hash__(self):\n        return hash(f\"{self.name}:{self.value}\")\n\n# Generalize ConvBlock and Identity block as ResidualBlock:\n# https://github.com/maciejbalawejder/Deep-Learning-Collection/blob/main/ConvNets/ResNeXt/resnext_pytorch.py\nclass ResidualBlock(Module):\n    def __init__(self, in_channels, out_channels, block_type: BlockType, stride, cardinatlity=32):\n        super().__init__()\n        assert out_channels % 32 == 0\n        self.C = cardinatlity\n        self.block_type = block_type\n        inner_channels = out_channels // 2\n        self.conv_tower = Sequential(\n            Conv2d_BN(in_channels, inner_channels, kernel=1),\n            Conv2d_BN(inner_channels, inner_channels, kernel=3, stride=stride, padding=1, groups=self.C),\n            Conv2d_BN(inner_channels, out_channels, kernel=1, acti=None)\n        )\n        if self.block_type is BlockType.CONV:\n            self.downsample = Conv2d_BN(in_channels, out_channels, kernel=1, stride=stride, acti=None)\n        self.relu = LeakyReLU(0.2, inplace=True)\n\n    def forward(self, x):\n        out = self.conv_tower(x)\n        if self.block_type is BlockType.CONV:\n            x = self.downsample(x)\n        out = self.relu(torch.add(out,x))\n        return out\n\n    def out_channels(self):\n      # unable to access output size of MaxPool2d, use hard-coded formula instead\n      gen = self.conv_tower.children()\n      last = next(gen)\n      for last in gen: pass\n      return last.out_channels()\n\n\n# taken from https://github.com/reppertj/earworm/blob/a2d8a70085748da5db378f7f5f68ad8c2926a274/modeling/music_metric_learning/modules/inception.py#L93\nclass ReductionBlock(Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # previously mistakenly thought conv3_pooling_out is this layer out_channel, turn out that the out_channel is the total channels of the 3 layers\n        # assert (out_channels * 8 // 3) % 16 == 0\n        # conv5_out = out_channels * 2 // 3\n        # conv3_pooling_out = out_channels\n\n        assert out_channels % 16 == 0\n        conv5_out = out_channels // 4\n        conv3_pooling_out = conv5_out * 3 // 2\n        self.conv3 = Conv2d_BN(in_channels, conv3_pooling_out, kernel=3, stride=2)\n        self.conv5 = Sequential(\n            Conv2d_BN(in_channels       , conv5_out * 3 // 4, kernel=1),\n            Conv2d_BN(conv5_out * 3 // 4, conv5_out * 7 // 8, kernel=3, padding=1),\n            Conv2d_BN(conv5_out * 7 // 8, conv5_out         , kernel=3, stride=2),\n        )\n        self.pooling = Sequential(\n            MaxPool2d(3, stride=2),\n            Conv2d_BN(in_channels, conv3_pooling_out, kernel=1),\n        )\n\n    def forward(self, x):\n        return torch.cat((self.conv3(x), self.conv5(x), self.pooling(x)), dim=1)\n\n\nIN_CHAN=\"in_chan\"\nOUT_CHAN=\"out_chan\"\nclass CovidNetBlock(Module):\n    def __init__(self, residual_block_layout: dict, out_channels):\n        super().__init__()\n        chan_dict_list = [l for k, v in residual_block_layout.items() for l in v if isinstance(k, BlockType) and isinstance(v, list) and isinstance(l, dict)]\n\n        print(f\"Creating CovidNetBlock with layout length of {len(chan_dict_list)}\")\n        self.blocks = Sequential()\n        for k,v in residual_block_layout.items():\n            for chan_dict in v:\n                # print(f\"DEBUG_LOG - creating ResidualBlock with in_chan:{chan_dict[IN_CHAN]}; out_chan:{chan_dict[OUT_CHAN]}; block_type:{k}\")\n                self.blocks.append(ResidualBlock(chan_dict[IN_CHAN], chan_dict[OUT_CHAN], k, 1))\n        last_out_chan = chan_dict_list[-1][OUT_CHAN] # get last layers output channels count\n        self.blocks.append(ReductionBlock(last_out_chan, out_channels))\n\n    def forward(self, x):\n        return self.blocks(x)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:40.085681Z","iopub.execute_input":"2023-01-29T08:10:40.086128Z","iopub.status.idle":"2023-01-29T08:10:40.156175Z","shell.execute_reply.started":"2023-01-29T08:10:40.086084Z","shell.execute_reply":"2023-01-29T08:10:40.155093Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Full Model","metadata":{}},{"cell_type":"code","source":"# https://onlinegdb.com/t9CIm197r\nclass CovidnetModel(Module):\n    def __init__(\n        self, \n        classes : int = 3,\n        ):\n        super().__init__()\n        \n        self.stem = StemBlock()\n\n        PRE_FC_OUT_CHAN = 192\n        residual_block_layout = {\n            BlockType.CONV:[\n                dict(in_chan=192, out_chan=256),\n                dict(in_chan=256, out_chan=512),\n                dict(in_chan=512, out_chan=1024),\n            ],\n            BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n        }\n        self.blocks = Sequential(\n            CovidNetBlock(residual_block_layout, 192),\n            CovidNetBlock(residual_block_layout, PRE_FC_OUT_CHAN)\n        )\n        \n        self.global_avg_pool = AdaptiveAvgPool2d((1,1))\n        self.fc = Linear(PRE_FC_OUT_CHAN, classes)\n\n    def forward(self, x):\n        # 3 x 224 x 224\n        x = self.stem(x)\n        # 192 x 28 x 28\n        x = self.blocks(x)\n        x = self.global_avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa","execution":{"iopub.status.busy":"2023-01-29T08:10:40.157417Z","iopub.execute_input":"2023-01-29T08:10:40.157740Z","iopub.status.idle":"2023-01-29T08:10:40.177375Z","shell.execute_reply.started":"2023-01-29T08:10:40.157708Z","shell.execute_reply":"2023-01-29T08:10:40.176296Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Training & Validation\n\n------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"LOG_DIR = os.path.join(CURR_DIR, \"log\")\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\ncurr_model = \"\"\n\ndef log_to_file(txt=None, print_to_console_only=False):\n  if txt is None:\n    txt = ''\n  txt += '\\n'\n  print(txt)\n  if print_to_console_only:\n    return\n  if not path.exists(LOG_DIR):\n    os.mkdir(LOG_DIR)\n  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n  with open(full_path, mode='a') as f:\n    f.write(txt)\n    \n# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n# Make sure to delete any references to tensor. Else this function will not have significant effect\ndef clean_vram():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\ndef name_of_caller(frame=1):\n    \"\"\"\n    Return \"class.function_name\" of the caller or just \"function_name\".\n    \"\"\"\n    frame = sys._getframe(frame)\n    fn_name = frame.f_code.co_name\n    var_names = frame.f_code.co_varnames\n    if var_names:\n        if var_names[0] == \"self\":\n            self_obj = frame.f_locals.get(\"self\")\n            if self_obj is not None:\n                return f\"{type(self_obj).__name__}.{fn_name}\" \n        if var_names[0] == \"cls\":\n            cls_obj = frame.f_locals.get(\"cls\")\n            if cls_obj is not None:\n                return f\"{cls_obj.__name__}.{fn_name}\"\n    return fn_name","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:40.178371Z","iopub.execute_input":"2023-01-29T08:10:40.178727Z","iopub.status.idle":"2023-01-29T08:10:40.195820Z","shell.execute_reply.started":"2023-01-29T08:10:40.178693Z","shell.execute_reply":"2023-01-29T08:10:40.194921Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Define Functions to Initialize Deep Learning Models","metadata":{}},{"cell_type":"code","source":"model_constructors = {\n  models.alexnet.__name__: models.alexnet, \n  models.squeezenet1_1.__name__: models.squeezenet1_1,\n  models.resnet50.__name__: models.resnet50, \n  models.resnet101.__name__: models.resnet101,\n  models.resnet152.__name__: models.resnet152, \n  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n  models.densenet201.__name__: models.densenet201, \n  models.googlenet.__name__: models.googlenet, \n  models.vgg16.__name__: models.vgg16, \n  models.vgg19.__name__: models.vgg19, \n  models.inception_v3.__name__: models.inception_v3, \n  CovidnetModel.__name__: CovidnetModel,\n}\n\n# This is only available in pytorch v0.13\n# from torchvision.models import *\n# model_weights = {\n#   models.alexnet.__name__: models.AlexNet_Weights.DEFAULT,\n#   models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n#   models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n#   models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n#   models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n#   models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n#   models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n#   models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n#   models.vgg16.__name__: VGG16_Weights.DEFAULT,\n#   models.vgg19.__name__: VGG19_Weights.DEFAULT,\n#   models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n# }\n\n# Experiment around dropout & Learning Rate & different optimizer (Adam)\ndef init_model(name):\n  if not path.exists(RESULT_DIR):\n    os.mkdir(RESULT_DIR)\n\n  clean_vram()\n  seed_everything()\n  \n  # fine-tune pretrain models to our usecase\n  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n  NUM_CLASSES = len(class_names)\n  DROPOUT = 0.5\n  if name == CovidnetModel.__name__:\n    model = model_constructors[name](NUM_CLASSES)\n  else:\n    model = model_constructors[name](True)\n    if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n      num_ftrs = model.classifier[6].in_features\n      model.classifier[6] = Linear(num_ftrs, NUM_CLASSES)\n      # model.classifier[6] = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n    elif name == models.densenet201.__name__:\n      num_ftrs = model.classifier.in_features\n      model.classifier = Linear(num_ftrs, NUM_CLASSES)\n      # model.classifier = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n    elif name == models.squeezenet1_1.__name__:\n      model.classifier = Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n      # model.classifier = Sequential(\n      #   Dropout(DROPOUT),\n      #   Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n      # )\n      model.num_classes = NUM_CLASSES\n    elif name == models.inception_v3.__name__:\n      auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n      model.AuxLogits.fc = Linear(auxLogits_num_ftrs, NUM_CLASSES)\n      # model.AuxLogits.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(auxLogits_num_ftrs, NUM_CLASSES)\n      # )\n      primary_num_ftrs = model.fc.in_features\n      model.fc = Linear(primary_num_ftrs, NUM_CLASSES)\n      # model.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(primary_num_ftrs, NUM_CLASSES)\n      # )\n    else:\n      # resnet, resnext & googlenet\n      num_ftrs = model.fc.in_features\n      model.fc= Linear(num_ftrs, NUM_CLASSES)\n      # model.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n\n  model = model.to(device)\n  criterion = CrossEntropyLoss()\n  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  if is_gpu_avail():\n    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n    scaler = torch.cuda.amp.GradScaler()\n  else:\n    raise RuntimeError('This code only support machine with GPU.')\n\n  # print('=====================================')\n  print(f'{name} is initialized')\n  # print('=====================================')\n  # print(model)\n  return model, criterion, optimizer, scaler\n\n# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\ndef save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n  torch.save({\n    'perf_metrics': perf_metrics,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    'history': history,\n    }, model_path)\n\ndef load_model(model, optimizer, scaler, model_path):\n  if not os.path.exists(model_path):\n    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n    return None, model, optimizer, scaler, None, None\n  checkpoint = torch.load(model_path)\n  perf_metrics = checkpoint['perf_metrics']\n  model.load_state_dict(checkpoint['model_state_dict'])\n  # model.load_state_dict(checkpoint['model_state_dict'], strict=False) # https://stackoverflow.com/questions/54058256/runtimeerror-errors-in-loading-state-dict-for-resnet\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n  history = checkpoint['history']\n  total_epoch = len(history) - 1\n  del checkpoint\n\n  return perf_metrics, model, optimizer, scaler, history, total_epoch","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:40.200210Z","iopub.execute_input":"2023-01-29T08:10:40.201912Z","iopub.status.idle":"2023-01-29T08:10:40.247788Z","shell.execute_reply.started":"2023-01-29T08:10:40.201855Z","shell.execute_reply":"2023-01-29T08:10:40.246921Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Define Function to Train Models","metadata":{}},{"cell_type":"code","source":"# training and validation loops\ndef train(model,\n    criterion,\n    optimizer,\n    scaler,\n    train_dataloader,\n    valid_dataloader,\n    model_path,\n    max_epochs_stop=10,\n    n_epochs=400,\n    min_epoch=300,\n    print_every=1):\n    \n    epochs_no_improve = 0\n    perf = {\n        'best_epoch': 0,\n        'valid_loss_min': np.Inf,\n        'valid_best_acc': 0,\n    }\n    total_epoch = 0\n\n    try:\n        if os.path.exists(model_path):\n            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n        else:\n            history = []\n            log_to_file(f'Starting Training from Scratch.\\n')\n    except:\n        history = []\n        log_to_file(f'exception: start from scratch.\\n')\n\n    overall_start = time.time()\n    if total_epoch >= n_epochs:\n        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n        history = pd.DataFrame(\n            history,\n            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n        return model, history, perf\n\n    seed_everything()\n\n    # Main loop - continue training on where we left off if there's a saved model\n    for epoch in range(total_epoch, n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = time.time()\n        for ii, (data, target) in enumerate (train_dataloader):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n            with torch.cuda.amp.autocast():\n              # output, aux_output = model(data)\n              # loss1 = criterion(output, target)\n              # loss2 = criterion(aux_output, target)\n              # loss = loss1 + 0.4*loss2\n              output = model(data)\n              loss = criterion(output, target)\n            # loss.backward()\n            # optimizer.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n            \n            # cleanup to save VRAM\n            del data, target\n#             clean_vram()\n\n        # After training loops ends, start validation\n        else:\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_dataloader:\n                    if is_gpu_avail():\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                    \n                    # cleanup to save VRAM\n                    del data, target\n#                     clean_vram()\n                train_loss = train_loss / train_data_size\n                valid_loss = valid_loss / valid_data_size\n                train_acc = train_acc / train_data_size\n                valid_acc = valid_acc / valid_data_size\n                history.append([train_loss, valid_loss,train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    log_to_file(f'Epoch: {epoch}', True)\n                    log_to_file(\n                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n                        True\n                    )\n                    log_to_file(\n                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n                        True\n                    )\n          \n                if valid_loss < perf['valid_loss_min']:\n                    epochs_no_improve = 0\n                    perf['best_epoch'] = epoch\n                    perf['valid_loss_min'] = valid_loss\n                    perf['valid_best_acc'] = valid_acc\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n                        log_to_file(\n                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n                        )\n                        total_time = time.time() - overall_start\n                        log_to_file(\n                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n                        )\n                        log_to_file()\n\n                        # Load the best state from saved model\n                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n                        # save the full history\n                        save_model(perf, model, optimizer, scaler, history, model_path)\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history, perf\n    \n    total_time = time.time() - overall_start\n    log_to_file(\n        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n    )\n    log_to_file(\n        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n    )\n    log_to_file()\n\n    # Load the best state from saved model\n    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n    # save the full history\n    save_model(perf, model, optimizer, scaler, history, model_path)\n\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n    \n    return model, history, perf\n\n\ndef save_train_val_loss_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_loss', 'valid_loss']:\n      plt.plot(\n          history[c], label=c)\n\n  title = f'{curr_model} - Training and Validation Losses'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Losses')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')\n\n\ndef save_train_val_acc_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_acc', 'valid_acc']:\n      plt.plot(\n          100 * history[c], label=c)\n      \n  title = f'{curr_model} - Training and Validation Accuracy'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Accuracy')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:40.250311Z","iopub.execute_input":"2023-01-29T08:10:40.252330Z","iopub.status.idle":"2023-01-29T08:10:40.329321Z","shell.execute_reply.started":"2023-01-29T08:10:40.252295Z","shell.execute_reply":"2023-01-29T08:10:40.328075Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Define Functions to Visualize Prediction","metadata":{}},{"cell_type":"code","source":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # mean = np.array([0.485, 0.456, 0.406])\n    # std = np.array([0.229, 0.224, 0.225])\n    # inp = std * inp + mean\n    # inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=[15, 15])\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n# confusion matrix \ndef getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n    model.eval()\n    confusion_matrix=np.zeros((2,2),dtype=int)\n    num_images=test_data_size\n    \n    with torch.no_grad():\n        for i, (data,target) in enumerate(dataloader):\n            data = data.to(device)\n            target = target.to(device)\n            \n            output = model(data) \n            _, pred = torch.max(output, 1)\n            \n            for j in range(data.size()[0]): \n                if pred[j]==1 and target[j]==1:\n                    term='TP'\n                    confusion_matrix[0][0]+=1\n                elif pred[j]==1 and target[j]==0:\n                    term='FP'\n                    confusion_matrix[1][0]+=1\n                elif pred[j]==0 and target[j]==1:\n                    term='FN'\n                    confusion_matrix[0][1]+=1\n                elif pred[j]==0 and target[j]==0:\n                    term='TN'\n                    confusion_matrix[1][1]+=1\n            \n                if show_image:\n                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n                    log_to_file(term, print_to_console_only)\n                    imshow(data.cpu().data[j])\n        \n        log_to_file(None, print_to_console_only)\n        category = 'Test' if is_test else 'Validation'\n        log_to_file('=====================', print_to_console_only)\n        log_to_file(f'{category} Results ', print_to_console_only)\n        log_to_file('=====================', print_to_console_only)\n        log_to_file('Confusion Matrix: ', print_to_console_only)\n        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n        log_to_file(None, print_to_console_only)\n\n        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(None, print_to_console_only)\n    return confusion_matrix\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:10:40.330749Z","iopub.execute_input":"2023-01-29T08:10:40.331302Z","iopub.status.idle":"2023-01-29T08:10:40.361476Z","shell.execute_reply.started":"2023-01-29T08:10:40.331268Z","shell.execute_reply":"2023-01-29T08:10:40.360747Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Run all models - Init Models + Training","metadata":{}},{"cell_type":"code","source":"model_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n    CovidnetModel.__name__, #11\n]\n\nfor i in range(0, len(model_list)):\n  # https://github.com/pytorch/pytorch/issues/50198\n  # skipped these because cannot use deterministic algorithm\n#   skip_model = [0, 1, 5, 8, 9, 10]\n  skip_model = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n  if i in skip_model:\n    continue\n  curr_model = model_list[i]\n\n  # Initialize model, criterion and optimizer\n  model, criterion, optimizer, scaler = init_model(curr_model)\n\n#   Training & Validation\n  model, history, perf = train(\n      model,\n      criterion,\n      optimizer,\n      scaler,\n      train_loader,\n      val_loader,\n      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n      max_epochs_stop=5,  # Early stopping intialization\n      n_epochs=100,\n      min_epoch=100,\n      print_every=10)\n\n  history\n  save_train_val_loss_graph(history, perf)\n  save_train_val_acc_graph(history, perf)\n  getConfusionMatrix(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-01-28T00:44:10.190636Z","iopub.execute_input":"2023-01-28T00:44:10.190997Z","iopub.status.idle":"2023-01-28T08:28:19.655407Z","shell.execute_reply.started":"2023-01-28T00:44:10.190966Z","shell.execute_reply":"2023-01-28T08:28:19.654369Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"env: PYTHONHASHSEED=18\nCreating CovidNetBlock with layout length of 4\nCreating CovidNetBlock with layout length of 4\nCovidnetModel is initialized\nModel has been trained for: 41 epochs.\n\nBest epoch: 41 with loss: 0.0126 and acc: 99.50%\n\n\nenv: PYTHONHASHSEED=18\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 49\t100.00% complete. 444.88 seconds elapsed in epoch.\n\nTraining Loss: 0.0156 \tValidation Loss: 0.0293\n\nTraining Accuracy: 99.51%\t Validation Accuracy: 99.20% \n\n\nEpoch: 59\t100.00% complete. 439.46 seconds elapsed in epoch.\n\nTraining Loss: 0.0096 \tValidation Loss: 0.0129\n\nTraining Accuracy: 99.64%\t Validation Accuracy: 99.50% \n\n\nEpoch: 69\t100.00% complete. 444.51 seconds elapsed in epoch.\n\nTraining Loss: 0.0155 \tValidation Loss: 0.0314\n\nTraining Accuracy: 99.51%\t Validation Accuracy: 98.80% \n\n\nEpoch: 79\t100.00% complete. 449.14 seconds elapsed in epoch.\n\nTraining Loss: 0.0058 \tValidation Loss: 0.0151\n\nTraining Accuracy: 99.79%\t Validation Accuracy: 99.60% \n\n\nEpoch: 89\t100.00% complete. 443.45 seconds elapsed in epoch.\n\nTraining Loss: 0.0059 \tValidation Loss: 0.0174\n\nTraining Accuracy: 99.74%\t Validation Accuracy: 99.60% \n\n\nEpoch: 99\t100.00% complete. 440.61 seconds elapsed in epoch.\n\nTraining Loss: 0.0080 \tValidation Loss: 0.0268\n\nTraining Accuracy: 99.77%\t Validation Accuracy: 99.20% \n\n\n\nBest epoch: 83 with loss: 0.0068 and acc: 99.6004%\n\n27818.8958 total seconds elapsed. 331.1773 seconds per epoch.\n\n\n\n\n\n=====================\n\nValidation Results \n\n=====================\n\nConfusion Matrix: \n\n[[101   0]\n [  0 168]]\n\n\n\nSensitivity: 100.0\n\nSpecificity: 100.0\n\nPPV: 100.0\n\nNPV: 100.0\n\nAccuracy: 100.0\n\nF1-Score: 1.0\n\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"## Download Results\n<a href=\"/kaggle/working/result/CovidnetModel - Training and Validation Accuracy.png\"> CovidnetModel - Training and Validation Accuracy.png </a> </br>\n<a href=\"/kaggle/working/result/CovidnetModel - Training and Validation Losses.png\"> CovidnetModel - Training and Validation Losses.png </a> </br>\n<a href=\"/kaggle/working/result/CovidnetModel.pt\"> CovidnetModel.pt </a> </br>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/result\nfrom IPython.display import FileLink\n# FileLink(r'CovidnetModel - Training and Validation Accuracy.png')\n# FileLink(r'CovidnetModel - Training and Validation Losses.png')\nFileLink(r'CovidnetModel.pt')","metadata":{"execution":{"iopub.status.busy":"2023-01-21T16:13:52.836066Z","iopub.execute_input":"2023-01-21T16:13:52.836951Z","iopub.status.idle":"2023-01-21T16:13:52.845442Z","shell.execute_reply.started":"2023-01-21T16:13:52.836891Z","shell.execute_reply":"2023-01-21T16:13:52.844365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Test (Evaluation)\n\n------------------------------------------------------------------------\n- After selecting the best hyperparameters and models, use the model to predict the result on the test images\n- Get the confusion matrix and prediction similar to what we did for validation images","metadata":{}},{"cell_type":"code","source":"def reloadModel():\n  saved_model_path = f'{path.join(RESULT_DIR, curr_model)}.pt'\n  if os.path.exists(saved_model_path):\n    model, criterion, optimizer, scaler = init_model(curr_model)\n    return load_model(model, optimizer, scaler, saved_model_path)\n  else:\n    return None, None, None, None, None, None\n\nmodel_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n    CovidnetModel.__name__, #11\n]\n\nfor i in range(0, len(model_list)):\n  skip_model = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n  if i in skip_model:\n    continue\n  curr_model = model_list[i]\n  _, model, _, _, _, _ = reloadModel()\n  seed_everything()\n  if model:\n    trainable_parameters = count_parameters(model)\n    print(f'model: {curr_model}, number of trainable parameters: {trainable_parameters}')\n\ngetConfusionMatrix(model, test_loader, True)\n# visualize_test_prediction(model)","metadata":{"execution":{"iopub.status.busy":"2023-01-29T08:25:26.006165Z","iopub.execute_input":"2023-01-29T08:25:26.006520Z","iopub.status.idle":"2023-01-29T08:26:13.764710Z","shell.execute_reply.started":"2023-01-29T08:25:26.006490Z","shell.execute_reply":"2023-01-29T08:26:13.763804Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"env: PYTHONHASHSEED=18\nCreating CovidNetBlock with layout length of 4\nCreating CovidNetBlock with layout length of 4\nCovidnetModel is initialized\nenv: PYTHONHASHSEED=18\nmodel: CovidnetModel, number of trainable parameters: 7724523\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:780: UserWarning: Note that order of the arguments: ceil_mode and return_indices will changeto match the args list in nn.MaxPool2d in a future release.\n  warnings.warn(\"Note that order of the arguments: ceil_mode and return_indices will change\"\n","output_type":"stream"},{"name":"stdout","text":"\n\n=====================\n\nTest Results \n\n=====================\n\nConfusion Matrix: \n\n[[100   1]\n [  1 167]]\n\n\n\nSensitivity: 99.00990099009901\n\nSpecificity: 99.4047619047619\n\nPPV: 99.00990099009901\n\nNPV: 99.4047619047619\n\nAccuracy: 99.25650557620818\n\nF1-Score: 0.9900990099009901\n\n\n\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"array([[100,   1],\n       [  1, 167]])"},"metadata":{}}]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Playground\n\n------------------------------------------------------------------------\n\n> Testbed that is not compulsary for any part of this notebook","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Models using PytorchViz\n\n-   https://pytorch.org/docs/stable/nn.html\n-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n-   [Mandrin explanation of pytorch resnet\n    code](https://www.jianshu.com/p/90d61f53d15d)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n%pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\nfrom torchviz import make_dot, make_dot_from_trace","metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchinfo import summary\n%pip install -U torchsummary\nfrom torchsummary import summary\n\nif is_gpu_avail():\n    device = torch.device('cuda')\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = torch.device('cpu')\n\n\n# model = models.googlenet()\n# summary(model, (3, 299, 299))\n\n# model = ResidualBlock(192, 192, BlockType.IDENTITY, 1)\n# summary(model, (192, 28, 28))\n\n# model = ResidualBlock(192, 256, BlockType.CONV, 1) # also works if 192,192\n# summary(model, (192, 28, 28))\n\n# model = ResidualBlock(256, 320, BlockType.CONV, 1) # also works if 192,192\n# summary(model, (256, 28, 28))\n\n# model = ReductionBlock(1024, 192)\n# summary(model, (1024, 28, 28))\n\n# residual_block_layout = {\n#     BlockType.CONV:[\n#         dict(in_chan=192, out_chan=256),\n#         dict(in_chan=256, out_chan=512),\n#         dict(in_chan=512, out_chan=1024),\n#     ],\n#     BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n# }\n# model = CovidNetBlock(residual_block_layout, 192) \n# summary(model, (192, 28, 28))\n\nmodel = CovidnetModel()\nsummary(model, (3, 224, 224))\n\n# model = models.resnext50_32x4d()\n# summary(model, (3, 224, 224))\n\n# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n# y = model(x)\n# dot = make_dot(y, params=dict(model.named_parameters()))\n\n# dot.format = \"png\"\n# dot.render(render_model_pic_file)\n# files.download(f\"{render_model_pic_file}.{dot.format}\")","metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1","execution":{"iopub.status.busy":"2023-01-29T08:27:13.853205Z","iopub.execute_input":"2023-01-29T08:27:13.853557Z","iopub.status.idle":"2023-01-29T08:27:25.034487Z","shell.execute_reply.started":"2023-01-29T08:27:13.853527Z","shell.execute_reply":"2023-01-29T08:27:25.032973Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCreating CovidNetBlock with layout length of 4\nCreating CovidNetBlock with layout length of 4\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n         LeakyReLU-3         [-1, 64, 112, 112]               0\n         Conv2d_BN-4         [-1, 64, 112, 112]               0\n         MaxPool2d-5           [-1, 64, 56, 56]               0\n            Conv2d-6           [-1, 80, 56, 56]           5,120\n       BatchNorm2d-7           [-1, 80, 56, 56]             160\n         LeakyReLU-8           [-1, 80, 56, 56]               0\n         Conv2d_BN-9           [-1, 80, 56, 56]               0\n           Conv2d-10          [-1, 192, 56, 56]         138,240\n      BatchNorm2d-11          [-1, 192, 56, 56]             384\n        LeakyReLU-12          [-1, 192, 56, 56]               0\n        Conv2d_BN-13          [-1, 192, 56, 56]               0\n        MaxPool2d-14          [-1, 192, 28, 28]               0\n        StemBlock-15          [-1, 192, 28, 28]               0\n           Conv2d-16          [-1, 128, 28, 28]          24,576\n      BatchNorm2d-17          [-1, 128, 28, 28]             256\n        LeakyReLU-18          [-1, 128, 28, 28]               0\n        Conv2d_BN-19          [-1, 128, 28, 28]               0\n           Conv2d-20          [-1, 128, 28, 28]           4,608\n      BatchNorm2d-21          [-1, 128, 28, 28]             256\n        LeakyReLU-22          [-1, 128, 28, 28]               0\n        Conv2d_BN-23          [-1, 128, 28, 28]               0\n           Conv2d-24          [-1, 256, 28, 28]          32,768\n      BatchNorm2d-25          [-1, 256, 28, 28]             512\n        Conv2d_BN-26          [-1, 256, 28, 28]               0\n           Conv2d-27          [-1, 256, 28, 28]          49,152\n      BatchNorm2d-28          [-1, 256, 28, 28]             512\n        Conv2d_BN-29          [-1, 256, 28, 28]               0\n        LeakyReLU-30          [-1, 256, 28, 28]               0\n    ResidualBlock-31          [-1, 256, 28, 28]               0\n           Conv2d-32          [-1, 256, 28, 28]          65,536\n      BatchNorm2d-33          [-1, 256, 28, 28]             512\n        LeakyReLU-34          [-1, 256, 28, 28]               0\n        Conv2d_BN-35          [-1, 256, 28, 28]               0\n           Conv2d-36          [-1, 256, 28, 28]          18,432\n      BatchNorm2d-37          [-1, 256, 28, 28]             512\n        LeakyReLU-38          [-1, 256, 28, 28]               0\n        Conv2d_BN-39          [-1, 256, 28, 28]               0\n           Conv2d-40          [-1, 512, 28, 28]         131,072\n      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n        Conv2d_BN-42          [-1, 512, 28, 28]               0\n           Conv2d-43          [-1, 512, 28, 28]         131,072\n      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n        Conv2d_BN-45          [-1, 512, 28, 28]               0\n        LeakyReLU-46          [-1, 512, 28, 28]               0\n    ResidualBlock-47          [-1, 512, 28, 28]               0\n           Conv2d-48          [-1, 512, 28, 28]         262,144\n      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n        LeakyReLU-50          [-1, 512, 28, 28]               0\n        Conv2d_BN-51          [-1, 512, 28, 28]               0\n           Conv2d-52          [-1, 512, 28, 28]          73,728\n      BatchNorm2d-53          [-1, 512, 28, 28]           1,024\n        LeakyReLU-54          [-1, 512, 28, 28]               0\n        Conv2d_BN-55          [-1, 512, 28, 28]               0\n           Conv2d-56         [-1, 1024, 28, 28]         524,288\n      BatchNorm2d-57         [-1, 1024, 28, 28]           2,048\n        Conv2d_BN-58         [-1, 1024, 28, 28]               0\n           Conv2d-59         [-1, 1024, 28, 28]         524,288\n      BatchNorm2d-60         [-1, 1024, 28, 28]           2,048\n        Conv2d_BN-61         [-1, 1024, 28, 28]               0\n        LeakyReLU-62         [-1, 1024, 28, 28]               0\n    ResidualBlock-63         [-1, 1024, 28, 28]               0\n           Conv2d-64          [-1, 512, 28, 28]         524,288\n      BatchNorm2d-65          [-1, 512, 28, 28]           1,024\n        LeakyReLU-66          [-1, 512, 28, 28]               0\n        Conv2d_BN-67          [-1, 512, 28, 28]               0\n           Conv2d-68          [-1, 512, 28, 28]          73,728\n      BatchNorm2d-69          [-1, 512, 28, 28]           1,024\n        LeakyReLU-70          [-1, 512, 28, 28]               0\n        Conv2d_BN-71          [-1, 512, 28, 28]               0\n           Conv2d-72         [-1, 1024, 28, 28]         524,288\n      BatchNorm2d-73         [-1, 1024, 28, 28]           2,048\n        Conv2d_BN-74         [-1, 1024, 28, 28]               0\n        LeakyReLU-75         [-1, 1024, 28, 28]               0\n    ResidualBlock-76         [-1, 1024, 28, 28]               0\n           Conv2d-77           [-1, 72, 13, 13]         663,552\n      BatchNorm2d-78           [-1, 72, 13, 13]             144\n        LeakyReLU-79           [-1, 72, 13, 13]               0\n        Conv2d_BN-80           [-1, 72, 13, 13]               0\n           Conv2d-81           [-1, 36, 28, 28]          36,864\n      BatchNorm2d-82           [-1, 36, 28, 28]              72\n        LeakyReLU-83           [-1, 36, 28, 28]               0\n        Conv2d_BN-84           [-1, 36, 28, 28]               0\n           Conv2d-85           [-1, 42, 28, 28]          13,608\n      BatchNorm2d-86           [-1, 42, 28, 28]              84\n        LeakyReLU-87           [-1, 42, 28, 28]               0\n        Conv2d_BN-88           [-1, 42, 28, 28]               0\n           Conv2d-89           [-1, 48, 13, 13]          18,144\n      BatchNorm2d-90           [-1, 48, 13, 13]              96\n        LeakyReLU-91           [-1, 48, 13, 13]               0\n        Conv2d_BN-92           [-1, 48, 13, 13]               0\n        MaxPool2d-93         [-1, 1024, 13, 13]               0\n           Conv2d-94           [-1, 72, 13, 13]          73,728\n      BatchNorm2d-95           [-1, 72, 13, 13]             144\n        LeakyReLU-96           [-1, 72, 13, 13]               0\n        Conv2d_BN-97           [-1, 72, 13, 13]               0\n   ReductionBlock-98          [-1, 192, 13, 13]               0\n    CovidNetBlock-99          [-1, 192, 13, 13]               0\n          Conv2d-100          [-1, 128, 13, 13]          24,576\n     BatchNorm2d-101          [-1, 128, 13, 13]             256\n       LeakyReLU-102          [-1, 128, 13, 13]               0\n       Conv2d_BN-103          [-1, 128, 13, 13]               0\n          Conv2d-104          [-1, 128, 13, 13]           4,608\n     BatchNorm2d-105          [-1, 128, 13, 13]             256\n       LeakyReLU-106          [-1, 128, 13, 13]               0\n       Conv2d_BN-107          [-1, 128, 13, 13]               0\n          Conv2d-108          [-1, 256, 13, 13]          32,768\n     BatchNorm2d-109          [-1, 256, 13, 13]             512\n       Conv2d_BN-110          [-1, 256, 13, 13]               0\n          Conv2d-111          [-1, 256, 13, 13]          49,152\n     BatchNorm2d-112          [-1, 256, 13, 13]             512\n       Conv2d_BN-113          [-1, 256, 13, 13]               0\n       LeakyReLU-114          [-1, 256, 13, 13]               0\n   ResidualBlock-115          [-1, 256, 13, 13]               0\n          Conv2d-116          [-1, 256, 13, 13]          65,536\n     BatchNorm2d-117          [-1, 256, 13, 13]             512\n       LeakyReLU-118          [-1, 256, 13, 13]               0\n       Conv2d_BN-119          [-1, 256, 13, 13]               0\n          Conv2d-120          [-1, 256, 13, 13]          18,432\n     BatchNorm2d-121          [-1, 256, 13, 13]             512\n       LeakyReLU-122          [-1, 256, 13, 13]               0\n       Conv2d_BN-123          [-1, 256, 13, 13]               0\n          Conv2d-124          [-1, 512, 13, 13]         131,072\n     BatchNorm2d-125          [-1, 512, 13, 13]           1,024\n       Conv2d_BN-126          [-1, 512, 13, 13]               0\n          Conv2d-127          [-1, 512, 13, 13]         131,072\n     BatchNorm2d-128          [-1, 512, 13, 13]           1,024\n       Conv2d_BN-129          [-1, 512, 13, 13]               0\n       LeakyReLU-130          [-1, 512, 13, 13]               0\n   ResidualBlock-131          [-1, 512, 13, 13]               0\n          Conv2d-132          [-1, 512, 13, 13]         262,144\n     BatchNorm2d-133          [-1, 512, 13, 13]           1,024\n       LeakyReLU-134          [-1, 512, 13, 13]               0\n       Conv2d_BN-135          [-1, 512, 13, 13]               0\n          Conv2d-136          [-1, 512, 13, 13]          73,728\n     BatchNorm2d-137          [-1, 512, 13, 13]           1,024\n       LeakyReLU-138          [-1, 512, 13, 13]               0\n       Conv2d_BN-139          [-1, 512, 13, 13]               0\n          Conv2d-140         [-1, 1024, 13, 13]         524,288\n     BatchNorm2d-141         [-1, 1024, 13, 13]           2,048\n       Conv2d_BN-142         [-1, 1024, 13, 13]               0\n          Conv2d-143         [-1, 1024, 13, 13]         524,288\n     BatchNorm2d-144         [-1, 1024, 13, 13]           2,048\n       Conv2d_BN-145         [-1, 1024, 13, 13]               0\n       LeakyReLU-146         [-1, 1024, 13, 13]               0\n   ResidualBlock-147         [-1, 1024, 13, 13]               0\n          Conv2d-148          [-1, 512, 13, 13]         524,288\n     BatchNorm2d-149          [-1, 512, 13, 13]           1,024\n       LeakyReLU-150          [-1, 512, 13, 13]               0\n       Conv2d_BN-151          [-1, 512, 13, 13]               0\n          Conv2d-152          [-1, 512, 13, 13]          73,728\n     BatchNorm2d-153          [-1, 512, 13, 13]           1,024\n       LeakyReLU-154          [-1, 512, 13, 13]               0\n       Conv2d_BN-155          [-1, 512, 13, 13]               0\n          Conv2d-156         [-1, 1024, 13, 13]         524,288\n     BatchNorm2d-157         [-1, 1024, 13, 13]           2,048\n       Conv2d_BN-158         [-1, 1024, 13, 13]               0\n       LeakyReLU-159         [-1, 1024, 13, 13]               0\n   ResidualBlock-160         [-1, 1024, 13, 13]               0\n          Conv2d-161             [-1, 72, 6, 6]         663,552\n     BatchNorm2d-162             [-1, 72, 6, 6]             144\n       LeakyReLU-163             [-1, 72, 6, 6]               0\n       Conv2d_BN-164             [-1, 72, 6, 6]               0\n          Conv2d-165           [-1, 36, 13, 13]          36,864\n     BatchNorm2d-166           [-1, 36, 13, 13]              72\n       LeakyReLU-167           [-1, 36, 13, 13]               0\n       Conv2d_BN-168           [-1, 36, 13, 13]               0\n          Conv2d-169           [-1, 42, 13, 13]          13,608\n     BatchNorm2d-170           [-1, 42, 13, 13]              84\n       LeakyReLU-171           [-1, 42, 13, 13]               0\n       Conv2d_BN-172           [-1, 42, 13, 13]               0\n          Conv2d-173             [-1, 48, 6, 6]          18,144\n     BatchNorm2d-174             [-1, 48, 6, 6]              96\n       LeakyReLU-175             [-1, 48, 6, 6]               0\n       Conv2d_BN-176             [-1, 48, 6, 6]               0\n       MaxPool2d-177           [-1, 1024, 6, 6]               0\n          Conv2d-178             [-1, 72, 6, 6]          73,728\n     BatchNorm2d-179             [-1, 72, 6, 6]             144\n       LeakyReLU-180             [-1, 72, 6, 6]               0\n       Conv2d_BN-181             [-1, 72, 6, 6]               0\n  ReductionBlock-182            [-1, 192, 6, 6]               0\n   CovidNetBlock-183            [-1, 192, 6, 6]               0\nAdaptiveAvgPool2d-184            [-1, 192, 1, 1]               0\n          Linear-185                    [-1, 3]             579\n================================================================\nTotal params: 7,724,523\nTrainable params: 7,724,523\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 283.39\nParams size (MB): 29.47\nEstimated Total Size (MB): 313.43\n----------------------------------------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Investigate Image in Dataset","metadata":{}},{"cell_type":"code","source":"import subprocess\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\nDATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\nDATASET_NAME = \"COVIDx_CT-3A\"\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n\n# count number of images\n# !ls -Uba1 /content/data/3A_images | grep -c png\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\nprint(f\"number of images: {output.decode()}\")\n\n# list first 10 images\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\nfirst_10_lines = output.decode()\n# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\nimg_list = first_10_lines.split('\\n')\nfirst_png = next(filter(lambda img: \"png\" in img, img_list), None)\nprint(f\"first_png: {first_png}\\n\")","metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the first test image, unbounded followed by bounded\nimport torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as torch_func_trans\nfrom PIL import Image\n\n# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\ndef get_data_from_split_file(split_file):\n    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n    files, classes, bboxes = [], [], []\n    with open(split_file, 'r') as f:\n        for line in f.readlines():\n            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n            files.append(path.join(DATASET_DIR, fname))\n            classes.append(int(cls))\n            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n    return files, classes, bboxes\n\ndef bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n    top = ymax\n    left = xmin\n    height = ymax - ymin\n    width = xmax - xmin\n    return top, left, height, width\n\nfiles, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\nfirst_file_tuple = (files[0], classes[0], bbox[0])\n\nprint(f\"first image: {first_file_tuple[0]}\")\nif 0:\n    # this way of reading image is deprecated\n    img = plt.imread(first_file_tuple[0])\n\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"unbounded\")\n    _ = plt.imshow(img)\n    _ = plt.axis('off')\n\n    torch_img = torch.from_numpy(img)\n    print(f\"torch_img size: {torch_img.size()}\")\n    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n    print(f\"pytorch_size: {pytorch_size}\")\n\n    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"bounded\")\n    _ = plt.imshow(cropped_img)\n    _ = plt.axis('off')\n\n# This is the recommended method for opening image\n# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\nwith Image.open(first_file_tuple[0]) as im:\n    print(\"\\n\")\n    print(\"Unbounded image\")\n    IMG = im\n    display(im)\n\n    print(\"\\n\")\n    print(\"Bounded image\")\n    display(im.crop(first_file_tuple[2]))\n\n    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n    print(\"\\n\")\n    print(\"Out of Scope Unbounded image\")\n    display(IMG)\n    ","metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate metadata.csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.expand_frame_repr', False)\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nmeta_df = pd.read_csv(METADATA_CSV)\nprint(\"First 5 rows in metadata.csv0\")\nprint(meta_df.head(5))\nprint(f\"classes: {sorted(meta_df['finding'].unique())}\")\n\nunverified = meta_df['verified finding'].eq('No').sum()\nverified = meta_df['verified finding'].eq('Yes').sum()\nprint(f\"Not verified: {unverified}\")\nprint(f\"Verified: {verified}\")\n\nunverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\nassert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\nprint(f\"first unverified ID: {unverified_patient_ids[0]}\")\n\nverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\nassert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\nprint(f\"first verified ID: {verified_patient_ids[0]}\")\n\nimgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\nprint(f\"total images: {len(imgs)}\")\nprint(f\"first 10 images: {imgs[:10]}\")\nimgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\nprint(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")","metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigating split dataset","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\nSEED = 18\nseed_everything(SEED)\nBATCH_SIZE = 128\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n# train_index, test_index = next(\n#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n#         dummy_X, full_dataset.targets\n#     )\n# )\n\nprint('*'*50)\nprint(\"temp_train\")\nprint('*'*50)\nprint(f\"First 10 index: {temp_train_index[:10]}\")\nprint(f\"First 10 label: {y_remaining[:10]}\")\nprint(f\"length of index: {len(temp_train_index)}\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {y_remaining.count(0)}\")\nprint(f\"Pneunomia: {y_remaining.count(1)}\")\nprint(f\"Covid-19: {y_remaining.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"test\")\nprint('*'*50)\nprint(f\"First 10 index: {test_index[:10]}\")\nprint(f\"First 10 label: {Y_test[:10]}\")\nprint(f\"length of index: {len(test_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_test.count(0)}\")\nprint(f\"Pneunomia: {Y_test.count(1)}\")\nprint(f\"Covid-19: {Y_test.count(2)}\")\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\nX_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\nprint()\nprint('*'*50)\nprint(\"train\")\nprint('*'*50)\nprint(f\"First 10 index: {train_index[:10]}\")\nprint(f\"First 10 label: {Y_train[:10]}\")\nprint(f\"length of index: {len(train_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_train.count(0)}\")\nprint(f\"Pneunomia: {Y_train.count(1)}\")\nprint(f\"Covid-19: {Y_train.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"val\")\nprint('*'*50)\nprint(f\"First 10 index: {val_index[:10]}\")\nprint(f\"First 10 label: {Y_val[:10]}\")\nprint(f\"length of index: {len(val_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_val.count(0)}\")\nprint(f\"Pneunomia: {Y_val.count(1)}\")\nprint(f\"Covid-19: {Y_val.count(2)}\")","metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Dataloader","metadata":{}},{"cell_type":"code","source":"import pprint\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # mean = np.array([0.485, 0.456, 0.406])\n    # std = np.array([0.229, 0.224, 0.225])\n    # inp = std * inp + mean\n    # inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=[15, 15])\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nseed_everything(19)\nclass_names, _ = full_dataset.find_classes()\ndata, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n# out = torchvision.utils.make_grid(data)\n# imshow(out)\n# pp = pprint.PrettyPrinter(compact=True)\n# pp.pprint([class_names[x] for x in classes])\n\nprint(f\"Class: {classes[0]}\")\nout = data[0]\nimshow(out)","metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying Avalance","metadata":{}},{"cell_type":"code","source":"# https://changhsinlee.com/colab-import-python/\n!pip install requests\n!pip install avalanche-lib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\n# Save datagenerators as file to colab working directory\n# If you are using GitHub, make sure you get the \"Raw\" version of the code\nurl = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\nr = requests.get(url)\n\n# make sure your filename is the same as how you want to import \nwith open('pytorchcv_models.py', 'w') as f:\n    f.write(r.text)\n\n# now we can import\nimport pytorchcv_models as pycv\nfrom types import SimpleNamespace\n\nargs = SimpleNamespace()\nargs.cuda = 0\npycv.main(args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"LOG_DIR = os.path.join(CURR_DIR, \"log\")\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\ncurr_model = \"\"\n\ndef log_to_file(txt=None, print_to_console_only=False):\n  if txt is None:\n    txt = ''\n  txt += '\\n'\n  print(txt)\n  if print_to_console_only:\n    return\n  if not path.exists(LOG_DIR):\n    os.mkdir(LOG_DIR)\n  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n  with open(full_path, mode='a') as f:\n    f.write(txt)\n    \n# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n# Make sure to delete any references to tensor. Else this function will not have significant effect\ndef clean_vram():\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\ndef name_of_caller(frame=1):\n    \"\"\"\n    Return \"class.function_name\" of the caller or just \"function_name\".\n    \"\"\"\n    frame = sys._getframe(frame)\n    fn_name = frame.f_code.co_name\n    var_names = frame.f_code.co_varnames\n    if var_names:\n        if var_names[0] == \"self\":\n            self_obj = frame.f_locals.get(\"self\")\n            if self_obj is not None:\n                return f\"{type(self_obj).__name__}.{fn_name}\" \n        if var_names[0] == \"cls\":\n            cls_obj = frame.f_locals.get(\"cls\")\n            if cls_obj is not None:\n                return f\"{cls_obj.__name__}.{fn_name}\"\n    return fn_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Initialize Deep Learning Models","metadata":{}},{"cell_type":"code","source":"model_constructors = {\n  models.alexnet.__name__: models.alexnet, \n  models.squeezenet1_1.__name__: models.squeezenet1_1,\n  models.resnet50.__name__: models.resnet50, \n  models.resnet101.__name__: models.resnet101,\n  models.resnet152.__name__: models.resnet152, \n  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n  models.densenet201.__name__: models.densenet201, \n  models.googlenet.__name__: models.googlenet, \n  models.vgg16.__name__: models.vgg16, \n  models.vgg19.__name__: models.vgg19, \n  models.inception_v3.__name__: models.inception_v3, \n}\n\nfrom torchvision.models import *\nmodel_weights = {\n  models.alexnet.__name__: AlexNet_Weights.DEFAULT,\n  models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n  models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n  models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n  models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n  models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n  models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n  models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n  models.vgg16.__name__: VGG16_Weights.DEFAULT,\n  models.vgg19.__name__: VGG19_Weights.DEFAULT,\n  models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n}\n\n# Experiment around dropout & Learning Rate & different optimizer (Adam)\ndef init_model(name):\n  if not path.exists(RESULT_DIR):\n    os.mkdir(RESULT_DIR)\n\n  clean_vram()\n  seed_everything()\n  model = model_constructors[name](weights=model_weights[name])\n  \n  # fine-tune pretrain models to our usecase\n  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n  NUM_CLASSES = len(class_names)\n  DROPOUT = 0.5\n  if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.classifier[6] = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n  elif name == models.densenet201.__name__:\n    num_ftrs = model.classifier.in_features\n    model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.classifier = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n  elif name == models.squeezenet1_1.__name__:\n    model.classifier = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n#     model.classifier = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n#     )\n    model.num_classes = NUM_CLASSES\n  elif name == models.inception_v3.__name__:\n    auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n    model.AuxLogits.fc = nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n#     model.AuxLogits.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n#     )\n    primary_num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(primary_num_ftrs, NUM_CLASSES)\n#     model.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(primary_num_ftrs, NUM_CLASSES)\n#     )\n  else:\n    # resnet, resnext & googlenet\n    num_ftrs = model.fc.in_features\n    model.fc= nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n\n  model = model.to(device)\n  criterion = nn.CrossEntropyLoss()\n  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  if is_gpu_avail():\n    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n    scaler = torch.cuda.amp.GradScaler()\n  else:\n    raise RuntimeError('This code only support machine with GPU.')\n\n  # print('=====================================')\n  print(f'{name} is initialized')\n  # print('=====================================')\n  # print(model)\n  return model, criterion, optimizer, scaler\n\n# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\ndef save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n  torch.save({\n    'perf_metrics': perf_metrics,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    'history': history,\n    }, model_path)\n\ndef load_model(model, optimizer, scaler, model_path):\n  if not os.path.exists(model_path):\n    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n    return None, model, optimizer, scaler, None, None\n  checkpoint = torch.load(model_path)\n  perf_metrics = checkpoint['perf_metrics']\n  model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n  history = checkpoint['history']\n  total_epoch = len(history) - 1\n  del checkpoint\n\n  return perf_metrics, model, optimizer, scaler, history, total_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Train Models","metadata":{}},{"cell_type":"code","source":"# training and validation loops\ndef train(model,\n    criterion,\n    optimizer,\n    scaler,\n    train_dataloader,\n    valid_dataloader,\n    model_path,\n    max_epochs_stop=10,\n    n_epochs=400,\n    min_epoch=300,\n    print_every=1):\n    \n    epochs_no_improve = 0\n    perf = {\n        'best_epoch': 0,\n        'valid_loss_min': np.Inf,\n        'valid_best_acc': 0,\n    }\n    total_epoch = 0\n\n    try:\n        if os.path.exists(model_path):\n            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n        else:\n            history = []\n            log_to_file(f'Starting Training from Scratch.\\n')\n    except:\n        history = []\n        log_to_file(f'exception: start from scratch.\\n')\n\n    overall_start = time.time()\n    if total_epoch >= n_epochs:\n        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n        history = pd.DataFrame(\n            history,\n            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n        return model, history, perf\n\n    seed_everything()\n\n    # Main loop - continue training on where we left off if there's a saved model\n    for epoch in range(total_epoch, n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = time.time()\n        for ii, (data, target) in enumerate (train_dataloader):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n            with torch.cuda.amp.autocast():\n              # output, aux_output = model(data)\n              # loss1 = criterion(output, target)\n              # loss2 = criterion(aux_output, target)\n              # loss = loss1 + 0.4*loss2\n              output = model(data)\n              loss = criterion(output, target)\n            # loss.backward()\n            # optimizer.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n            \n            # cleanup to save VRAM\n            del data, target\n#             clean_vram()\n\n        # After training loops ends, start validation\n        else:\n            print(\">>> Starting validation...\\n\")\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_dataloader:\n                    if is_gpu_avail():\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                    \n                    # cleanup to save VRAM\n                    del data, target\n#                     clean_vram()\n                train_loss = train_loss / train_data_size\n                valid_loss = valid_loss / valid_data_size\n                train_acc = train_acc / train_data_size\n                valid_acc = valid_acc / valid_data_size\n                history.append([train_loss, valid_loss,train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    log_to_file(f'Epoch: {epoch}', True)\n                    log_to_file(\n                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n                        True\n                    )\n                    log_to_file(\n                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n                        True\n                    )\n          \n                if valid_loss < perf['valid_loss_min']:\n                    print(f\">>> Saving model: valid_loss:{valid_loss}\\n\")\n                    epochs_no_improve = 0\n                    perf['best_epoch'] = epoch\n                    perf['valid_loss_min'] = valid_loss\n                    perf['valid_best_acc'] = valid_acc\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n                else:\n                    print(f\">>> No improvement: valid_loss:{valid_loss}; epoch:{epoch}; epochs_no_improve:{epochs_no_improve}\\n\")\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n                        log_to_file(\n                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n                        )\n                        total_time = time.time() - overall_start\n                        log_to_file(\n                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n                        )\n                        log_to_file()\n\n                        # Load the best state from saved model\n                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n                        # save the full history\n                        save_model(perf, model, optimizer, scaler, history, model_path)\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history, perf\n    \n    total_time = time.time() - overall_start\n    log_to_file(\n        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n    )\n    log_to_file(\n        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n    )\n    log_to_file()\n\n    # Load the best state from saved model\n    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n    # save the full history\n    save_model(perf, model, optimizer, scaler, history, model_path)\n\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n    \n    return model, history, perf\n\n\ndef save_train_val_loss_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_loss', 'valid_loss']:\n      plt.plot(\n          history[c], label=c)\n\n  title = f'{curr_model} - Training and Validation Losses'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Losses')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')\n\n\ndef save_train_val_acc_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_acc', 'valid_acc']:\n      plt.plot(\n          100 * history[c], label=c)\n      \n  title = f'{curr_model} - Training and Validation Accuracy'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Accuracy')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Functions to Visualize Prediction","metadata":{}},{"cell_type":"code","source":"# confusion matrix \ndef getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n    model.eval()\n    confusion_matrix=np.zeros((2,2),dtype=int)\n    num_images=test_data_size\n    \n    with torch.no_grad():\n        for i, (data,target) in enumerate(dataloader):\n            data = data.to(device)\n            target = target.to(device)\n            \n            output = model(data) \n            _, pred = torch.max(output, 1)\n            \n            for j in range(data.size()[0]): \n                if pred[j]==1 and target[j]==1:\n                    term='TP'\n                    confusion_matrix[0][0]+=1\n                elif pred[j]==1 and target[j]==0:\n                    term='FP'\n                    confusion_matrix[1][0]+=1\n                elif pred[j]==0 and target[j]==1:\n                    term='FN'\n                    confusion_matrix[0][1]+=1\n                elif pred[j]==0 and target[j]==0:\n                    term='TN'\n                    confusion_matrix[1][1]+=1\n            \n                if show_image:\n                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n                    log_to_file(term, print_to_console_only)\n                    imshow(data.cpu().data[j])\n        \n        log_to_file(None, print_to_console_only)\n        category = 'Test' if is_test else 'Validation'\n        log_to_file('=====================', print_to_console_only)\n        log_to_file(f'{category} Results ', print_to_console_only)\n        log_to_file('=====================', print_to_console_only)\n        log_to_file('Confusion Matrix: ', print_to_console_only)\n        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n        log_to_file(None, print_to_console_only)\n\n        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(None, print_to_console_only)\n    return confusion_matrix\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef save_test_acc_n_loss_graph(model, dataloader, criterion):\n  pass\n  # NOT NEEDED YET\n  # with torch.no_grad():\n  #   model.eval()\n  #   for data, target in dataloader:\n  #       if is_gpu_avail():\n  #           data, target = data.cuda(), target.cuda()\n  #       output = model(data)\n  #       loss = criterion(output, target)\n  #       test_loss += loss.item() * data.size(0)\n  #       _, pred = torch.max(output, dim=1)\n  #       correct_tensor = pred.eq(target.data.view_as(pred))\n  #       accuracy = torch.mean(\n  #           correct_tensor.type(torch.FloatTensor))\n  #       test_acc += accuracy.item() * data.size(0)\n  #   train_loss = train_loss / train_data_size\n  #   test_loss = test_loss / test_data_size\n  #   train_acc = train_acc / train_data_size\n  #   test_acc = test_acc / test_data_size\n\n\n# def visualize_test_prediction(model):\n#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n\n#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n\n#   # log_to_file(img_list)\n\n#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n#                               for img in img_list])\n#   pred_logits_tensor = model(test_batch)\n#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n\n#   row = 12\n#   col = 3\n#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n#   r = 0\n#   c = 0\n#   for i, img in enumerate(img_list):\n#       if c >= col:\n#         r += 1\n#         c = 0\n#       ax = axs[r, c]\n#       ax.axis('off')\n#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n#                                                               100*pred_probs[i,1]))\n#       ax.imshow(img)\n#       c +=1\n\n#   title = f'{curr_model} - Covid Image Prediction'\n#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n#   plt.savefig(full_path, bbox_inches='tight')\n\n\ndef getPredProbs(model, datasetStr, count, isSeeded=True):\n  if isSeeded:\n    seed_everything()\n  \n  dataset = data[datasetStr].samples\n  img_list = []\n  for i, (img_path, cls_idx) in enumerate(dataset):\n    if i >= count:\n      break\n    img_list.append(Image.open(img_path).convert('RGB'))\n\n  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n                              for img in img_list])\n  pred_logits_tensor = model(test_batch)\n  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run all models - Init Models + Training","metadata":{}},{"cell_type":"code","source":"model_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n]\n\nfor i in range(0,11):\n  # https://github.com/pytorch/pytorch/issues/50198\n  # skipped these because cannot use deterministic algorithm\n#   skip_model = [0, 1, 5, 8, 9, 10]\n  skip_model = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n  if i in skip_model:\n    continue\n  curr_model = model_list[i]\n\n  # Initialize model, criterion and optimizer\n  model, criterion, optimizer, scaler = init_model(curr_model)\n\n#   Training & Validation\n  model, history, perf = train(\n      model,\n      criterion,\n      optimizer,\n      scaler,\n      train_loader,\n      val_loader,\n      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n      max_epochs_stop=5,  # Early stopping intialization\n      n_epochs=5,\n      min_epoch=5,\n      print_every=10)\n\n  save_train_val_loss_graph(history, perf)\n  save_train_val_acc_graph(history, perf)\n  getConfusionMatrix(model, val_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Out of memory issue\n\n- References\n    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n- The fix\n    - Delete unused tensor, force garbage collection and run `empty_cache()`\n    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB\n    - [mixed precision training & delete checkpoint](https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3)","metadata":{}},{"cell_type":"code","source":"torch.cuda.memory_stats(device)\n# print(torch.cuda.memory_summary(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}