{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Summary\n\n------------------------------------------------------------------------\n\n> Expand to see summary and details","metadata":{}},{"cell_type":"markdown","source":"## Overview and Explanation\n\n1.  This notebook reuses a lot of the [original transfer learning\n    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n    -   Here the focus is on building the new custom model using the\n        CovidNet-CT database.\n2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n    -   is not longer needed for notebook running in kaggle. Remained\n        here for references only\n    -   is where the dataset is being acquired.\n    -   Explanation of various phases in the [CovidNet-CT ML\n        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n        -   train phase is train phase\n        -   test phase is validation phase\n        -   infer phase is test phase\n3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n    -   refers to the way [CovidNet-CT preprocess its\n        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n        to use PyTorch\n    -   Two highlights\n        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n            used by the imagenet model\n        -   the image is cropped to the bounding box provided with the\n            dataset before resize to 512x512\n4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n    -   refers to [how CovidNet-CT\n        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n    -   This part is almost identical to the original transfer learning\n        model notebook.","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Import and Deterministic Setup\n\n------------------------------------------------------------------------\n\nAll modules will be imported here including modules used in the\n[Playground](#playground) section","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\n%env CUDA_LAUNCH_BLOCKING=1\nimport random\nimport numpy as np\nimport torch\n\nfrom os import path\nimport math\nimport sys\n\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, classification_report #, accuracy_score, precision_score, recall_score, f1_score\n\nimport torchvision\nfrom torchvision import models, transforms #, datasets\nimport matplotlib.pyplot as plt\n\nfrom torch import optim\nfrom torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Linear, CrossEntropyLoss\nfrom torch.optim import lr_scheduler\nimport time\nimport torch.nn.functional as F\n\nimport gc\nimport pandas as pd\nimport enum\nfrom enum import Enum\n\n# ensure reproducibility across different executions\n# https://pytorch.org/docs/stable/notes/randomness.html\n# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\nSEED = 18\ndef seed_everything(seed=18):\n    random.seed(seed)\n    %env PYTHONHASHSEED=$seed\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n#torch.set_deterministic(True)\ntorch.use_deterministic_algorithms(True)\n%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n        \n# https://www.kaggle.com/code/manabendrarout/vision-transformer-vit-pytorch-on-tpus-train/notebook\ndef is_tpu_avail():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        TPU_DETECTED = True\n    except:\n        pass\n\n    return TPU_DETECTED\n\n\ndef is_gpu_avail():\n    GPU_DETECTED = False\n    try:\n        GPU_DETECTED = torch.cuda.is_available()\n    except:\n        pass\n\n    return GPU_DETECTED","metadata":{"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","execution":{"iopub.status.busy":"2023-06-10T12:55:34.37674Z","iopub.execute_input":"2023-06-10T12:55:34.377142Z","iopub.status.idle":"2023-06-10T12:55:37.642474Z","shell.execute_reply.started":"2023-06-10T12:55:34.377054Z","shell.execute_reply":"2023-06-10T12:55:37.640908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Data Preprocessing\n\n------------------------------------------------------------------------\n\n-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset class to load CovidNet data\n\n- Various references used: \n  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10","metadata":{}},{"cell_type":"code","source":"class CovidNetDataset(Dataset):\n    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n        super().__init__()\n        self.img_dir = img_dir\n        self.split_files = split_files\n        self.size = 0\n        self.limit_size = limit_size\n\n        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n        self.classes = self.get_classes()\n        self.stradify_removal_based_on_limit()\n        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n        self.transform = transform\n\n    def __len__(self):\n        return self.size # len(self.imgs)\n\n    def __getitem__(self, index):\n        # filename = self.df[index, \"FILENAME\"]\n        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n        # image = Image.open(os.path.join(self.img_dir, filename))\n\n        label = self.targets[index]\n        with open(self.imgs[index], \"rb\") as f:\n            image = Image.open(f)\n            image = image.crop(self.bboxes[index])\n            image = image.copy()\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, label\n\n    \n    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n    def find_classes(self):\n        class_names = ['Normal', 'Pneumonia', 'COVID-19']\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(class_names)}\n        return class_names, class_to_idx\n\n\n    def get_classes(self, csv_path=None):\n        \"\"\"Returns class name array (that corresponds to targets array) and class_to_idx.\n        See :class:`CovidNetDataset` for details.\n        \"\"\"\n\n        # hard code classes as the order are not alphabetic\n        all_class = ['Normal', 'Pneumonia', 'COVID-19']\n        classes = []\n\n        # class_to_idx = {cls_name: i for i, cls_name in enumerate(all_class)}\n        idx_to_class = {i : cls_name for i, cls_name in enumerate(all_class)}\n\n        for idx in self.targets:\n            classes.extend(idx_to_class[idx])\n\n        return classes\n\n\n    def get_all_split_file_data(self):\n        files, classes, bboxes = [], [], []\n        for split_file in self.split_files:\n            f, cls, bb = self.get_data_from_split_file(split_file)\n            files.extend(f)\n            classes.extend(cls)\n            bboxes.extend(bb)\n        return files, classes, bboxes\n\n\n    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n    def get_data_from_split_file(self, split_file):\n        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n        files, classes, bboxes = [], [], []\n        with open(split_file, 'r') as f:\n            for line in f.readlines():\n                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n                files.append(path.join(self.img_dir, fname))\n                classes.append(int(cls))\n                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n                self.size += 1\n        return files, classes, bboxes\n    \n\n    '''Try to do stratified removal based on limit count if it is specified'''\n    def stradify_removal_based_on_limit(self):\n        _, class_to_idx = self.find_classes()\n        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n            return\n        \n        total_remove_count = self.size - self.limit_size\n        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n        for idx, count in occurrence.items():\n            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n        \n        print(f\"occurrence= {occurrence}\")\n        print(f\"target_remove_count= {target_remove_count}\")\n        \n        for i in reversed(range(len(self.targets))):\n            idx = self.targets[i]\n            if target_remove_count[idx] > 0:\n                del self.targets[i]\n                del self.imgs[i]\n                del self.bboxes[i]\n                target_remove_count[idx] -= 1\n                self.size -= 1\n                \n                \n                \n# https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\nclass CovidNetDatasetWithPath(CovidNetDataset):\n    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n        super().__init__(img_dir, split_files, limit_size, transform)\n\n\n    \"\"\"Custom dataset that includes image file paths. Originally extends torchvision.datasets.ImageFolder\n    Now adapted to extend CovidNetDataset\n    \"\"\"  \n    def __getitem__(self, index):\n        original_tuple = super().__getitem__(index) \n        path = self.imgs[index]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:37.64731Z","iopub.execute_input":"2023-06-10T12:55:37.647836Z","iopub.status.idle":"2023-06-10T12:55:37.682129Z","shell.execute_reply.started":"2023-06-10T12:55:37.647806Z","shell.execute_reply":"2023-06-10T12:55:37.681032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spliting dataset into train, val, test\n\n-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nCURR_DIR = \"/kaggle/working\"\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nMAX_COUNT = 10000\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_COUNT)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\n_, _, _, _, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\n# dataset size\ntrain_data_size = len(train_index)\nvalid_data_size = len(val_index)\ntest_data_size = len(test_index)\n\nprint(f\"First 10 train_index: {train_index[:10]}\")\nprint(f\"length of train_index: {train_data_size}\\n\")\nprint(f\"First 10 val_index: {val_index[:10]}\")\nprint(f\"length of val_index: {valid_data_size}\\n\")\nprint(f\"First 10 test_index: {test_index[:10]}\")\nprint(f\"length of test_index: {test_data_size}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:37.686922Z","iopub.execute_input":"2023-06-10T12:55:37.689649Z","iopub.status.idle":"2023-06-10T12:55:41.659664Z","shell.execute_reply.started":"2023-06-10T12:55:37.689607Z","shell.execute_reply":"2023-06-10T12:55:41.658686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying transforms to dataset","metadata":{}},{"cell_type":"markdown","source":"### Define a wrapper dataset\n\n- This is to have the flexibility of applying different transforms to each of the splitted dataset \n- References \n    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)","metadata":{}},{"cell_type":"code","source":"class WrapperDataset:\n    def __init__(self, dataset, transform=None, target_transform=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        image, label = self.dataset[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)\n    \nclass WrapperDatasetWith3Items:\n    def __init__(self, dataset, transform=None, target_transform=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        image, label, imgPth = self.dataset[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label, imgPth\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.665107Z","iopub.execute_input":"2023-06-10T12:55:41.667729Z","iopub.status.idle":"2023-06-10T12:55:41.701503Z","shell.execute_reply.started":"2023-06-10T12:55:41.66767Z","shell.execute_reply":"2023-06-10T12:55:41.699877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the transforms\n\n- References for mean and std of images \n    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n    - Bounding box causing issue when batching as stacking don’t work with\n    different size \n        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`","metadata":{}},{"cell_type":"code","source":"covidnet_std_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n])\n\ncovidnet_train_transform = transforms.Compose([\n    transforms.RandomChoice(transforms=[\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=.3, hue=.3),\n        transforms.RandomPerspective(distortion_scale=0.4),\n        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n    ])\n\nimage_transforms = {\n    'train': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ]),\n    'val': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'test': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'playground': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.703094Z","iopub.execute_input":"2023-06-10T12:55:41.703679Z","iopub.status.idle":"2023-06-10T12:55:41.720295Z","shell.execute_reply.started":"2023-06-10T12:55:41.703642Z","shell.execute_reply":"2023-06-10T12:55:41.719286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Dataset Loader","metadata":{}},{"cell_type":"code","source":"seed_everything(SEED)\nBATCH_SIZE = 16\n\ntrain_sampler = SubsetRandomSampler(train_index)\nval_sampler = SubsetRandomSampler(val_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\ntest_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n\nclass_names, class_to_idx = full_dataset.find_classes()\nprint(class_names)\nprint(class_to_idx)\n\nif is_tpu_avail():\n    device = 'TPU'\nelif is_gpu_avail():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\nprint(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.722034Z","iopub.execute_input":"2023-06-10T12:55:41.722737Z","iopub.status.idle":"2023-06-10T12:55:41.802465Z","shell.execute_reply.started":"2023-06-10T12:55:41.722671Z","shell.execute_reply":"2023-06-10T12:55:41.801412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Custom Model\n\n------------------------------------------------------------------------\n\n- The design of this custom model is illustrated in a draw.io diagram \n    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"### Links\n\n-   [10 CNN Architecture\n    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n    -   [Visualizing pytorch\n        models](https://github.com/szagoruyko/pytorchviz)\n-   Main model building references\n    -   The [CT-3A github\n        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n        -   [tensorflow pretrained\n            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n        -   How to [convert tensorflow checkpoints into pytorch\n            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n            -   [pytorch\n                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n        -   [pytorch\n            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n            about combining two existing models\n    -   [Pytorch resnext50\n        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n    -   [pytorch beginner tutorial on building\n        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n-   Other model building references\n    -   [Custom\n        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n    -   [Resnest convolution block\n        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram","metadata":{}},{"cell_type":"markdown","source":"## Components","metadata":{}},{"cell_type":"code","source":"# modified from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L46\nclass Conv2d_BN(Module):\n    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, groups=1, acti=True):\n        super().__init__() # same as super(Conv2d_BN, self).__init__()\n        if acti:\n            self.conv2d_bn = Sequential(\n                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n                BatchNorm2d(out_channels),\n                LeakyReLU(0.2, inplace=True)\n            )\n        else:\n            self.conv2d_bn = Sequential(\n                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n                BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        return self.conv2d_bn(x)\n\n    def out_channels(self):\n      return next(self.conv2d_bn.children()).out_channels\n\n\n# Taken from https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py#L63\nclass StemBlock(Module):\n    def __init__(self, in_channels=3):\n        super().__init__()\n        # For simplicity Sequential module can be used here, explicitly name every layer for practise and readibility\n        self.conv1 = Conv2d_BN(in_channels, out_channels=64, kernel=7, stride=2, padding=3)\n        self.maxpool1 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n        self.conv2 = Conv2d_BN(in_channels=self.conv1.out_channels(), out_channels=80, kernel=1)\n        self.conv3 = Conv2d_BN(in_channels=self.conv2.out_channels(), out_channels=192, kernel=3, padding=1)\n        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n\n    def forward(self, x):\n        # N x 3 x 224 x 224\n        x = self.conv1(x)\n        # N x 64 x 112 x 112\n        x = self.maxpool1(x)\n        # N x 64 x 56 x 56\n        x = self.conv2(x)\n        # N x 80 x 56 x 56\n        x = self.conv3(x)\n        # N x 192 x 56 x 56\n        return self.maxpool2(x)\n        # N x 192 x 28 x 28\n\n    def out_channels(self):\n      # unable to access output size of MaxPool2d, use hard-coded formula instead\n      return math.floor(self.conv3.out_channels()/2)\n\n\n# https://stackoverflow.com/questions/4950155/objects-as-keys-in-python-dictionaries\nclass BlockType(Enum):\n    CONV = enum.auto()\n    IDENTITY = enum.auto()\n\n    def __eq__(self, other):\n        return self.name == other.name and self.value == other.value\n\n    def __hash__(self):\n        return hash(f\"{self.name}:{self.value}\")\n\n# Generalize ConvBlock and Identity block as ResidualBlock:\n# https://github.com/maciejbalawejder/Deep-Learning-Collection/blob/main/ConvNets/ResNeXt/resnext_pytorch.py\nclass ResidualBlock(Module):\n    def __init__(self, in_channels, out_channels, block_type: BlockType, stride, cardinatlity=32):\n        super().__init__()\n        assert out_channels % 32 == 0\n        self.C = cardinatlity\n        self.block_type = block_type\n        inner_channels = out_channels // 2\n        self.conv_tower = Sequential(\n            Conv2d_BN(in_channels, inner_channels, kernel=1),\n            Conv2d_BN(inner_channels, inner_channels, kernel=3, stride=stride, padding=1, groups=self.C),\n            Conv2d_BN(inner_channels, out_channels, kernel=1, acti=None)\n        )\n        if self.block_type is BlockType.CONV:\n            self.downsample = Conv2d_BN(in_channels, out_channels, kernel=1, stride=stride, acti=None)\n        self.relu = LeakyReLU(0.2, inplace=True)\n\n    def forward(self, x):\n        out = self.conv_tower(x)\n        if self.block_type is BlockType.CONV:\n            x = self.downsample(x)\n        out = self.relu(torch.add(out,x))\n        return out\n\n    def out_channels(self):\n      # unable to access output size of MaxPool2d, use hard-coded formula instead\n      gen = self.conv_tower.children()\n      last = next(gen)\n      for last in gen: pass\n      return last.out_channels()\n\n\n# taken from https://github.com/reppertj/earworm/blob/a2d8a70085748da5db378f7f5f68ad8c2926a274/modeling/music_metric_learning/modules/inception.py#L93\nclass ReductionBlock(Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        # previously mistakenly thought conv3_pooling_out is this layer out_channel, turn out that the out_channel is the total channels of the 3 layers\n        # assert (out_channels * 8 // 3) % 16 == 0\n        # conv5_out = out_channels * 2 // 3\n        # conv3_pooling_out = out_channels\n\n        assert out_channels % 16 == 0\n        conv5_out = out_channels // 4\n        conv3_pooling_out = conv5_out * 3 // 2\n        self.conv3 = Conv2d_BN(in_channels, conv3_pooling_out, kernel=3, stride=2)\n        self.conv5 = Sequential(\n            Conv2d_BN(in_channels       , conv5_out * 3 // 4, kernel=1),\n            Conv2d_BN(conv5_out * 3 // 4, conv5_out * 7 // 8, kernel=3, padding=1),\n            Conv2d_BN(conv5_out * 7 // 8, conv5_out         , kernel=3, stride=2),\n        )\n        self.pooling = Sequential(\n            MaxPool2d(3, stride=2),\n            Conv2d_BN(in_channels, conv3_pooling_out, kernel=1),\n        )\n\n    def forward(self, x):\n        return torch.cat((self.conv3(x), self.conv5(x), self.pooling(x)), dim=1)\n\n\nIN_CHAN=\"in_chan\"\nOUT_CHAN=\"out_chan\"\nclass CovidNetBlock(Module):\n    def __init__(self, residual_block_layout: dict, out_channels):\n        super().__init__()\n        chan_dict_list = [l for k, v in residual_block_layout.items() for l in v if isinstance(k, BlockType) and isinstance(v, list) and isinstance(l, dict)]\n\n        print(f\"Creating CovidNetBlock with layout length of {len(chan_dict_list)}\")\n        self.blocks = Sequential()\n        for k,v in residual_block_layout.items():\n            for chan_dict in v:\n                # print(f\"DEBUG_LOG - creating ResidualBlock with in_chan:{chan_dict[IN_CHAN]}; out_chan:{chan_dict[OUT_CHAN]}; block_type:{k}\")\n                self.blocks.append(ResidualBlock(chan_dict[IN_CHAN], chan_dict[OUT_CHAN], k, 1))\n        last_out_chan = chan_dict_list[-1][OUT_CHAN] # get last layers output channels count\n        self.blocks.append(ReductionBlock(last_out_chan, out_channels))\n\n    def forward(self, x):\n        return self.blocks(x)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.804228Z","iopub.execute_input":"2023-06-10T12:55:41.80463Z","iopub.status.idle":"2023-06-10T12:55:41.852849Z","shell.execute_reply.started":"2023-06-10T12:55:41.80459Z","shell.execute_reply":"2023-06-10T12:55:41.851684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Full Model","metadata":{}},{"cell_type":"code","source":"# https://onlinegdb.com/t9CIm197r\nclass CovidnetModel(Module):\n    def __init__(\n        self, \n        classes : int = 3,\n        ):\n        super().__init__()\n        \n        self.stem = StemBlock()\n\n        PRE_FC_OUT_CHAN = 192\n        residual_block_layout = {\n            BlockType.CONV:[\n                dict(in_chan=192, out_chan=256),\n                dict(in_chan=256, out_chan=512),\n                dict(in_chan=512, out_chan=1024),\n            ],\n            BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n        }\n        self.blocks = Sequential(\n            CovidNetBlock(residual_block_layout, 192),\n            CovidNetBlock(residual_block_layout, PRE_FC_OUT_CHAN)\n        )\n        \n        self.global_avg_pool = AdaptiveAvgPool2d((1,1))\n        self.fc = Linear(PRE_FC_OUT_CHAN, classes)\n\n    def forward(self, x):\n        # 3 x 224 x 224\n        x = self.stem(x)\n        # 192 x 28 x 28\n        x = self.blocks(x)\n        x = self.global_avg_pool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x","metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa","execution":{"iopub.status.busy":"2023-06-10T12:55:41.854412Z","iopub.execute_input":"2023-06-10T12:55:41.854827Z","iopub.status.idle":"2023-06-10T12:55:41.868178Z","shell.execute_reply.started":"2023-06-10T12:55:41.854779Z","shell.execute_reply":"2023-06-10T12:55:41.867468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Training & Validation\n\n------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"LOG_DIR = os.path.join(CURR_DIR, \"log\")\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\ncurr_model = \"\"\n\ndef log_to_file(txt=None, print_to_console_only=False):\n  if txt is None:\n    txt = ''\n    txt += '\\n'\n  print(txt)\n  if print_to_console_only:\n    return\n  if not path.exists(LOG_DIR):\n    os.mkdir(LOG_DIR)\n  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n  with open(full_path, mode='a') as f:\n    f.write(txt)\n    \n# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n# Make sure to delete any references to tensor. Else this function will not have significant effect\ndef clean_vram():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\ndef name_of_caller(frame=1):\n    \"\"\"\n    Return \"class.function_name\" of the caller or just \"function_name\".\n    \"\"\"\n    frame = sys._getframe(frame)\n    fn_name = frame.f_code.co_name\n    var_names = frame.f_code.co_varnames\n    if var_names:\n        if var_names[0] == \"self\":\n            self_obj = frame.f_locals.get(\"self\")\n            if self_obj is not None:\n                return f\"{type(self_obj).__name__}.{fn_name}\" \n        if var_names[0] == \"cls\":\n            cls_obj = frame.f_locals.get(\"cls\")\n            if cls_obj is not None:\n                return f\"{cls_obj.__name__}.{fn_name}\"\n    return fn_name","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.869595Z","iopub.execute_input":"2023-06-10T12:55:41.870832Z","iopub.status.idle":"2023-06-10T12:55:41.884027Z","shell.execute_reply.started":"2023-06-10T12:55:41.870789Z","shell.execute_reply":"2023-06-10T12:55:41.883337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Functions to Initialize Deep Learning Models","metadata":{}},{"cell_type":"code","source":"model_constructors = {\n  models.alexnet.__name__: models.alexnet, \n  models.squeezenet1_1.__name__: models.squeezenet1_1,\n  models.resnet50.__name__: models.resnet50, \n  models.resnet101.__name__: models.resnet101,\n  models.resnet152.__name__: models.resnet152, \n  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n  models.densenet201.__name__: models.densenet201, \n  models.googlenet.__name__: models.googlenet, \n  models.vgg16.__name__: models.vgg16, \n  models.vgg19.__name__: models.vgg19, \n  models.inception_v3.__name__: models.inception_v3, \n  CovidnetModel.__name__: CovidnetModel,\n}\n\n# This is only available in pytorch v0.13\n# from torchvision.models import *\n# model_weights = {\n#   models.alexnet.__name__: models.AlexNet_Weights.DEFAULT,\n#   models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n#   models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n#   models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n#   models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n#   models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n#   models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n#   models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n#   models.vgg16.__name__: VGG16_Weights.DEFAULT,\n#   models.vgg19.__name__: VGG19_Weights.DEFAULT,\n#   models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n# }\n\n# Experiment around dropout & Learning Rate & different optimizer (Adam)\ndef init_model(name):\n  if not path.exists(RESULT_DIR):\n    os.mkdir(RESULT_DIR)\n\n  clean_vram()\n  seed_everything()\n  \n  # fine-tune pretrain models to our usecase\n  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n  NUM_CLASSES = len(class_names)\n  DROPOUT = 0.5\n  if name == CovidnetModel.__name__:\n    model = model_constructors[name](NUM_CLASSES)\n  else:\n    model = model_constructors[name](True)\n    if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n      num_ftrs = model.classifier[6].in_features\n      model.classifier[6] = Linear(num_ftrs, NUM_CLASSES)\n      # model.classifier[6] = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n    elif name == models.densenet201.__name__:\n      num_ftrs = model.classifier.in_features\n      model.classifier = Linear(num_ftrs, NUM_CLASSES)\n      # model.classifier = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n    elif name == models.squeezenet1_1.__name__:\n      model.classifier = Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n      # model.classifier = Sequential(\n      #   Dropout(DROPOUT),\n      #   Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n      # )\n      model.num_classes = NUM_CLASSES\n    elif name == models.inception_v3.__name__:\n      auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n      model.AuxLogits.fc = Linear(auxLogits_num_ftrs, NUM_CLASSES)\n      # model.AuxLogits.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(auxLogits_num_ftrs, NUM_CLASSES)\n      # )\n      primary_num_ftrs = model.fc.in_features\n      model.fc = Linear(primary_num_ftrs, NUM_CLASSES)\n      # model.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(primary_num_ftrs, NUM_CLASSES)\n      # )\n    else:\n      # resnet, resnext & googlenet\n      num_ftrs = model.fc.in_features\n      model.fc= Linear(num_ftrs, NUM_CLASSES)\n      # model.fc = Sequential(\n      #   Dropout(DROPOUT),\n      #   Linear(num_ftrs, NUM_CLASSES)\n      # )\n\n  model = model.to(device)\n  criterion = CrossEntropyLoss()\n  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  if is_gpu_avail():\n    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n    scaler = torch.cuda.amp.GradScaler()\n  else:\n    raise RuntimeError('This code only support machine with GPU.')\n\n  # print('=====================================')\n  print(f'{name} is initialized')\n  # print('=====================================')\n  # print(model)\n  return model, criterion, optimizer, scaler\n\n# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\ndef save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n  torch.save({\n    'perf_metrics': perf_metrics,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    'history': history,\n    }, model_path)\n\ndef load_model(model, optimizer, scaler, model_path):\n  if not os.path.exists(model_path):\n    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n    return None, model, optimizer, scaler, None, None\n  checkpoint = torch.load(model_path)\n  perf_metrics = checkpoint['perf_metrics']\n  model.load_state_dict(checkpoint['model_state_dict'])\n  # model.load_state_dict(checkpoint['model_state_dict'], strict=False) # https://stackoverflow.com/questions/54058256/runtimeerror-errors-in-loading-state-dict-for-resnet\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n  history = checkpoint['history']\n  total_epoch = len(history) - 1\n  del checkpoint\n\n  return perf_metrics, model, optimizer, scaler, history, total_epoch","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.887814Z","iopub.execute_input":"2023-06-10T12:55:41.888205Z","iopub.status.idle":"2023-06-10T12:55:41.912593Z","shell.execute_reply.started":"2023-06-10T12:55:41.888177Z","shell.execute_reply":"2023-06-10T12:55:41.911485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Function to Train Models","metadata":{}},{"cell_type":"code","source":"# training and validation loops\ndef train(model,\n    criterion,\n    optimizer,\n    scaler,\n    train_dataloader,\n    valid_dataloader,\n    model_path,\n    max_epochs_stop=10,\n    n_epochs=400,\n    min_epoch=300,\n    print_every=1):\n    \n    epochs_no_improve = 0\n    perf = {\n        'best_epoch': 0,\n        'valid_loss_min': np.Inf,\n        'valid_best_acc': 0,\n    }\n    total_epoch = 0\n\n    try:\n        if os.path.exists(model_path):\n            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n        else:\n            history = []\n            log_to_file(f'Starting Training from Scratch.\\n')\n    except:\n        history = []\n        log_to_file(f'exception: start from scratch.\\n')\n\n    overall_start = time.time()\n    if total_epoch >= n_epochs:\n        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n        history = pd.DataFrame(\n            history,\n            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n        return model, history, perf\n\n    seed_everything()\n\n    # Main loop - continue training on where we left off if there's a saved model\n    for epoch in range(total_epoch, n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = time.time()\n        for ii, (data, target) in enumerate (train_dataloader):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            with torch.cuda.amp.autocast():\n                if curr_model is models.inception_v3.__name__:\n                    # https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n                    output, aux_output = model(data)\n                    loss1 = criterion(output, target)\n                    loss2 = criterion(aux_output, target)\n                    loss = loss1 + 0.4*loss2\n                else:\n                    output = model(data)\n                    loss = criterion(output, target)\n            # loss.backward()\n            # optimizer.step()\n            scaler.scale(loss).backward()\n            \n            # only for resnet152 to solve exploding gradient issue https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch\n            if curr_model is models.resnet152.__name__:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n            \n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n            \n            # cleanup to save VRAM\n            del data, target\n#             clean_vram()\n\n        # After training loops ends, start validation\n        else:\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_dataloader:\n                    if is_gpu_avail():\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                    \n                    # cleanup to save VRAM\n                    del data, target\n#                     clean_vram()\n                train_loss = train_loss / train_data_size\n                valid_loss = valid_loss / valid_data_size\n                train_acc = train_acc / train_data_size\n                valid_acc = valid_acc / valid_data_size\n                history.append([train_loss, valid_loss,train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    log_to_file(f'Epoch: {epoch}', True)\n                    log_to_file(\n                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n                        True\n                    )\n                    log_to_file(\n                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n                        True\n                    )\n          \n                if valid_loss < perf['valid_loss_min']:\n                    epochs_no_improve = 0\n                    perf['best_epoch'] = epoch\n                    perf['valid_loss_min'] = valid_loss\n                    perf['valid_best_acc'] = valid_acc\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n                else:\n                    # disable early stopping and always save, remove this to reenable early stopping\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n#                     epochs_no_improve += 1\n#                     # Trigger early stopping\n#                     if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n#                         log_to_file(\n#                             f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n#                         )\n#                         total_time = time.time() - overall_start\n#                         log_to_file(\n#                             f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n#                         )\n#                         log_to_file()\n\n#                         # Load the best state from saved model\n#                         _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n#                         # save the full history\n#                         save_model(perf, model, optimizer, scaler, history, model_path)\n\n#                         # Format history\n#                         history = pd.DataFrame(\n#                             history,\n#                             columns=[\n#                                 'train_loss', 'valid_loss', 'train_acc',\n#                                 'valid_acc'\n#                             ])\n#                         return model, history, perf\n    \n    total_time = time.time() - overall_start\n    log_to_file(\n        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n    )\n    log_to_file(\n        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n    )\n    log_to_file()\n\n    # Load the best state from saved model\n    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n    # save the full history\n    save_model(perf, model, optimizer, scaler, history, model_path)\n\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n    \n    return model, history, perf\n\n\ndef save_train_val_loss_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_loss', 'valid_loss']:\n      plt.plot(\n          history[c], label=c)\n\n  title = f'{curr_model} - Training and Validation Losses'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Losses')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')\n\n\ndef save_train_val_acc_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_acc', 'valid_acc']:\n      plt.plot(\n          100 * history[c], label=c)\n      \n  title = f'{curr_model} - Training and Validation Accuracy'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Accuracy')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.91437Z","iopub.execute_input":"2023-06-10T12:55:41.916213Z","iopub.status.idle":"2023-06-10T12:55:41.954154Z","shell.execute_reply.started":"2023-06-10T12:55:41.916183Z","shell.execute_reply":"2023-06-10T12:55:41.953057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Functions to Visualize Prediction\n- For multiclass, confusion matrix calculation is different from binary classification.\n- Use sklearn confusion_metrics libs for all the calculations\n\n    - https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n    - https://www.analyticsvidhya.com/blog/2021/06/confusion-matrix-for-multi-class-classification/#Confusion_Matrix_for_Multi-Class_Classification","metadata":{}},{"cell_type":"code","source":"def imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # mean = np.array([0.485, 0.456, 0.406])\n    # std = np.array([0.229, 0.224, 0.225])\n    # inp = std * inp + mean\n    # inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=[15, 15])\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n# confusion matrix \ndef getMultiClsConfMat(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n    model.eval()\n    nb_classes = len(class_names)\n\n    # Initialize the prediction and label lists(tensors)\n    predlist=torch.zeros(0,dtype=torch.long, device='cpu')\n    lbllist=torch.zeros(0,dtype=torch.long, device='cpu')\n\n    with torch.no_grad():\n        for i, (inputs, classes) in enumerate(dataloader):\n            inputs = inputs.to(device)\n            classes = classes.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            # Append batch prediction results\n            predlist=torch.cat([predlist,preds.view(-1).cpu()])\n            lbllist=torch.cat([lbllist,classes.view(-1).cpu()])\n\n    labels = lbllist.numpy()\n    preds = predlist.numpy()\n    conf_mat = confusion_matrix(labels, preds)\n    \n    # https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format?answertab=scoredesc#tab-top\n    clsf_report = pd.DataFrame(classification_report(labels, preds, target_names=class_names, output_dict=True)).transpose()\n    summary_report = clsf_report.to_csv(None)\n#     summary_report = classification_report(labels, preds, target_names=class_names)\n        \n    log_to_file(None, print_to_console_only)\n    category = 'Test' if is_test else 'Validation'\n    log_to_file('=====================', print_to_console_only)\n    log_to_file(f'{category} Results ', print_to_console_only)\n    log_to_file('=====================', print_to_console_only)\n    log_to_file('Confusion Matrix: ', print_to_console_only)\n    log_to_file(conf_mat, print_to_console_only)\n    log_to_file(None, print_to_console_only)\n\n    log_to_file('Classification Report: ', print_to_console_only)\n    log_to_file(summary_report, print_to_console_only)\n    log_to_file(None, print_to_console_only)\n    \n    return conf_mat\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# https://github.com/frgfm/torch-cam/issues/195\ndef getPredProbsForGradCam(model, dataloader, count, isSeeded=True):\n    if isSeeded:\n        seed_everything()\n\n    model.eval()\n    with torch.no_grad():\n        for i, (data, target, _) in enumerate(dataloader):\n            if i >= count:\n                break\n            if is_gpu_avail():\n                data, _ = data.cuda(), target.cuda()\n            pred_logits_tensor = model(data)\n            print(pred_logits_tensor)\n            print(f'Is any weight is NaN: {torch.any(torch.isnan(pred_logits_tensor.parameters()))}')\n\n#     test_batch = torch.stack([img for img in img_list])\n#     pred_logits_tensor = model(test_batch)\n    return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.955507Z","iopub.execute_input":"2023-06-10T12:55:41.956788Z","iopub.status.idle":"2023-06-10T12:55:41.978879Z","shell.execute_reply.started":"2023-06-10T12:55:41.956743Z","shell.execute_reply":"2023-06-10T12:55:41.977872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run all models - Init Models + Training","metadata":{}},{"cell_type":"code","source":"model_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n    CovidnetModel.__name__, #11\n]\n\nfor i in range(0, len(model_list)):\n  # https://github.com/pytorch/pytorch/issues/50198\n  # skipped these because cannot use deterministic algorithm\n#   skip_model = [0, 1, 5, 8, 9, 10]\n#   skip_model = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n#   if i in skip_model:\n#     continue\n  if model_list[i] is not models.resnet101.__name__:\n    continue\n  curr_model = model_list[i]\n\n  # Initialize model, criterion and optimizer\n  model, criterion, optimizer, scaler = init_model(curr_model)\n\n#   Training & Validation\n  model, history, perf = train(\n      model,\n      criterion,\n      optimizer,\n      scaler,\n      train_loader,\n      val_loader,\n      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n      max_epochs_stop=5,  # Early stopping intialization\n      n_epochs=100,\n      min_epoch=100,\n      print_every=10)\n\n  save_train_val_loss_graph(history, perf)\n  save_train_val_acc_graph(history, perf)\n#   getConfusionMatrix(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T12:55:41.980168Z","iopub.execute_input":"2023-06-10T12:55:41.980626Z","iopub.status.idle":"2023-06-10T15:12:03.530266Z","shell.execute_reply.started":"2023-06-10T12:55:41.980586Z","shell.execute_reply":"2023-06-10T15:12:03.529303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Download Results\n<a href=\"/kaggle/working/result/CovidnetModel - Training and Validation Accuracy.png\"> CovidnetModel - Training and Validation Accuracy.png </a> </br>\n<a href=\"/kaggle/working/result/CovidnetModel - Training and Validation Losses.png\"> CovidnetModel - Training and Validation Losses.png </a> </br>\n<a href=\"/kaggle/working/result/CovidnetModel.pt\"> CovidnetModel.pt </a> </br>","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/log\n!ls\nfrom IPython.display import FileLink\n# FileLink(r'CovidnetModel - Training and Validation Accuracy.png')\n# FileLink(r'CovidnetModel - Training and Validation Losses.png')\nFileLink(r'googlenet.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Final Evaluation\n\n------------------------------------------------------------------------\n- After selecting the best hyperparameters and models, use the model to predict the result on the test images\n    - Get the confusion matrix and prediction similar to what we did for validation images\n- Run GradCam to visualize which regions in the image the model are activated","metadata":{}},{"cell_type":"markdown","source":"## Run Test","metadata":{}},{"cell_type":"code","source":"def reloadModel():\n  saved_model_path = f'{path.join(RESULT_DIR, curr_model)}.pt'\n  if os.path.exists(saved_model_path):\n    model, criterion, optimizer, scaler = init_model(curr_model)\n    return load_model(model, optimizer, scaler, saved_model_path)\n  else:\n    return None, None, None, None, None, None\n\nmodel_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n    CovidnetModel.__name__, #11\n]\n\nfor i in range(0, len(model_list)):\n#   skip_model = [0, 1, 4, 8, 9, 10]\n#   if i in skip_model:\n#     continue\n  if model_list[i] is not models.resnet101.__name__:\n    continue\n  curr_model = model_list[i]\n  _, model, _, _, _, _ = reloadModel()\n  seed_everything()\n  if model:\n    trainable_parameters = count_parameters(model)\n    print(f'model: {curr_model}, number of trainable parameters: {trainable_parameters}')\n\n  getMultiClsConfMat(model, val_loader, False, False, True)\n#   getMultiClsConfMat(model, test_loader, True, False, True)\n# visualize_test_prediction(model)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T15:25:13.710144Z","iopub.execute_input":"2023-06-10T15:25:13.710537Z","iopub.status.idle":"2023-06-10T15:25:47.116479Z","shell.execute_reply.started":"2023-06-10T15:25:13.710505Z","shell.execute_reply":"2023-06-10T15:25:47.11539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run GradCam\n- This repo is used instead due to much better support with pytorch - https://github.com/frgfm/torch-cam\n- [This reference](https://github.com/jacobgil/pytorch-grad-cam) is obselete as need to find the layer manually, and facing `IndexError: index 2 is out of bounds for dimension 1 with size 2`","metadata":{}},{"cell_type":"code","source":"CURR_DIR = \"/kaggle/working\"\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\n\n# !pip install git+https://github.com/frgfm/torch-cam.git#egg=torchcam\n!pip install torchcam==0.3.0\nfrom torchvision.io.image import read_image\nfrom torchvision.transforms.functional import normalize, resize, to_tensor, to_pil_image\nfrom torchcam.cams import SmoothGradCAMpp\n# from torchcam.methods import SmoothGradCAMpp\nfrom torchcam.utils import overlay_mask\nfrom PIL import ImageOps\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport math\n\n\"\"\" \nClass activation explorer using TorchCAM.\nIterate through validation database and overlay images with class activation \nheatmap.\n\nReference: https://github.com/frgfm/torch-cam \n\"\"\"\ndef runGradCam():\n    _, model, _, _, _, _ = reloadModel()\n\n    if not model:\n        return\n\n#     torch.autograd.set_detect_anomaly(True)\n    MAX_COUNT = 10000\n    full_dataset_with_path = CovidNetDatasetWithPath(DATASET_DIR, SPLIT_FILES, MAX_COUNT)\n    cam_loader = DataLoader(WrapperDatasetWith3Items(full_dataset_with_path, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\n    class_names, _ = full_dataset_with_path.find_classes()\n    \n    NUM_ITER = 1\n    pred_probs = getPredProbsForGradCam(model, cam_loader, NUM_ITER)\n    cam_extractor = SmoothGradCAMpp(model) # this steps already does model.eval()\n    \n    seed_everything()\n\n    for i, (img, cls, ori_path) in enumerate(cam_loader):\n        if i >= NUM_ITER:\n            break\n\n#         print(f\"img={img}, img shape={img.shape}, cls={cls}, ori_path={ori_path}\")\n        if is_gpu_avail():\n            img = img.cuda()\n        # Preprocess your data and feed it to the model\n#         out = model(img.unsqueeze(0))\n        out = model(img)\n#         print(f\"runGradCam out={out}\")\n        # Retrieve the CAM by passing the class index and the model output\n        cls_idx = cls.item()\n        activation_map = cam_extractor(cls_idx, out)\n\n        # Resize the CAM and overlay it\n        # https://matplotlib.org/stable/tutorials/colors/colormaps.html#miscellaneous\n        ori_img = to_pil_image(img.squeeze(0))\n        activation_map = to_pil_image(activation_map[0], mode='F')\n        plt.imshow(activation_map); plt.axis('off'); plt.tight_layout(); plt.show()\n        \n#         print(f\"img shape={img.shape}, img type={img.dtype}, act_map type={activation_map.dtype}\")\n#         result = overlay_mask(ori_img, activation_map, colormap='jet', alpha=0.5)\n        result = overlay_mask(ori_img, activation_map, alpha=0.5)\n\n        # Display it\n        # https://matplotlib.org/stable/gallery/axes_grid1/simple_axesgrid.html\n        fig = plt.figure(figsize=(20., 20.))\n        grid = ImageGrid(fig, 111,    # similar to subplot(111)\n                                        nrows_ncols=(1, 2),    # creates 1x2 grid of axes\n                                        axes_pad=0.1,    # pad between axes in inch.\n                                        )\n        for ax, im in zip(grid, [ori_img, result]):\n            # Iterating over the grid returns the Axes.\n            cls_str = class_names[cls_idx]\n            ax.set_title(\"{:.4f}% Covid, \\n{:.4f}% NonCovid, Actual:{}, ImagePath:{}\".format(100*pred_probs[i,0],\n                100*pred_probs[i,1],\n                cls_str, ori_path), fontsize=10)\n            ax.imshow(im)\n        title = f'{curr_model} - GradCam++ Visualization'\n        full_path = os.path.join(RESULT_DIR, f'{title}.png')\n        plt.savefig(full_path)\n        plt.show()\n\nBATCH_SIZE = 1\n# curr_model = CovidnetModel.__name__\ncurr_model = models.resnet101.__name__\nrunGradCam()","metadata":{"execution":{"iopub.status.busy":"2023-06-10T15:25:52.330254Z","iopub.execute_input":"2023-06-10T15:25:52.330652Z","iopub.status.idle":"2023-06-10T15:26:10.45432Z","shell.execute_reply.started":"2023-06-10T15:25:52.330616Z","shell.execute_reply":"2023-06-10T15:26:10.45261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/frgfm/torch-cam.git#egg=torchcam\n!pip install torchcam\nfrom torchvision.io.image import read_image\nfrom torchvision.transforms.functional import normalize, resize, to_tensor, to_pil_image\nfrom torchcam.methods import SmoothGradCAMpp\nfrom torchcam.utils import overlay_mask\nfrom PIL import ImageOps\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport math\n\nMAX_COUNT = 10000\nfull_dataset_with_path = CovidNetDatasetWithPath(DATASET_DIR, SPLIT_FILES, MAX_COUNT)\n# cam_loader = DataLoader(WrapperDataset(full_dataset_with_path, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\n# NUM_ITER = 2\nprint(next(iter(full_dataset_with_path)))\n# pred_probs = getPredProbsForGradCam(model, cam_loader, NUM_ITER)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(next(iter(WrapperDatasetWith3Items(full_dataset_with_path, image_transforms['val']))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Playground\n\n------------------------------------------------------------------------\n\n> Testbed that is not compulsary for any part of this notebook","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Models using PytorchViz\n\n-   https://pytorch.org/docs/stable/nn.html\n-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n-   [Mandrin explanation of pytorch resnet\n    code](https://www.jianshu.com/p/90d61f53d15d)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n%pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\nfrom torchviz import make_dot, make_dot_from_trace","metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from torchinfo import summary\n%pip install -U torchsummary\nfrom torchsummary import summary\n\nif is_gpu_avail():\n    device = torch.device('cuda')\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = torch.device('cpu')\n\n\n# model = models.googlenet()\n# summary(model, (3, 299, 299))\n\n# model = ResidualBlock(192, 192, BlockType.IDENTITY, 1)\n# summary(model, (192, 28, 28))\n\n# model = ResidualBlock(192, 256, BlockType.CONV, 1) # also works if 192,192\n# summary(model, (192, 28, 28))\n\n# model = ResidualBlock(256, 320, BlockType.CONV, 1) # also works if 192,192\n# summary(model, (256, 28, 28))\n\n# model = ReductionBlock(1024, 192)\n# summary(model, (1024, 28, 28))\n\n# residual_block_layout = {\n#     BlockType.CONV:[\n#         dict(in_chan=192, out_chan=256),\n#         dict(in_chan=256, out_chan=512),\n#         dict(in_chan=512, out_chan=1024),\n#     ],\n#     BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n# }\n# model = CovidNetBlock(residual_block_layout, 192) \n# summary(model, (192, 28, 28))\n\nmodel = CovidnetModel()\nsummary(model, (3, 224, 224))\n\n# model = models.resnext50_32x4d()\n# summary(model, (3, 224, 224))\n\n# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n# y = model(x)\n# dot = make_dot(y, params=dict(model.named_parameters()))\n\n# dot.format = \"png\"\n# dot.render(render_model_pic_file)\n# files.download(f\"{render_model_pic_file}.{dot.format}\")","metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate Image in Dataset","metadata":{}},{"cell_type":"code","source":"import subprocess\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\nDATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\nDATASET_NAME = \"COVIDx_CT-3A\"\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n\n# count number of images\n# !ls -Uba1 /content/data/3A_images | grep -c png\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\nprint(f\"number of images: {output.decode()}\")\n\n# list first 10 images\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\nfirst_10_lines = output.decode()\n# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\nimg_list = first_10_lines.split('\\n')\nfirst_png = next(filter(lambda img: \"png\" in img, img_list), None)\nprint(f\"first_png: {first_png}\\n\")","metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the first test image, unbounded followed by bounded\nimport torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as torch_func_trans\nfrom PIL import Image\n\n# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\ndef get_data_from_split_file(split_file):\n    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n    files, classes, bboxes = [], [], []\n    with open(split_file, 'r') as f:\n        for line in f.readlines():\n            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n            files.append(path.join(DATASET_DIR, fname))\n            classes.append(int(cls))\n            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n    return files, classes, bboxes\n\ndef bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n    top = ymax\n    left = xmin\n    height = ymax - ymin\n    width = xmax - xmin\n    return top, left, height, width\n\nfiles, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\nfirst_file_tuple = (files[0], classes[0], bbox[0])\n\nprint(f\"first image: {first_file_tuple[0]}\")\nif 0:\n    # this way of reading image is deprecated\n    img = plt.imread(first_file_tuple[0])\n\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"unbounded\")\n    _ = plt.imshow(img)\n    _ = plt.axis('off')\n\n    torch_img = torch.from_numpy(img)\n    print(f\"torch_img size: {torch_img.size()}\")\n    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n    print(f\"pytorch_size: {pytorch_size}\")\n\n    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"bounded\")\n    _ = plt.imshow(cropped_img)\n    _ = plt.axis('off')\n\n# This is the recommended method for opening image\n# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\nwith Image.open(first_file_tuple[0]) as im:\n    print(\"\\n\")\n    print(\"Unbounded image\")\n    IMG = im\n    display(im)\n\n    print(\"\\n\")\n    print(\"Bounded image\")\n    display(im.crop(first_file_tuple[2]))\n\n    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n    print(\"\\n\")\n    print(\"Out of Scope Unbounded image\")\n    display(IMG)\n    ","metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate metadata.csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.expand_frame_repr', False)\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nmeta_df = pd.read_csv(METADATA_CSV)\nprint(\"First 5 rows in metadata.csv0\")\nprint(meta_df.head(5))\nprint(f\"classes: {sorted(meta_df['finding'].unique())}\")\n\nunverified = meta_df['verified finding'].eq('No').sum()\nverified = meta_df['verified finding'].eq('Yes').sum()\nprint(f\"Not verified: {unverified}\")\nprint(f\"Verified: {verified}\")\n\nunverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\nassert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\nprint(f\"first unverified ID: {unverified_patient_ids[0]}\")\n\nverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\nassert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\nprint(f\"first verified ID: {verified_patient_ids[0]}\")\n\nimgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\nprint(f\"total images: {len(imgs)}\")\nprint(f\"first 10 images: {imgs[:10]}\")\nimgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\nprint(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")","metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigating split dataset","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\nSEED = 18\nseed_everything(SEED)\nBATCH_SIZE = 64\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n# train_index, test_index = next(\n#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n#         dummy_X, full_dataset.targets\n#     )\n# )\n\nprint('*'*50)\nprint(\"temp_train\")\nprint('*'*50)\nprint(f\"First 10 index: {temp_train_index[:10]}\")\nprint(f\"First 10 label: {y_remaining[:10]}\")\nprint(f\"length of index: {len(temp_train_index)}\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {y_remaining.count(0)}\")\nprint(f\"Pneunomia: {y_remaining.count(1)}\")\nprint(f\"Covid-19: {y_remaining.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"test\")\nprint('*'*50)\nprint(f\"First 10 index: {test_index[:10]}\")\nprint(f\"First 10 label: {Y_test[:10]}\")\nprint(f\"length of index: {len(test_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_test.count(0)}\")\nprint(f\"Pneunomia: {Y_test.count(1)}\")\nprint(f\"Covid-19: {Y_test.count(2)}\")\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\nX_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\nprint()\nprint('*'*50)\nprint(\"train\")\nprint('*'*50)\nprint(f\"First 10 index: {train_index[:10]}\")\nprint(f\"First 10 label: {Y_train[:10]}\")\nprint(f\"length of index: {len(train_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_train.count(0)}\")\nprint(f\"Pneunomia: {Y_train.count(1)}\")\nprint(f\"Covid-19: {Y_train.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"val\")\nprint('*'*50)\nprint(f\"First 10 index: {val_index[:10]}\")\nprint(f\"First 10 label: {Y_val[:10]}\")\nprint(f\"length of index: {len(val_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_val.count(0)}\")\nprint(f\"Pneunomia: {Y_val.count(1)}\")\nprint(f\"Covid-19: {Y_val.count(2)}\")","metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Dataloader","metadata":{}},{"cell_type":"code","source":"import pprint\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # mean = np.array([0.485, 0.456, 0.406])\n    # std = np.array([0.229, 0.224, 0.225])\n    # inp = std * inp + mean\n    # inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=[15, 15])\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nseed_everything(19)\nclass_names, _ = full_dataset.find_classes()\ndata, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n# out = torchvision.utils.make_grid(data)\n# imshow(out)\n# pp = pprint.PrettyPrinter(compact=True)\n# pp.pprint([class_names[x] for x in classes])\n\nprint(f\"Class: {classes[0]}\")\nout = data[0]\nimshow(out)","metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying Avalance","metadata":{}},{"cell_type":"code","source":"# https://changhsinlee.com/colab-import-python/\n!pip install requests\n!pip install avalanche-lib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\n# Save datagenerators as file to colab working directory\n# If you are using GitHub, make sure you get the \"Raw\" version of the code\nurl = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\nr = requests.get(url)\n\n# make sure your filename is the same as how you want to import \nwith open('pytorchcv_models.py', 'w') as f:\n    f.write(r.text)\n\n# now we can import\nimport pytorchcv_models as pycv\nfrom types import SimpleNamespace\n\nargs = SimpleNamespace()\nargs.cuda = 0\npycv.main(args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"LOG_DIR = os.path.join(CURR_DIR, \"log\")\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\ncurr_model = \"\"\n\ndef log_to_file(txt=None, print_to_console_only=False):\n  if txt is None:\n    txt = ''\n  txt += '\\n'\n  print(txt)\n  if print_to_console_only:\n    return\n  if not path.exists(LOG_DIR):\n    os.mkdir(LOG_DIR)\n  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n  with open(full_path, mode='a') as f:\n    f.write(txt)\n    \n# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n# Make sure to delete any references to tensor. Else this function will not have significant effect\ndef clean_vram():\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\ndef name_of_caller(frame=1):\n    \"\"\"\n    Return \"class.function_name\" of the caller or just \"function_name\".\n    \"\"\"\n    frame = sys._getframe(frame)\n    fn_name = frame.f_code.co_name\n    var_names = frame.f_code.co_varnames\n    if var_names:\n        if var_names[0] == \"self\":\n            self_obj = frame.f_locals.get(\"self\")\n            if self_obj is not None:\n                return f\"{type(self_obj).__name__}.{fn_name}\" \n        if var_names[0] == \"cls\":\n            cls_obj = frame.f_locals.get(\"cls\")\n            if cls_obj is not None:\n                return f\"{cls_obj.__name__}.{fn_name}\"\n    return fn_name","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Initialize Deep Learning Models","metadata":{}},{"cell_type":"code","source":"model_constructors = {\n  models.alexnet.__name__: models.alexnet, \n  models.squeezenet1_1.__name__: models.squeezenet1_1,\n  models.resnet50.__name__: models.resnet50, \n  models.resnet101.__name__: models.resnet101,\n  models.resnet152.__name__: models.resnet152, \n  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n  models.densenet201.__name__: models.densenet201, \n  models.googlenet.__name__: models.googlenet, \n  models.vgg16.__name__: models.vgg16, \n  models.vgg19.__name__: models.vgg19, \n  models.inception_v3.__name__: models.inception_v3, \n}\n\nfrom torchvision.models import *\nmodel_weights = {\n  models.alexnet.__name__: AlexNet_Weights.DEFAULT,\n  models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n  models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n  models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n  models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n  models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n  models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n  models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n  models.vgg16.__name__: VGG16_Weights.DEFAULT,\n  models.vgg19.__name__: VGG19_Weights.DEFAULT,\n  models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n}\n\n# Experiment around dropout & Learning Rate & different optimizer (Adam)\ndef init_model(name):\n  if not path.exists(RESULT_DIR):\n    os.mkdir(RESULT_DIR)\n\n  clean_vram()\n  seed_everything()\n  model = model_constructors[name](weights=model_weights[name])\n  \n  # fine-tune pretrain models to our usecase\n  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n  NUM_CLASSES = len(class_names)\n  DROPOUT = 0.5\n  if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n    num_ftrs = model.classifier[6].in_features\n    model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.classifier[6] = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n  elif name == models.densenet201.__name__:\n    num_ftrs = model.classifier.in_features\n    model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.classifier = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n  elif name == models.squeezenet1_1.__name__:\n    model.classifier = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n#     model.classifier = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n#     )\n    model.num_classes = NUM_CLASSES\n  elif name == models.inception_v3.__name__:\n    auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n    model.AuxLogits.fc = nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n#     model.AuxLogits.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n#     )\n    primary_num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(primary_num_ftrs, NUM_CLASSES)\n#     model.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(primary_num_ftrs, NUM_CLASSES)\n#     )\n  else:\n    # resnet, resnext & googlenet\n    num_ftrs = model.fc.in_features\n    model.fc= nn.Linear(num_ftrs, NUM_CLASSES)\n#     model.fc = nn.Sequential(\n#       nn.Dropout(DROPOUT),\n#       nn.Linear(num_ftrs, NUM_CLASSES)\n#     )\n\n  model = model.to(device)\n  criterion = nn.CrossEntropyLoss()\n  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  if is_gpu_avail():\n    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n    scaler = torch.cuda.amp.GradScaler()\n  else:\n    raise RuntimeError('This code only support machine with GPU.')\n\n  # print('=====================================')\n  print(f'{name} is initialized')\n  # print('=====================================')\n  # print(model)\n  return model, criterion, optimizer, scaler\n\n# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\ndef save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n  torch.save({\n    'perf_metrics': perf_metrics,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    'history': history,\n    }, model_path)\n\ndef load_model(model, optimizer, scaler, model_path):\n  if not os.path.exists(model_path):\n    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n    return None, model, optimizer, scaler, None, None\n  checkpoint = torch.load(model_path)\n  perf_metrics = checkpoint['perf_metrics']\n  model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n  history = checkpoint['history']\n  total_epoch = len(history) - 1\n  del checkpoint\n\n  return perf_metrics, model, optimizer, scaler, history, total_epoch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Train Models","metadata":{}},{"cell_type":"code","source":"# training and validation loops\ndef train(model,\n    criterion,\n    optimizer,\n    scaler,\n    train_dataloader,\n    valid_dataloader,\n    model_path,\n    max_epochs_stop=10,\n    n_epochs=400,\n    min_epoch=300,\n    print_every=1):\n    \n    epochs_no_improve = 0\n    perf = {\n        'best_epoch': 0,\n        'valid_loss_min': np.Inf,\n        'valid_best_acc': 0,\n    }\n    total_epoch = 0\n\n    try:\n        if os.path.exists(model_path):\n            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n        else:\n            history = []\n            log_to_file(f'Starting Training from Scratch.\\n')\n    except:\n        history = []\n        log_to_file(f'exception: start from scratch.\\n')\n\n    overall_start = time.time()\n    if total_epoch >= n_epochs:\n        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n        history = pd.DataFrame(\n            history,\n            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n        return model, history, perf\n\n    seed_everything()\n\n    # Main loop - continue training on where we left off if there's a saved model\n    for epoch in range(total_epoch, n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = time.time()\n        for ii, (data, target) in enumerate (train_dataloader):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n            with torch.cuda.amp.autocast():\n              # output, aux_output = model(data)\n              # loss1 = criterion(output, target)\n              # loss2 = criterion(aux_output, target)\n              # loss = loss1 + 0.4*loss2\n              output = model(data)\n              loss = criterion(output, target)\n            # loss.backward()\n            # optimizer.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n            \n            # cleanup to save VRAM\n            del data, target\n#             clean_vram()\n\n        # After training loops ends, start validation\n        else:\n            print(\">>> Starting validation...\\n\")\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_dataloader:\n                    if is_gpu_avail():\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                    \n                    # cleanup to save VRAM\n                    del data, target\n#                     clean_vram()\n                train_loss = train_loss / train_data_size\n                valid_loss = valid_loss / valid_data_size\n                train_acc = train_acc / train_data_size\n                valid_acc = valid_acc / valid_data_size\n                history.append([train_loss, valid_loss,train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    log_to_file(f'Epoch: {epoch}', True)\n                    log_to_file(\n                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n                        True\n                    )\n                    log_to_file(\n                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n                        True\n                    )\n          \n                if valid_loss < perf['valid_loss_min']:\n                    print(f\">>> Saving model: valid_loss:{valid_loss}\\n\")\n                    epochs_no_improve = 0\n                    perf['best_epoch'] = epoch\n                    perf['valid_loss_min'] = valid_loss\n                    perf['valid_best_acc'] = valid_acc\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n                else:\n                    print(f\">>> No improvement: valid_loss:{valid_loss}; epoch:{epoch}; epochs_no_improve:{epochs_no_improve}\\n\")\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n                        log_to_file(\n                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n                        )\n                        total_time = time.time() - overall_start\n                        log_to_file(\n                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n                        )\n                        log_to_file()\n\n                        # Load the best state from saved model\n                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n                        # save the full history\n                        save_model(perf, model, optimizer, scaler, history, model_path)\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history, perf\n    \n    total_time = time.time() - overall_start\n    log_to_file(\n        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n    )\n    log_to_file(\n        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n    )\n    log_to_file()\n\n    # Load the best state from saved model\n    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n    # save the full history\n    save_model(perf, model, optimizer, scaler, history, model_path)\n\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n    \n    return model, history, perf\n\n\ndef save_train_val_loss_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_loss', 'valid_loss']:\n      plt.plot(\n          history[c], label=c)\n\n  title = f'{curr_model} - Training and Validation Losses'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Losses')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')\n\n\ndef save_train_val_acc_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_acc', 'valid_acc']:\n      plt.plot(\n          100 * history[c], label=c)\n      \n  title = f'{curr_model} - Training and Validation Accuracy'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Accuracy')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Define Functions to Visualize Prediction","metadata":{}},{"cell_type":"code","source":"# confusion matrix \ndef getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n    model.eval()\n    confusion_matrix=np.zeros((2,2),dtype=int)\n    num_images=test_data_size\n    \n    with torch.no_grad():\n        for i, (data,target) in enumerate(dataloader):\n            data = data.to(device)\n            target = target.to(device)\n            \n            output = model(data) \n            _, pred = torch.max(output, 1)\n            \n            for j in range(data.size()[0]): \n                if pred[j]==1 and target[j]==1:\n                    term='TP'\n                    confusion_matrix[0][0]+=1\n                elif pred[j]==1 and target[j]==0:\n                    term='FP'\n                    confusion_matrix[1][0]+=1\n                elif pred[j]==0 and target[j]==1:\n                    term='FN'\n                    confusion_matrix[0][1]+=1\n                elif pred[j]==0 and target[j]==0:\n                    term='TN'\n                    confusion_matrix[1][1]+=1\n            \n                if show_image:\n                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n                    log_to_file(term, print_to_console_only)\n                    imshow(data.cpu().data[j])\n        \n        log_to_file(None, print_to_console_only)\n        category = 'Test' if is_test else 'Validation'\n        log_to_file('=====================', print_to_console_only)\n        log_to_file(f'{category} Results ', print_to_console_only)\n        log_to_file('=====================', print_to_console_only)\n        log_to_file('Confusion Matrix: ', print_to_console_only)\n        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n        log_to_file(None, print_to_console_only)\n\n        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(None, print_to_console_only)\n    return confusion_matrix\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef save_test_acc_n_loss_graph(model, dataloader, criterion):\n  pass\n  # NOT NEEDED YET\n  # with torch.no_grad():\n  #   model.eval()\n  #   for data, target in dataloader:\n  #       if is_gpu_avail():\n  #           data, target = data.cuda(), target.cuda()\n  #       output = model(data)\n  #       loss = criterion(output, target)\n  #       test_loss += loss.item() * data.size(0)\n  #       _, pred = torch.max(output, dim=1)\n  #       correct_tensor = pred.eq(target.data.view_as(pred))\n  #       accuracy = torch.mean(\n  #           correct_tensor.type(torch.FloatTensor))\n  #       test_acc += accuracy.item() * data.size(0)\n  #   train_loss = train_loss / train_data_size\n  #   test_loss = test_loss / test_data_size\n  #   train_acc = train_acc / train_data_size\n  #   test_acc = test_acc / test_data_size\n\n\n# def visualize_test_prediction(model):\n#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n\n#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n\n#   # log_to_file(img_list)\n\n#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n#                               for img in img_list])\n#   pred_logits_tensor = model(test_batch)\n#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n\n#   row = 12\n#   col = 3\n#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n#   r = 0\n#   c = 0\n#   for i, img in enumerate(img_list):\n#       if c >= col:\n#         r += 1\n#         c = 0\n#       ax = axs[r, c]\n#       ax.axis('off')\n#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n#                                                               100*pred_probs[i,1]))\n#       ax.imshow(img)\n#       c +=1\n\n#   title = f'{curr_model} - Covid Image Prediction'\n#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n#   plt.savefig(full_path, bbox_inches='tight')\n\n\ndef getPredProbs(model, datasetStr, count, isSeeded=True):\n  if isSeeded:\n    seed_everything()\n  \n  dataset = data[datasetStr].samples\n  img_list = []\n  for i, (img_path, cls_idx) in enumerate(dataset):\n    if i >= count:\n      break\n    img_list.append(Image.open(img_path).convert('RGB'))\n\n  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n                              for img in img_list])\n  pred_logits_tensor = model(test_batch)\n  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Run all models - Init Models + Training","metadata":{}},{"cell_type":"code","source":"model_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n]\n\nfor i in range(0,11):\n  # https://github.com/pytorch/pytorch/issues/50198\n  # skipped these because cannot use deterministic algorithm\n#   skip_model = [0, 1, 5, 8, 9, 10]\n  skip_model = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n  if i in skip_model:\n    continue\n  curr_model = model_list[i]\n\n  # Initialize model, criterion and optimizer\n  model, criterion, optimizer, scaler = init_model(curr_model)\n\n#   Training & Validation\n  model, history, perf = train(\n      model,\n      criterion,\n      optimizer,\n      scaler,\n      train_loader,\n      val_loader,\n      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n      max_epochs_stop=5,  # Early stopping intialization\n      n_epochs=5,\n      min_epoch=5,\n      print_every=10)\n\n  save_train_val_loss_graph(history, perf)\n  save_train_val_acc_graph(history, perf)\n  getConfusionMatrix(model, val_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Out of memory issue\n\n- References\n    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n- The fix\n    - Delete unused tensor, force garbage collection and run `empty_cache()`\n    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB\n    - [mixed precision training & delete checkpoint](https://medium.com/deep-learning-for-protein-design/a-comprehensive-guide-to-memory-usage-in-pytorch-b9b7c78031d3)","metadata":{}},{"cell_type":"code","source":"torch.cuda.memory_stats(device)\n# print(torch.cuda.memory_summary(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}