{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"-   <a href=\"#summary\" id=\"toc-summary\">Summary</a>\n    -   <a href=\"#overview-and-explanation\"\n        id=\"toc-overview-and-explanation\">Overview and Explanation</a>\n-   <a href=\"#import-and-deterministic-setup\"\n    id=\"toc-import-and-deterministic-setup\">Import and Deterministic\n    Setup</a>\n-   <a href=\"#data-preprocessing\" id=\"toc-data-preprocessing\">Data\n    Preprocessing</a>\n    -   <a href=\"#custom-dataset-class-to-load-covidnet-data\"\n        id=\"toc-custom-dataset-class-to-load-covidnet-data\">Custom Dataset class\n        to load CovidNet data</a>\n    -   <a href=\"#spliting-dataset-into-train-val-test\"\n        id=\"toc-spliting-dataset-into-train-val-test\">Spliting dataset into\n        train, val, test</a>\n    -   <a href=\"#applying-transforms-to-dataset\"\n        id=\"toc-applying-transforms-to-dataset\">Applying transforms to\n        dataset</a>\n        -   <a href=\"#define-a-wrapper-dataset\"\n            id=\"toc-define-a-wrapper-dataset\">Define a wrapper dataset</a>\n        -   <a href=\"#defining-the-transforms\"\n            id=\"toc-defining-the-transforms\">Defining the transforms</a>\n        -   <a href=\"#creating-the-dataset-loader\"\n            id=\"toc-creating-the-dataset-loader\">Creating the Dataset Loader</a>\n-   <a href=\"#custom-model\" id=\"toc-custom-model\">Custom Model</a>\n    -   <a href=\"#references\" id=\"toc-references\">References</a>\n        -   <a href=\"#links\" id=\"toc-links\">Links</a>\n    -   <a href=\"#first-try\" id=\"toc-first-try\">First try</a>\n    -   <a href=\"#full-model\" id=\"toc-full-model\">Full Model</a>\n-   <a href=\"#training-validation\" id=\"toc-training-validation\">Training\n    &amp; Validation</a>\n-   <a href=\"#test-evaluation\" id=\"toc-test-evaluation\">Test\n    (Evaluation)</a>\n-   <a href=\"#playground\" id=\"toc-playground\">Playground</a>\n    -   <a href=\"#visualizing-models-using-pytorchviz\"\n        id=\"toc-visualizing-models-using-pytorchviz\">Visualizing Models using\n        PytorchViz</a>\n    -   <a href=\"#investigate-image-in-dataset\"\n        id=\"toc-investigate-image-in-dataset\">Investigate Image in Dataset</a>\n    -   <a href=\"#investigate-metadata.csv\"\n        id=\"toc-investigate-metadata.csv\">Investigate metadata.csv</a>\n    -   <a href=\"#investigating-split-dataset\"\n        id=\"toc-investigating-split-dataset\">Investigating split dataset</a>\n    -   <a href=\"#visualizing-dataloader\"\n        id=\"toc-visualizing-dataloader\">Visualizing Dataloader</a>\n    -   <a href=\"#trying-avalance\" id=\"toc-trying-avalance\">Trying Avalance</a>\n    -   <a href=\"#pretrained-model\" id=\"toc-pretrained-model\">Pretrained\n        Model</a>\n        -   <a href=\"#helper-functions\" id=\"toc-helper-functions\">Helper\n            Functions</a>\n        -   <a href=\"#define-function-to-initialize-deep-learning-models\"\n            id=\"toc-define-function-to-initialize-deep-learning-models\">Define\n            Function to Initialize Deep Learning Models</a>\n        -   <a href=\"#define-function-to-train-models\"\n            id=\"toc-define-function-to-train-models\">Define Function to Train\n            Models</a>\n        -   <a href=\"#define-functions-to-visualize-prediction\"\n            id=\"toc-define-functions-to-visualize-prediction\">Define Functions to\n            Visualize Prediction</a>\n        -   <a href=\"#run-all-models---init-models-training\"\n            id=\"toc-run-all-models---init-models-training\">Run all models - Init\n            Models + Training</a>\n    -   <a href=\"#out-of-memory-issue\" id=\"out-of-memory-issue\">Out of memory issue</a>","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Summary\n\n------------------------------------------------------------------------\n\n> Expand to see summary and details","metadata":{}},{"cell_type":"markdown","source":"## Overview and Explanation\n\n1.  This notebook reuses a lot of the [original transfer learning\n    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n    -   Here the focus is on building the new custom model using the\n        CovidNet-CT database.\n2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n    -   is not longer needed for notebook running in kaggle. Remained\n        here for references only\n    -   is where the dataset is being acquired.\n    -   Explanation of various phases in the [CovidNet-CT ML\n        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n        -   train phase is train phase\n        -   test phase is validation phase\n        -   infer phase is test phase\n3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n    -   refers to the way [CovidNet-CT preprocess its\n        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n        to use PyTorch\n    -   Two highlights\n        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n            used by the imagenet model\n        -   the image is cropped to the bounding box provided with the\n            dataset before resize to 512x512\n4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n    -   refers to [how CovidNet-CT\n        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n    -   This part is almost identical to the original transfer learning\n        model notebook.","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Import and Deterministic Setup\n\n------------------------------------------------------------------------\n\nAll modules will be imported here including modules used in the\n[Playground](#playground) section","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\n%env CUDA_LAUNCH_BLOCKING=1\nimport random\nimport numpy as np\nimport torch\n\nfrom os import path\nimport math\n\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\n\nimport torchvision\nfrom torchvision import models, transforms #, datasets\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.optim import lr_scheduler\nimport time\nimport torch.nn.functional as F\n\nimport gc\nimport pandas as pd\n\n# ensure reproducibility across different executions\n# https://pytorch.org/docs/stable/notes/randomness.html\n# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\nSEED = 18\ndef seed_everything(seed=18):\n    random.seed(seed)\n    %env PYTHONHASHSEED=$seed\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n#torch.set_deterministic(True)\ntorch.use_deterministic_algorithms(True)\n%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n        \n# https://www.kaggle.com/code/manabendrarout/vision-transformer-vit-pytorch-on-tpus-train/notebook\ndef is_tpu_avail():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        TPU_DETECTED = True\n    except:\n        pass\n\n    return TPU_DETECTED\n\n\ndef is_gpu_avail():\n    GPU_DETECTED = False\n    try:\n        GPU_DETECTED = torch.cuda.is_available()\n    except:\n        pass\n\n    return GPU_DETECTED","metadata":{"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","execution":{"iopub.status.busy":"2022-11-18T17:41:23.309648Z","iopub.execute_input":"2022-11-18T17:41:23.310250Z","iopub.status.idle":"2022-11-18T17:41:26.269913Z","shell.execute_reply.started":"2022-11-18T17:41:23.310158Z","shell.execute_reply":"2022-11-18T17:41:26.268980Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"env: CUDA_LAUNCH_BLOCKING=1\nenv: CUBLAS_WORKSPACE_CONFIG=:4096:8\nenv: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Data Preprocessing\n\n------------------------------------------------------------------------\n\n-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)","metadata":{}},{"cell_type":"markdown","source":"## Custom Dataset class to load CovidNet data\n\n- Various references used: \n  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10","metadata":{}},{"cell_type":"code","source":"class CovidNetDataset(Dataset):\n    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n        # don't seem to need the csv file\n        # self.df = pd.read_csv(csv_path)\n        # _, self.class_to_idx  = self.find_classes(csv_path);\n\n        self.img_dir = img_dir\n        self.split_files = split_files\n        \n        self.size = 0\n        self.limit_size = limit_size\n        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n        self.stradify_removal_based_on_limit()\n        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, index):\n        # filename = self.df[index, \"FILENAME\"]\n        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n        # image = Image.open(os.path.join(self.img_dir, filename))\n\n        label = self.targets[index]\n        with open(self.imgs[index], \"rb\") as f:\n            image = Image.open(f)\n            image = image.crop(self.bboxes[index])\n            image = image.copy()\n\n        if self.transform is not None:\n            image = self.transform(image)\n    \n\n        return image, label\n\n    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n    def find_classes(self, csv_path=None):\n        \"\"\"Returns class name array and class_to_idx.\n        See :class:`CovidNetDataset` for details.\n        \"\"\"\n        # class_col = \"finding\"\n        # classes = sorted(self.df[class_col].unique())\n        # if not classes:\n        #     raise FileNotFoundError(f\"Couldn't find any class from '{class_col}' column in {csv_path}.\")\n\n        # hard code classes as the order are not alphabetic\n        classes = ['Normal', 'Pneumonia', 'COVID-19']\n\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n        return classes, class_to_idx\n\n    def get_all_split_file_data(self):\n        files, classes, bboxes = [], [], []\n        for split_file in self.split_files:\n            f, cls, bb = self.get_data_from_split_file(split_file)\n            files.extend(f)\n            classes.extend(cls)\n            bboxes.extend(bb)\n        return files, classes, bboxes\n\n    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n    def get_data_from_split_file(self, split_file):\n        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n        files, classes, bboxes = [], [], []\n        with open(split_file, 'r') as f:\n            for line in f.readlines():\n                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n                files.append(path.join(self.img_dir, fname))\n                classes.append(int(cls))\n                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n                self.size += 1\n        return files, classes, bboxes\n    \n    '''Try to do stratified removal based on limit count if it is specified'''\n    def stradify_removal_based_on_limit(self):\n        _, class_to_idx = self.find_classes()\n        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n            return\n        \n        total_remove_count = self.size - self.limit_size\n        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n        for idx, count in occurrence.items():\n            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n        \n        print(occurrence)\n        print(target_remove_count)\n        \n        for i in reversed(range(len(self.targets))):\n            idx = self.targets[i]\n            if target_remove_count[idx] > 0:\n                del self.targets[i]\n                del self.imgs[i]\n                del self.bboxes[i]\n                target_remove_count[idx] -= 1\n                self.size -= 1","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:41:46.244124Z","iopub.execute_input":"2022-11-18T17:41:46.244476Z","iopub.status.idle":"2022-11-18T17:41:46.261408Z","shell.execute_reply.started":"2022-11-18T17:41:46.244439Z","shell.execute_reply":"2022-11-18T17:41:46.260431Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Spliting dataset into train, val, test\n\n-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nCURR_DIR = \"/kaggle/working\"\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nMAX_COUNT = 10000\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_COUNT)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\n_, _, _, _, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\n# dataset size\ntrain_data_size = len(train_index)\nvalid_data_size = len(val_index)\ntest_data_size = len(test_index)\n\nprint(f\"First 10 train_index: {train_index[:10]}\")\nprint(f\"length of train_index: {train_data_size}\\n\")\nprint(f\"First 10 val_index: {val_index[:10]}\")\nprint(f\"length of val_index: {valid_data_size}\\n\")\nprint(f\"First 10 test_index: {test_index[:10]}\")\nprint(f\"length of test_index: {test_data_size}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:41:50.548011Z","iopub.execute_input":"2022-11-18T17:41:50.548830Z","iopub.status.idle":"2022-11-18T17:41:53.201812Z","shell.execute_reply.started":"2022-11-18T17:41:50.548784Z","shell.execute_reply":"2022-11-18T17:41:53.200775Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{0: 71488, 1: 42943, 2: 310593}\n{0: 69806, 1: 41932, 2: 303285}\nLength of full dataset: 10001\nFirst 10 train_index: [6373 6939 9338  105 7573 4773 9125 4210 4499 1754]\nlength of train_index: 7999\n\nFirst 10 val_index: [6942 5767  536 6017 6502 1373 8798 3052 4665 9964]\nlength of val_index: 1001\n\nFirst 10 test_index: [9022 9017 9868 1724  795 4754 4568 1985 9612 9534]\nlength of test_index: 1001\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Applying transforms to dataset","metadata":{}},{"cell_type":"markdown","source":"### Define a wrapper dataset\n\n- This is to have the flexibility of applying different transforms to each of the splitted dataset \n- References \n    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)","metadata":{}},{"cell_type":"code","source":"class WrapperDataset:\n    def __init__(self, dataset, transform=None, target_transform=None):\n        self.dataset = dataset\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        image, label = self.dataset[index]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:08.998164Z","iopub.execute_input":"2022-11-18T17:42:08.998677Z","iopub.status.idle":"2022-11-18T17:42:09.010707Z","shell.execute_reply.started":"2022-11-18T17:42:08.998643Z","shell.execute_reply":"2022-11-18T17:42:09.009848Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Defining the transforms\n\n- References for mean and std of images \n    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n    - Bounding box causing issue when batching as stacking don’t work with\n    different size \n        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`","metadata":{}},{"cell_type":"code","source":"covidnet_std_transform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n])\n\ncovidnet_train_transform = transforms.Compose([\n    transforms.RandomChoice(transforms=[\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=.3, hue=.3),\n        transforms.RandomPerspective(distortion_scale=0.4),\n        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n    ])\n\nimage_transforms = {\n    'train': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ]),\n    'val': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'test': transforms.Compose([\n        covidnet_std_transform\n    ]),\n    'playground': transforms.Compose([\n        covidnet_train_transform,\n        covidnet_std_transform\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:11.199649Z","iopub.execute_input":"2022-11-18T17:42:11.200129Z","iopub.status.idle":"2022-11-18T17:42:11.213511Z","shell.execute_reply.started":"2022-11-18T17:42:11.200085Z","shell.execute_reply":"2022-11-18T17:42:11.210634Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Creating the Dataset Loader","metadata":{}},{"cell_type":"code","source":"seed_everything(SEED)\nBATCH_SIZE = 32\n\ntrain_sampler = SubsetRandomSampler(train_index)\nval_sampler = SubsetRandomSampler(val_index)\ntest_sampler = SubsetRandomSampler(test_index)\n\ntrain_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\ntest_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n\nclass_names, class_to_idx = full_dataset.find_classes()\nprint(class_names)\nprint(class_to_idx)\n\nif is_tpu_avail():\n    device = 'TPU'\nelif is_gpu_avail():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\nprint(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:14.970352Z","iopub.execute_input":"2022-11-18T17:42:14.970718Z","iopub.status.idle":"2022-11-18T17:42:15.053900Z","shell.execute_reply.started":"2022-11-18T17:42:14.970687Z","shell.execute_reply":"2022-11-18T17:42:15.052824Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"env: PYTHONHASHSEED=18\n['Normal', 'Pneumonia', 'COVID-19']\n{'Normal': 0, 'Pneumonia': 1, 'COVID-19': 2}\nUsing device: cuda\ntrain size:7999; validation size:1001; test size:1001\n","output_type":"stream"}]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Custom Model\n\n------------------------------------------------------------------------\n\n- The design of this custom model is illustrated in a draw.io diagram \n    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"### Links\n\n-   [10 CNN Architecture\n    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n    -   [Visualizing pytorch\n        models](https://github.com/szagoruyko/pytorchviz)\n-   Main model building references\n    -   The [CT-3A github\n        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n        -   [tensorflow pretrained\n            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n        -   How to [convert tensorflow checkpoints into pytorch\n            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n            -   [pytorch\n                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n        -   [pytorch\n            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n            about combining two existing models\n    -   [Pytorch resnext50\n        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n    -   [pytorch beginner tutorial on building\n        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n-   Other model building references\n    -   [Custom\n        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n    -   [Resnest convolution block\n        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram","metadata":{}},{"cell_type":"markdown","source":"## First try","metadata":{}},{"cell_type":"code","source":"class CovidNetModel(nn.Module):\n    def __init__(self):\n        super().__init__() # same as super(CovidNetModel, self).__init__()\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=5, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(3, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool1(x)\n        return x\n\n    def _make_block():\n        pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Full Model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using {device} device\")\n\nclass CovidNetModel(nn.Module):\n    def __init__(self):\n        super().__init__() # same as super(CovidNetModel, self).__init__()","metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Training & Validation\n\n------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Test (Evaluation)\n\n------------------------------------------------------------------------","metadata":{}},{"cell_type":"markdown","source":"------------------------------------------------------------------------\n\n# Playground\n\n------------------------------------------------------------------------\n\n> Testbed that is not compulsary for any part of this notebook","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Models using PytorchViz\n\n-   https://pytorch.org/docs/stable/nn.html\n-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n-   [Mandrin explanation of pytorch resnet\n    code](https://www.jianshu.com/p/90d61f53d15d)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\n%pip install -U git+https://github.com/szagoruyko/pytorchviz.git@master\nfrom torchviz import make_dot, make_dot_from_trace","metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\nfrom torchsummary import summary\n\nmodel = CovidNetModel()\nprint(model)\nsummary(model, (3, 224, 224))\n\nrender_model_pic_file = \"resnext50_32x4d\"\nmodel = models.resnext50_32x4d()\nsummary(model, (3, 224, 224))\n\n# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n# y = model(x)\n# dot = make_dot(y, params=dict(model.named_parameters()))\n\n# dot.format = \"png\"\n# dot.render(render_model_pic_file)\n# files.download(f\"{render_model_pic_file}.{dot.format}\")","metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate Image in Dataset","metadata":{}},{"cell_type":"code","source":"import subprocess\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\nDATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\nDATASET_NAME = \"COVIDx_CT-3A\"\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n\n# count number of images\n# !ls -Uba1 /content/data/3A_images | grep -c png\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\nprint(f\"number of images: {output.decode()}\")\n\n# list first 10 images\nls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\noutput = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\nfirst_10_lines = output.decode()\n# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\nimg_list = first_10_lines.split('\\n')\nfirst_png = next(filter(lambda img: \"png\" in img, img_list), None)\nprint(f\"first_png: {first_png}\\n\")","metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show the first test image, unbounded followed by bounded\nimport torch\nimport matplotlib.pyplot as plt\nimport torchvision.transforms.functional as torch_func_trans\nfrom PIL import Image\n\n# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\ndef get_data_from_split_file(split_file):\n    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n    files, classes, bboxes = [], [], []\n    with open(split_file, 'r') as f:\n        for line in f.readlines():\n            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n            files.append(path.join(DATASET_DIR, fname))\n            classes.append(int(cls))\n            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n    return files, classes, bboxes\n\ndef bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n    top = ymax\n    left = xmin\n    height = ymax - ymin\n    width = xmax - xmin\n    return top, left, height, width\n\nfiles, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\nfirst_file_tuple = (files[0], classes[0], bbox[0])\n\nprint(f\"first image: {first_file_tuple[0]}\")\nif 0:\n    # this way of reading image is deprecated\n    img = plt.imread(first_file_tuple[0])\n\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"unbounded\")\n    _ = plt.imshow(img)\n    _ = plt.axis('off')\n\n    torch_img = torch.from_numpy(img)\n    print(f\"torch_img size: {torch_img.size()}\")\n    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n    print(f\"pytorch_size: {pytorch_size}\")\n\n    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n    fig = plt.figure(figsize=(15,15))\n    plt.title(\"bounded\")\n    _ = plt.imshow(cropped_img)\n    _ = plt.axis('off')\n\n# This is the recommended method for opening image\n# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\nwith Image.open(first_file_tuple[0]) as im:\n    print(\"\\n\")\n    print(\"Unbounded image\")\n    IMG = im\n    display(im)\n\n    print(\"\\n\")\n    print(\"Bounded image\")\n    display(im.crop(first_file_tuple[2]))\n\n    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n    print(\"\\n\")\n    print(\"Out of Scope Unbounded image\")\n    display(IMG)\n    ","metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigate metadata.csv","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.expand_frame_repr', False)\n\n# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nmeta_df = pd.read_csv(METADATA_CSV)\nprint(\"First 5 rows in metadata.csv0\")\nprint(meta_df.head(5))\nprint(f\"classes: {sorted(meta_df['finding'].unique())}\")\n\nunverified = meta_df['verified finding'].eq('No').sum()\nverified = meta_df['verified finding'].eq('Yes').sum()\nprint(f\"Not verified: {unverified}\")\nprint(f\"Verified: {verified}\")\n\nunverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\nassert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\nprint(f\"first unverified ID: {unverified_patient_ids[0]}\")\n\nverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\nassert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\nprint(f\"first verified ID: {verified_patient_ids[0]}\")\n\nimgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\nprint(f\"total images: {len(imgs)}\")\nprint(f\"first 10 images: {imgs[:10]}\")\nimgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\nprint(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")","metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Investigating split dataset","metadata":{}},{"cell_type":"code","source":"# Specify all the filepath of the dataset\nDATA_DIR = \"/kaggle/input/covidxct\"\ndirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\nassert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n\nDATASET_DIR = path.join(DATA_DIR, dirs[0])\nMETADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n\nDATASET_NAME = \"COVIDx_CT-3A\"\nTRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\nVAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\nTEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\nSPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n\nfull_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES)\nfull_data_len = len(full_dataset)\nprint(f\"Length of full dataset: {full_data_len}\")\n\nSEED = 18\nseed_everything(SEED)\nBATCH_SIZE = 128\n\n# # Defines ratios, w.r.t. whole dataset.\nratio_train = 0.8\nratio_val = 0.1\nratio_test = 0.1\ndummy_X = np.zeros(full_data_len)\nindexes = np.arange(full_data_len)\n\n# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n# to be used in the next step. \n# Note that an additional indexes array is provided\nx_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n# train_index, test_index = next(\n#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n#         dummy_X, full_dataset.targets\n#     )\n# )\n\nprint('*'*50)\nprint(\"temp_train\")\nprint('*'*50)\nprint(f\"First 10 index: {temp_train_index[:10]}\")\nprint(f\"First 10 label: {y_remaining[:10]}\")\nprint(f\"length of index: {len(temp_train_index)}\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {y_remaining.count(0)}\")\nprint(f\"Pneunomia: {y_remaining.count(1)}\")\nprint(f\"Covid-19: {y_remaining.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"test\")\nprint('*'*50)\nprint(f\"First 10 index: {test_index[:10]}\")\nprint(f\"First 10 label: {Y_test[:10]}\")\nprint(f\"length of index: {len(test_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_test.count(0)}\")\nprint(f\"Pneunomia: {Y_test.count(1)}\")\nprint(f\"Covid-19: {Y_test.count(2)}\")\n\n# Adjusts val ratio, w.r.t. remaining dataset.\nratio_remaining = 1 - ratio_test\nratio_val_adjusted = ratio_val / ratio_remaining\n\n# Produces train and val splits.\nX_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n\nprint()\nprint('*'*50)\nprint(\"train\")\nprint('*'*50)\nprint(f\"First 10 index: {train_index[:10]}\")\nprint(f\"First 10 label: {Y_train[:10]}\")\nprint(f\"length of index: {len(train_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_train.count(0)}\")\nprint(f\"Pneunomia: {Y_train.count(1)}\")\nprint(f\"Covid-19: {Y_train.count(2)}\")\n\nprint()\nprint('*'*50)\nprint(\"val\")\nprint('*'*50)\nprint(f\"First 10 index: {val_index[:10]}\")\nprint(f\"First 10 label: {Y_val[:10]}\")\nprint(f\"length of index: {len(val_index)}\\n\")\nprint(\">>>Distribution of labels:\")\nprint(f\"Normal: {Y_val.count(0)}\")\nprint(f\"Pneunomia: {Y_val.count(1)}\")\nprint(f\"Covid-19: {Y_val.count(2)}\")","metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualizing Dataloader","metadata":{}},{"cell_type":"code","source":"import pprint\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    # mean = np.array([0.485, 0.456, 0.406])\n    # std = np.array([0.229, 0.224, 0.225])\n    # inp = std * inp + mean\n    # inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=[15, 15])\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nseed_everything(19)\nclass_names, _ = full_dataset.find_classes()\ndata, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n# out = torchvision.utils.make_grid(data)\n# imshow(out)\n# pp = pprint.PrettyPrinter(compact=True)\n# pp.pprint([class_names[x] for x in classes])\n\nprint(f\"Class: {classes[0]}\")\nout = data[0]\nimshow(out)","metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Trying Avalance","metadata":{}},{"cell_type":"code","source":"# https://changhsinlee.com/colab-import-python/\n!pip install requests\n!pip install avalanche-lib","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\n\n# Save datagenerators as file to colab working directory\n# If you are using GitHub, make sure you get the \"Raw\" version of the code\nurl = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\nr = requests.get(url)\n\n# make sure your filename is the same as how you want to import \nwith open('pytorchcv_models.py', 'w') as f:\n    f.write(r.text)\n\n# now we can import\nimport pytorchcv_models as pycv\nfrom types import SimpleNamespace\n\nargs = SimpleNamespace()\nargs.cuda = 0\npycv.main(args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pretrained Model","metadata":{}},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"LOG_DIR = os.path.join(CURR_DIR, \"log\")\nRESULT_DIR = os.path.join(CURR_DIR, 'result')\ncurr_model = \"\"\n\ndef log_to_file(txt=None, print_to_console_only=False):\n  if txt is None:\n    txt = ''\n  txt += '\\n'\n  print(txt)\n  if print_to_console_only:\n    return\n  if not path.exists(LOG_DIR):\n    os.mkdir(LOG_DIR)\n  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n  with open(full_path, mode='a') as f:\n    f.write(txt)\n    \n# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n# Make sure to delete any references to tensor. Else this function will not have significant effect\ndef clean_vram():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:29.315845Z","iopub.execute_input":"2022-11-18T17:42:29.316921Z","iopub.status.idle":"2022-11-18T17:42:29.326009Z","shell.execute_reply.started":"2022-11-18T17:42:29.316876Z","shell.execute_reply":"2022-11-18T17:42:29.324752Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Initialize Deep Learning Models","metadata":{}},{"cell_type":"code","source":"model_constructors = {\n  models.alexnet.__name__: models.alexnet, \n  models.squeezenet1_1.__name__: models.squeezenet1_1,\n  models.resnet50.__name__: models.resnet50, \n  models.resnet101.__name__: models.resnet101,\n  models.resnet152.__name__: models.resnet152, \n  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n  models.densenet201.__name__: models.densenet201, \n  models.googlenet.__name__: models.googlenet, \n  models.vgg16.__name__: models.vgg16, \n  models.vgg19.__name__: models.vgg19, \n  models.inception_v3.__name__: models.inception_v3, \n}\n\n# Experiment around dropout & Learning Rate & different optimizer (Adam)\ndef init_model(name):\n  if not path.exists(RESULT_DIR):\n    os.mkdir(RESULT_DIR)\n\n  clean_vram()\n  seed_everything()\n  model = model_constructors[name](pretrained=True) \n  \n  # fine-tune pretrain models to our usecase\n  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n  NUM_CLASSES = len(class_names)\n  DROPOUT = 0.5\n  if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n    num_ftrs = model.classifier[6].in_features\n    # model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)\n    model.classifier[6] = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Linear(num_ftrs, NUM_CLASSES)\n    )\n  elif name == models.densenet201.__name__:\n    num_ftrs = model.classifier.in_features\n    # model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n    model.classifier = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Linear(num_ftrs, NUM_CLASSES)\n    )\n  elif name == models.squeezenet1_1.__name__:\n    # model.classifier = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n    model.classifier = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n    )\n    model.num_classes = NUM_CLASSES\n  elif name == models.inception_v3.__name__:\n    auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n    # model.AuxLogits.fc = nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n    model.AuxLogits.fc = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n    )\n    primary_num_ftrs = model.fc.in_features\n    # model.fc = nn.Linear(primary_num_ftrs, NUM_CLASSES)\n    model.fc = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Linear(primary_num_ftrs, NUM_CLASSES)\n    )\n  else:\n    # resnet, resnext & googlenet\n    num_ftrs = model.fc.in_features\n    # model.fc= nn.Linear(num_ftrs, NUM_CLASSES)\n    model.fc = nn.Sequential(\n      nn.Dropout(DROPOUT),\n      nn.Linear(num_ftrs, NUM_CLASSES)\n    )\n\n  model = model.to(device)\n  criterion = nn.CrossEntropyLoss()\n  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n  if is_gpu_avail():\n    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n    scaler = torch.cuda.amp.GradScaler()\n  else:\n    raise RuntimeError('This code only support machine with GPU.')\n\n  # print('=====================================')\n  print(f'{name} is initialized')\n  # print('=====================================')\n  # print(model)\n  return model, criterion, optimizer, scaler\n\n# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\ndef save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n  torch.save({\n    'perf_metrics': perf_metrics,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    \"scaler_state_dict\": scaler.state_dict(),\n    'history': history,\n    }, model_path)\n\ndef load_model(model, optimizer, scaler, model_path):\n  checkpoint = torch.load(model_path)\n  perf_metrics = checkpoint['perf_metrics']\n  model.load_state_dict(checkpoint['model_state_dict'])\n  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n  history = checkpoint['history']\n  total_epoch = len(history) - 1\n\n  return perf_metrics, model, optimizer, scaler, history, total_epoch","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:32.460666Z","iopub.execute_input":"2022-11-18T17:42:32.461039Z","iopub.status.idle":"2022-11-18T17:42:32.478278Z","shell.execute_reply.started":"2022-11-18T17:42:32.460990Z","shell.execute_reply":"2022-11-18T17:42:32.477192Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Define Function to Train Models","metadata":{}},{"cell_type":"code","source":"# training and validation loops\ndef train(model,\n    criterion,\n    optimizer,\n    scaler,\n    train_dataloader,\n    valid_dataloader,\n    model_path,\n    max_epochs_stop=10,\n    n_epochs=400,\n    min_epoch=300,\n    print_every=1):\n    \n    epochs_no_improve = 0\n    perf = {\n        'best_epoch': 0,\n        'valid_loss_min': np.Inf,\n        'valid_best_acc': 0,\n    }\n    total_epoch = 0\n\n    try:\n        if os.path.exists(model_path):\n            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n        else:\n            history = []\n            log_to_file(f'Starting Training from Scratch.\\n')\n    except:\n        history = []\n        log_to_file(f'exception: start from scratch.\\n')\n\n    overall_start = time.time()\n    if total_epoch >= n_epochs:\n        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n        history = pd.DataFrame(\n            history,\n            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n        return model, history, perf\n\n    seed_everything()\n\n    # Main loop - continue training on where we left off if there's a saved model\n    for epoch in range(total_epoch, n_epochs):\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = time.time()\n        for ii, (data, target) in enumerate (train_dataloader):\n            data, target = data.cuda(), target.cuda()\n            optimizer.zero_grad()\n\n            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n            with torch.cuda.amp.autocast():\n              # output, aux_output = model(data)\n              # loss1 = criterion(output, target)\n              # loss2 = criterion(aux_output, target)\n              # loss = loss1 + 0.4*loss2\n              output = model(data)\n              loss = criterion(output, target)\n            # loss.backward()\n            # optimizer.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            train_loss += loss.item() * data.size(0)\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            train_acc += accuracy.item() * data.size(0)\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n            \n            # cleanup to save VRAM\n            del data, target\n#             clean_vram()\n\n        # After training loops ends, start validation\n        else:\n            with torch.no_grad():\n                model.eval()\n                for data, target in valid_dataloader:\n                    if is_gpu_avail():\n                        data, target = data.cuda(), target.cuda()\n                    output = model(data)\n                    loss = criterion(output, target)\n                    valid_loss += loss.item() * data.size(0)\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    valid_acc += accuracy.item() * data.size(0)\n                    \n                    # cleanup to save VRAM\n                    del data, target\n#                     clean_vram()\n                train_loss = train_loss / train_data_size\n                valid_loss = valid_loss / valid_data_size\n                train_acc = train_acc / train_data_size\n                valid_acc = valid_acc / valid_data_size\n                history.append([train_loss, valid_loss,train_acc, valid_acc])\n                if (epoch + 1) % print_every == 0:\n                    log_to_file(f'Epoch: {epoch}', True)\n                    log_to_file(\n                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n                        True\n                    )\n                    log_to_file(\n                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n                        True\n                    )\n          \n                if valid_loss < perf['valid_loss_min']:\n                    epochs_no_improve = 0\n                    perf['best_epoch'] = epoch\n                    perf['valid_loss_min'] = valid_loss\n                    perf['valid_best_acc'] = valid_acc\n                    save_model(perf, model, optimizer, scaler, history, model_path)\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n                        log_to_file(\n                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n                        )\n                        total_time = time.time() - overall_start\n                        log_to_file(\n                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n                        )\n                        log_to_file()\n\n                        # Load the best state from saved model\n                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n                        # save the full history\n                        save_model(perf, model, optimizer, scaler, history, model_path)\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history, perf\n    \n    total_time = time.time() - overall_start\n    log_to_file(\n        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n    )\n    log_to_file(\n        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n    )\n    log_to_file()\n\n    # Load the best state from saved model\n    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n    # save the full history\n    save_model(perf, model, optimizer, scaler, history, model_path)\n\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n    \n    return model, history, perf\n\n\ndef save_train_val_loss_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_loss', 'valid_loss']:\n      plt.plot(\n          history[c], label=c)\n\n  title = f'{curr_model} - Training and Validation Losses'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Losses')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')\n\n\ndef save_train_val_acc_graph(history, perf):\n  plt.figure(figsize=(8, 6))\n  for c in ['train_acc', 'valid_acc']:\n      plt.plot(\n          100 * history[c], label=c)\n      \n  title = f'{curr_model} - Training and Validation Accuracy'\n  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n  plt.xlabel('Epochs')\n  plt.ylabel('Average Accuracy')\n  plt.title(title)\n  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n  plt.legend()\n  plt.savefig(full_path, bbox_inches='tight')","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:37.618665Z","iopub.execute_input":"2022-11-18T17:42:37.619044Z","iopub.status.idle":"2022-11-18T17:42:37.646950Z","shell.execute_reply.started":"2022-11-18T17:42:37.618991Z","shell.execute_reply":"2022-11-18T17:42:37.645658Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Define Functions to Visualize Prediction","metadata":{}},{"cell_type":"code","source":"# confusion matrix \ndef getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n    model.eval()\n    confusion_matrix=np.zeros((2,2),dtype=int)\n    num_images=test_data_size\n    \n    with torch.no_grad():\n        for i, (data,target) in enumerate(dataloader):\n            data = data.to(device)\n            target = target.to(device)\n            \n            output = model(data) \n            _, pred = torch.max(output, 1)\n            \n            for j in range(data.size()[0]): \n                if pred[j]==1 and target[j]==1:\n                    term='TP'\n                    confusion_matrix[0][0]+=1\n                elif pred[j]==1 and target[j]==0:\n                    term='FP'\n                    confusion_matrix[1][0]+=1\n                elif pred[j]==0 and target[j]==1:\n                    term='FN'\n                    confusion_matrix[0][1]+=1\n                elif pred[j]==0 and target[j]==0:\n                    term='TN'\n                    confusion_matrix[1][1]+=1\n            \n                if show_image:\n                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n                    log_to_file(term, print_to_console_only)\n                    imshow(data.cpu().data[j])\n        \n        log_to_file(None, print_to_console_only)\n        category = 'Test' if is_test else 'Validation'\n        log_to_file('=====================', print_to_console_only)\n        log_to_file(f'{category} Results ', print_to_console_only)\n        log_to_file('=====================', print_to_console_only)\n        log_to_file('Confusion Matrix: ', print_to_console_only)\n        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n        log_to_file(None, print_to_console_only)\n\n        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n        log_to_file(None, print_to_console_only)\n    return confusion_matrix\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef save_test_acc_n_loss_graph(model, dataloader, criterion):\n  pass\n  # NOT NEEDED YET\n  # with torch.no_grad():\n  #   model.eval()\n  #   for data, target in dataloader:\n  #       if is_gpu_avail():\n  #           data, target = data.cuda(), target.cuda()\n  #       output = model(data)\n  #       loss = criterion(output, target)\n  #       test_loss += loss.item() * data.size(0)\n  #       _, pred = torch.max(output, dim=1)\n  #       correct_tensor = pred.eq(target.data.view_as(pred))\n  #       accuracy = torch.mean(\n  #           correct_tensor.type(torch.FloatTensor))\n  #       test_acc += accuracy.item() * data.size(0)\n  #   train_loss = train_loss / train_data_size\n  #   test_loss = test_loss / test_data_size\n  #   train_acc = train_acc / train_data_size\n  #   test_acc = test_acc / test_data_size\n\n\n# def visualize_test_prediction(model):\n#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n\n#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n\n#   # log_to_file(img_list)\n\n#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n#                               for img in img_list])\n#   pred_logits_tensor = model(test_batch)\n#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n\n#   row = 12\n#   col = 3\n#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n#   r = 0\n#   c = 0\n#   for i, img in enumerate(img_list):\n#       if c >= col:\n#         r += 1\n#         c = 0\n#       ax = axs[r, c]\n#       ax.axis('off')\n#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n#                                                               100*pred_probs[i,1]))\n#       ax.imshow(img)\n#       c +=1\n\n#   title = f'{curr_model} - Covid Image Prediction'\n#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n#   plt.savefig(full_path, bbox_inches='tight')\n\n\ndef getPredProbs(model, datasetStr, count, isSeeded=True):\n  if isSeeded:\n    seed_everything()\n  \n  dataset = data[datasetStr].samples\n  img_list = []\n  for i, (img_path, cls_idx) in enumerate(dataset):\n    if i >= count:\n      break\n    img_list.append(Image.open(img_path).convert('RGB'))\n\n  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n                              for img in img_list])\n  pred_logits_tensor = model(test_batch)\n  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:43.414003Z","iopub.execute_input":"2022-11-18T17:42:43.414372Z","iopub.status.idle":"2022-11-18T17:42:43.434106Z","shell.execute_reply.started":"2022-11-18T17:42:43.414340Z","shell.execute_reply":"2022-11-18T17:42:43.431063Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Run all models - Init Models + Training","metadata":{}},{"cell_type":"code","source":"model_list = [\n    models.alexnet.__name__, # 0\n    models.squeezenet1_1.__name__, #1\n    models.resnet50.__name__, # 2\n    models.resnet101.__name__, # 3\n    models.resnet152.__name__, # 4\n    models.resnext101_32x8d.__name__, # 5\n    models.densenet201.__name__, # 6\n    models.googlenet.__name__, # 7\n    models.vgg16.__name__, # 8\n    models.vgg19.__name__, #9\n    models.inception_v3.__name__, #10\n]\n\nfor i in range(0,11):\n  # https://github.com/pytorch/pytorch/issues/50198\n  # skipped these because cannot use deterministic algorithm\n#   skip_model = [0, 1, 5, 8, 9, 10]\n  skip_model = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n  if i in skip_model:\n    continue\n  curr_model = model_list[i]\n\n  # Initialize model, criterion and optimizer\n  model, criterion, optimizer, scaler = init_model(curr_model)\n\n#   Training & Validation\n  model, history, perf = train(\n      model,\n      criterion,\n      optimizer,\n      scaler,\n      train_loader,\n      val_loader,\n      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n      max_epochs_stop=5,  # Early stopping intialization\n      n_epochs=1,\n      min_epoch=1,\n      print_every=10)\n\n  save_train_val_loss_graph(history, perf)\n  save_train_val_acc_graph(history, perf)\n  getConfusionMatrix(model, val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-11-18T17:42:47.440338Z","iopub.execute_input":"2022-11-18T17:42:47.440722Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"env: PYTHONHASHSEED=18\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-394f9c45.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2e2520a7ea488394683e1b2e20197d"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Out of memory issue\n\n- References\n    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n- The fix\n    - Delete unused tensor, force garbage collection and run `empty_cache()`\n    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB","metadata":{}},{"cell_type":"code","source":"torch.cuda.memory_stats(device)\n# print(torch.cuda.memory_summary(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
