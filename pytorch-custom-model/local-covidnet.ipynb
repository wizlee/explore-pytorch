{"cells":[{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Summary\n","\n","------------------------------------------------------------------------\n","\n","> Expand to see summary and details"]},{"cell_type":"markdown","metadata":{},"source":["## Overview and Explanation\n","\n","1.  This notebook reuses a lot of the [original transfer learning\n","    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n","    -   Here the focus is on building the new custom model using the\n","        CovidNet-CT database.\n","2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n","    -   is not longer needed for notebook running in kaggle. Remained\n","        here for references only\n","    -   is where the dataset is being acquired.\n","    -   Explanation of various phases in the [CovidNet-CT ML\n","        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n","        -   train phase is train phase\n","        -   test phase is validation phase\n","        -   infer phase is test phase\n","3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n","    -   refers to the way [CovidNet-CT preprocess its\n","        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n","    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n","        to use PyTorch\n","    -   Two highlights\n","        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n","            used by the imagenet model\n","        -   the image is cropped to the bounding box provided with the\n","            dataset before resize to 512x512\n","4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n","    -   refers to [how CovidNet-CT\n","        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n","    -   This part is almost identical to the original transfer learning\n","        model notebook."]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Import and Deterministic Setup\n","\n","------------------------------------------------------------------------\n","\n","All modules will be imported here including modules used in the\n","[Playground](#playground) section"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:44.921196Z","iopub.status.busy":"2022-11-17T15:12:44.920745Z","iopub.status.idle":"2022-11-17T15:12:45.868415Z","shell.execute_reply":"2022-11-17T15:12:45.866381Z","shell.execute_reply.started":"2022-11-17T15:12:44.921088Z"},"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: CUDA_LAUNCH_BLOCKING=1\n","env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n","env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:22\n"]}],"source":["from __future__ import print_function, division\n","import os\n","%env CUDA_LAUNCH_BLOCKING=1\n","import random\n","import numpy as np\n","import torch\n","\n","from os import path\n","import zipfile\n","import math\n","import sys\n","\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from PIL import Image\n","from sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\n","\n","import torchvision\n","from torchvision import models, transforms #, datasets\n","import matplotlib.pyplot as plt\n","\n","from torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, ModuleList, MaxPool2d, AdaptiveAvgPool2d, Linear, CrossEntropyLoss\n","from torch import optim\n","from torch.optim import lr_scheduler\n","import time\n","import torch.nn.functional as F\n","\n","import gc\n","import pandas as pd\n","import enum\n","from enum import Enum\n","\n","# ensure reproducibility across different executions\n","# https://pytorch.org/docs/stable/notes/randomness.html\n","# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n","# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n","SEED = 18\n","def seed_everything(seed=18):\n","    random.seed(seed)\n","    %env PYTHONHASHSEED=$seed\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","#torch.set_deterministic(True)\n","torch.use_deterministic_algorithms(True)\n","%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n","%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:22\n","\n","\n","def is_gpu_avail():\n","    GPU_DETECTED = False\n","    try:\n","        GPU_DETECTED = torch.cuda.is_available()\n","    except:\n","        pass\n","\n","    return GPU_DETECTED"]},{"cell_type":"markdown","metadata":{"id":"wMQLloEgzPol"},"source":["---\n","# Setup Kaggle data access to download dataset\n","---\n","\n","- [dataset source from kaggle](https://www.kaggle.com/datasets/hgunraj/covidxct)\n","- On 02-06-2022, CT-3A datasets with 425,024 CT images are uploaded. \n","    - The way of how CT-3A and CT-3B are constructured can be found in [this link](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md).\n","    - This notebook only uses CT-3A that is already constructed and ready for download from the kaggle source."]},{"cell_type":"markdown","metadata":{"id":"w_OfYhgp1V1z"},"source":["## Use Kaggle API to download dataset\n","\n","### Solving google drive not enough capacity to store data\n","- [Article explaining how to access Kaggle Data in colab](https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7)\n","- [An old reddit post that seems to be relevant](https://old.reddit.com/r/PiratedGames/comments/uypcw2/use_google_colab_to_mass_transfer_files_from/)\n","- Some rclone idea\n","    - [rclone gui](https://rclone.org/gui/)\n","    - [rclone lab](https://github.com/acamposxp/RcloneLab)\n","    - [rclone idea 1](https://github.com/ella-tj/Any-File-2-GDrive)\n","    - [rclone idea 2](https://github.com/eaustin6/Rclone-Setup-on-Google-Colab)\n","    - [rclone idea 2](https://towardsdatascience.com/why-you-should-try-rclone-with-google-drive-and-colab-753f3ec04ba1)\n","    - [reddit colab thread](https://old.reddit.com/r/DataHoarder/comments/g069dx/google_colab_is_like_running_code_from_your/)\n","    - [socialgrep on rclone and colab](https://socialgrep.com/search?query=rclone%2Ccolab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"CZdji66q0Zpt","outputId":"b8a595be-ee84-4084-c706-e72b0528a3c4"},"outputs":[],"source":["CURR_DIR = os.getcwd()\n","dataset_zip = path.join(CURR_DIR, \"covidxct.zip\")\n","dataset_dir = path.join(CURR_DIR, \"data\")\n","\n","# manually download dataset from https://www.kaggle.com/datasets/c395fb339f210700ba392d81bf200f766418238c2734e5237b5dd0b6fc724fcb\n","# The kaggle command despite don't show error, not able to save/download the zip into local file system\n","# kaggle datasets download -d hgunraj/covidxct\n","\n","if not path.exists(dataset_dir):\n","    os.mkdir(dataset_dir)\n","    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n","        zip_ref.extractall(dataset_dir)\n","if path.exists(dataset_zip):\n","  print(dataset_zip)\n","    # os.remove(dataset_zip)"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Data Preprocessing\n","\n","------------------------------------------------------------------------\n","\n","-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n","-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n","    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n","-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset class to load CovidNet data\n","\n","- Various references used: \n","  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n","  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n","  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n","  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n","  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n","  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:50.079878Z","iopub.status.busy":"2022-11-17T15:12:50.079366Z","iopub.status.idle":"2022-11-17T15:12:50.092973Z","shell.execute_reply":"2022-11-17T15:12:50.091975Z","shell.execute_reply.started":"2022-11-17T15:12:50.079844Z"},"trusted":true},"outputs":[],"source":["class CovidNetDataset(Dataset):\n","    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n","        # don't seem to need the csv file\n","        # self.df = pd.read_csv(csv_path)\n","        # _, self.class_to_idx  = self.find_classes(csv_path);\n","\n","        self.img_dir = img_dir\n","        self.split_files = split_files\n","        \n","        self.size = 0\n","        self.limit_size = limit_size\n","        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n","        self.stradify_removal_based_on_limit()\n","        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, index):\n","        # filename = self.df[index, \"FILENAME\"]\n","        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n","        # image = Image.open(os.path.join(self.img_dir, filename))\n","\n","        label = self.targets[index]\n","        with open(self.imgs[index], \"rb\") as f:\n","            image = Image.open(f)\n","            image = image.crop(self.bboxes[index])\n","            image = image.copy()\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","    \n","\n","        return image, label\n","\n","    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n","    def find_classes(self, csv_path=None):\n","        \"\"\"Returns class name array and class_to_idx.\n","        See :class:`CovidNetDataset` for details.\n","        \"\"\"\n","        # class_col = \"finding\"\n","        # classes = sorted(self.df[class_col].unique())\n","        # if not classes:\n","        #     raise FileNotFoundError(f\"Couldn't find any class from '{class_col}' column in {csv_path}.\")\n","\n","        # hard code classes as the order are not alphabetic\n","        classes = ['Normal', 'Pneumonia', 'COVID-19']\n","\n","        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n","        return classes, class_to_idx\n","\n","    def get_all_split_file_data(self):\n","        files, classes, bboxes = [], [], []\n","        for split_file in self.split_files:\n","            f, cls, bb = self.get_data_from_split_file(split_file)\n","            files.extend(f)\n","            classes.extend(cls)\n","            bboxes.extend(bb)\n","        return files, classes, bboxes\n","\n","    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","    def get_data_from_split_file(self, split_file):\n","        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","        files, classes, bboxes = [], [], []\n","        with open(split_file, 'r') as f:\n","            for line in f.readlines():\n","                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","                files.append(path.join(self.img_dir, fname))\n","                classes.append(int(cls))\n","                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","                self.size += 1\n","        return files, classes, bboxes\n","    \n","    '''Try to do stratified removal based on limit count if it is specified'''\n","    def stradify_removal_based_on_limit(self):\n","        _, class_to_idx = self.find_classes()\n","        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n","        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n","            return\n","        \n","        total_remove_count = self.size - self.limit_size\n","        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n","        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n","        for idx, count in occurrence.items():\n","            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n","        \n","        print(occurrence)\n","        print(target_remove_count)\n","        \n","        for i in reversed(range(len(self.targets))):\n","            idx = self.targets[i]\n","            if target_remove_count[idx] > 0:\n","                del self.targets[i]\n","                del self.imgs[i]\n","                del self.bboxes[i]\n","                target_remove_count[idx] -= 1\n","                self.size -= 1"]},{"cell_type":"markdown","metadata":{},"source":["## Spliting dataset into train, val, test\n","\n","-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n","    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n","    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n","-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n","-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n","-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n","-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:54.991228Z","iopub.status.busy":"2022-11-17T15:12:54.990831Z","iopub.status.idle":"2022-11-17T15:12:57.295385Z","shell.execute_reply":"2022-11-17T15:12:57.294226Z","shell.execute_reply.started":"2022-11-17T15:12:54.991172Z"},"trusted":true},"outputs":[],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = path.join(CURR_DIR, \"data\")\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, 10000)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","_, _, _, _, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","# dataset size\n","train_data_size = len(train_index)\n","valid_data_size = len(val_index)\n","test_data_size = len(test_index)\n","\n","print(f\"First 10 train_index: {train_index[:10]}\")\n","print(f\"length of train_index: {train_data_size}\\n\")\n","print(f\"First 10 val_index: {val_index[:10]}\")\n","print(f\"length of val_index: {valid_data_size}\\n\")\n","print(f\"First 10 test_index: {test_index[:10]}\")\n","print(f\"length of test_index: {test_data_size}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Applying transforms to dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Define a wrapper dataset\n","\n","- This is to have the flexibility of applying different transforms to each of the splitted dataset \n","- References \n","    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n","    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n","    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:05.619690Z","iopub.status.busy":"2022-11-17T15:13:05.619009Z","iopub.status.idle":"2022-11-17T15:13:05.626266Z","shell.execute_reply":"2022-11-17T15:13:05.625249Z","shell.execute_reply.started":"2022-11-17T15:13:05.619650Z"},"trusted":true},"outputs":[],"source":["class WrapperDataset:\n","    def __init__(self, dataset, transform=None, target_transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        image, label = self.dataset[index]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        if self.target_transform is not None:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.dataset)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining the transforms\n","\n","- References for mean and std of images \n","    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n","    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n","    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n","    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n","    - Bounding box causing issue when batching as stacking don’t work with\n","    different size \n","        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n","        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:08.902356Z","iopub.status.busy":"2022-11-17T15:13:08.901940Z","iopub.status.idle":"2022-11-17T15:13:08.915480Z","shell.execute_reply":"2022-11-17T15:13:08.914234Z","shell.execute_reply.started":"2022-11-17T15:13:08.902324Z"},"trusted":true},"outputs":[],"source":["covidnet_std_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n","])\n","\n","covidnet_train_transform = transforms.Compose([\n","    transforms.RandomChoice(transforms=[\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=.3, hue=.3),\n","        transforms.RandomPerspective(distortion_scale=0.4),\n","        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n","    ])\n","\n","image_transforms = {\n","    'train': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ]),\n","    'val': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'test': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'playground': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ])\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Dataset Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:11.477712Z","iopub.status.busy":"2022-11-17T15:13:11.477114Z","iopub.status.idle":"2022-11-17T15:13:11.529327Z","shell.execute_reply":"2022-11-17T15:13:11.528231Z","shell.execute_reply.started":"2022-11-17T15:13:11.477670Z"},"trusted":true},"outputs":[],"source":["seed_everything(SEED)\n","BATCH_SIZE = 2\n","\n","train_sampler = SubsetRandomSampler(train_index)\n","val_sampler = SubsetRandomSampler(val_index)\n","test_sampler = SubsetRandomSampler(test_index)\n","\n","train_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\n","val_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\n","test_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n","\n","class_names, class_to_idx = full_dataset.find_classes()\n","print(class_names)\n","print(class_to_idx)\n","\n","if is_gpu_avail():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","    \n","print(f'Using device: {device}')\n","print(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Custom Model\n","\n","------------------------------------------------------------------------\n","\n","- The design of this custom model is illustrated in a draw.io diagram \n","    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n","    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)"]},{"cell_type":"markdown","metadata":{},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["### Links\n","\n","-   [10 CNN Architecture\n","    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n","    -   [Visualizing pytorch\n","        models](https://github.com/szagoruyko/pytorchviz)\n","-   Main model building references\n","    -   The [CT-3A github\n","        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n","        -   [tensorflow pretrained\n","            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n","        -   How to [convert tensorflow checkpoints into pytorch\n","            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n","            -   [pytorch\n","                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n","        -   [pytorch\n","            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n","            about combining two existing models\n","    -   [Pytorch resnext50\n","        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n","    -   [pytorch beginner tutorial on building\n","        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n","-   Other model building references\n","    -   [Custom\n","        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n","    -   [Resnest convolution block\n","        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n","    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram\n","-   How to determine ConvNet output channel/size\n","    -   [How to determine output channel ref 1](https://datascience.stackexchange.com/questions/47328/how-to-choose-the-number-of-output-channels-in-a-convolutional-layer)\n","    -   [How to determine output channel ref 2](https://stats.stackexchange.com/questions/380996/convolutional-network-how-to-choose-output-channels-number-stride-and-padding)\n","    -   TLDR.: The exact number of output channel don't matter, experiment around the common range and don't use values that are too extreme\n","    -   For output size, it can be easier calculated using [this calcuator](https://asiltureli.github.io/Convolution-Layer-Calculator) or even manually."]},{"cell_type":"markdown","metadata":{},"source":["## Components"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# modified from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L46\n","class Conv2d_BN(Module):\n","    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, groups=1, acti=True):\n","        super().__init__() # same as super(Conv2d_BN, self).__init__()\n","        if acti:\n","            self.conv2d_bn = Sequential(\n","                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n","                BatchNorm2d(out_channels),\n","                LeakyReLU(0.2, inplace=True)\n","            )\n","        else:\n","            self.conv2d_bn = Sequential(\n","                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n","                BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        return self.conv2d_bn(x)\n","\n","    def out_channels(self):\n","      return next(self.conv2d_bn.children()).out_channels\n","\n","\n","# Taken from https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py#L63\n","class StemBlock(Module):\n","    def __init__(self, in_channels=3):\n","        super().__init__()\n","        # For simplicity Sequential module can be used here, explicitly name every layer for practise and readibility\n","        self.conv1 = Conv2d_BN(in_channels, out_channels=64, kernel=7, stride=2, padding=3)\n","        self.maxpool1 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","        self.conv2 = Conv2d_BN(in_channels=self.conv1.out_channels(), out_channels=80, kernel=1)\n","        self.conv3 = Conv2d_BN(in_channels=self.conv2.out_channels(), out_channels=192, kernel=3, padding=1)\n","        self.maxpool2 = MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)\n","\n","    def forward(self, x):\n","        # N x 3 x 224 x 224\n","        x = self.conv1(x)\n","        # N x 64 x 112 x 112\n","        x = self.maxpool1(x)\n","        # N x 64 x 56 x 56\n","        x = self.conv2(x)\n","        # N x 80 x 56 x 56\n","        x = self.conv3(x)\n","        # N x 192 x 56 x 56\n","        return self.maxpool2(x)\n","        # N x 192 x 28 x 28\n","\n","    def out_channels(self):\n","      # unable to access output size of MaxPool2d, use hard-coded formula instead\n","      return math.floor(self.conv3.out_channels()/2)\n","\n","\n","# https://stackoverflow.com/questions/4950155/objects-as-keys-in-python-dictionaries\n","class BlockType(Enum):\n","    CONV = enum.auto()\n","    IDENTITY = enum.auto()\n","\n","    def __eq__(self, other):\n","        return self.name == other.name and self.value == other.value\n","\n","    def __hash__(self):\n","        return hash(f\"{self.name}:{self.value}\")\n","\n","# Generalize ConvBlock and Identity block as ResidualBlock:\n","# https://github.com/maciejbalawejder/Deep-Learning-Collection/blob/main/ConvNets/ResNeXt/resnext_pytorch.py\n","class ResidualBlock(Module):\n","    def __init__(self, in_channels, out_channels, block_type: BlockType, stride, cardinatlity=32):\n","        super().__init__()\n","        assert out_channels % 32 == 0\n","        self.C = cardinatlity\n","        self.block_type = block_type\n","        inner_channels = out_channels // 2\n","        self.conv_tower = Sequential(\n","            Conv2d_BN(in_channels, inner_channels, kernel=1),\n","            Conv2d_BN(inner_channels, inner_channels, kernel=3, stride=stride, padding=1, groups=self.C),\n","            Conv2d_BN(inner_channels, out_channels, kernel=1, acti=None)\n","        )\n","        if self.block_type is BlockType.CONV:\n","            self.downsample = Conv2d_BN(in_channels, out_channels, kernel=1, stride=stride, acti=None)\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower(x)\n","        if self.block_type is BlockType.CONV:\n","            x = self.downsample(x)\n","        out = self.relu(torch.add(out,x))\n","        return out\n","\n","    def out_channels(self):\n","      # unable to access output size of MaxPool2d, use hard-coded formula instead\n","      gen = self.conv_tower.children()\n","      last = next(gen)\n","      for last in gen: pass\n","      return last.out_channels()\n","\n","\n","# taken from https://github.com/reppertj/earworm/blob/a2d8a70085748da5db378f7f5f68ad8c2926a274/modeling/music_metric_learning/modules/inception.py#L93\n","class ReductionBlock(Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        # previously mistakenly thought conv3_pooling_out is this layer out_channel, turn out that the out_channel is the total channels of the 3 layers\n","        # assert (out_channels * 8 // 3) % 16 == 0\n","        # conv5_out = out_channels * 2 // 3\n","        # conv3_pooling_out = out_channels\n","\n","        assert out_channels % 16 == 0\n","        conv5_out = out_channels // 4\n","        conv3_pooling_out = conv5_out * 3 // 2\n","        self.conv3 = Conv2d_BN(in_channels, conv3_pooling_out, kernel=3, stride=2)\n","        self.conv5 = Sequential(\n","            Conv2d_BN(in_channels       , conv5_out * 3 // 4, kernel=1),\n","            Conv2d_BN(conv5_out * 3 // 4, conv5_out * 7 // 8, kernel=3, padding=1),\n","            Conv2d_BN(conv5_out * 7 // 8, conv5_out         , kernel=3, stride=2),\n","        )\n","        self.pooling = Sequential(\n","            MaxPool2d(3, stride=2),\n","            Conv2d_BN(in_channels, conv3_pooling_out, kernel=1),\n","        )\n","\n","    def forward(self, x):\n","        return torch.cat((self.conv3(x), self.conv5(x), self.pooling(x)), dim=1)\n","\n","\n","IN_CHAN=\"in_chan\"\n","OUT_CHAN=\"out_chan\"\n","class CovidNetBlock(Module):\n","    def __init__(self, residual_block_layout: dict, out_channels):\n","        super().__init__()\n","        chan_dict_list = [l for k, v in residual_block_layout.items() for l in v if isinstance(k, BlockType) and isinstance(v, list) and isinstance(l, dict)]\n","\n","        print(f\"Creating CovidNetBlock with layout length of {len(chan_dict_list)}\")\n","        self.blocks = Sequential()\n","        for k,v in residual_block_layout.items():\n","            for chan_dict in v:\n","                # print(f\"DEBUG_LOG - creating ResidualBlock with in_chan:{chan_dict[IN_CHAN]}; out_chan:{chan_dict[OUT_CHAN]}; block_type:{k}\")\n","                self.blocks.append(ResidualBlock(chan_dict[IN_CHAN], chan_dict[OUT_CHAN], k, 1))\n","        last_out_chan = chan_dict_list[-1][OUT_CHAN] # get last layers output channels count\n","        self.blocks.append(ReductionBlock(last_out_chan, out_channels))\n","\n","    def forward(self, x):\n","        return self.blocks(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Full Model"]},{"cell_type":"code","execution_count":3,"metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa"},"outputs":[],"source":["# https://onlinegdb.com/t9CIm197r\n","class CovidnetModel(Module):\n","    def __init__(\n","        self, \n","        classes : int = 3,\n","        ):\n","        super().__init__()\n","        \n","        self.stem = StemBlock()\n","\n","        PRE_FC_OUT_CHAN = 192\n","        residual_block_layout = {\n","            BlockType.CONV:[\n","                dict(in_chan=192, out_chan=256),\n","                dict(in_chan=256, out_chan=512),\n","                dict(in_chan=512, out_chan=1024),\n","            ],\n","            BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n","        }\n","        self.blocks = Sequential(\n","            CovidNetBlock(residual_block_layout, 192),\n","            CovidNetBlock(residual_block_layout, PRE_FC_OUT_CHAN)\n","        )\n","        \n","        self.global_avg_pool = AdaptiveAvgPool2d((1,1))\n","        self.fc = Linear(PRE_FC_OUT_CHAN, classes)\n","\n","    def forward(self, x):\n","        # 3 x 224 x 224\n","        x = self.stem(x)\n","        # 192 x 28 x 28\n","        x = self.blocks(x)\n","        x = self.global_avg_pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Training & Validation\n","\n","------------------------------------------------------------------------"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:19.895641Z","iopub.status.busy":"2022-11-17T15:13:19.895051Z","iopub.status.idle":"2022-11-17T15:13:19.903105Z","shell.execute_reply":"2022-11-17T15:13:19.901963Z","shell.execute_reply.started":"2022-11-17T15:13:19.895607Z"},"trusted":true},"outputs":[],"source":["LOG_DIR = os.path.join(CURR_DIR, \"log\")\n","RESULT_DIR = os.path.join(CURR_DIR, 'result')\n","curr_model = \"\"\n","\n","def log_to_file(txt=None, print_to_console_only=False):\n","  if txt is None:\n","    txt = ''\n","  txt += '\\n'\n","  print(txt)\n","  if print_to_console_only:\n","    return\n","  if not path.exists(LOG_DIR):\n","    os.mkdir(LOG_DIR)\n","  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n","  with open(full_path, mode='a') as f:\n","    f.write(txt)\n","    \n","# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n","# Make sure to delete any references to tensor. Else this function will not have significant effect\n","def clean_vram():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\n","def name_of_caller(frame=1):\n","    \"\"\"\n","    Return \"class.function_name\" of the caller or just \"function_name\".\n","    \"\"\"\n","    frame = sys._getframe(frame)\n","    fn_name = frame.f_code.co_name\n","    var_names = frame.f_code.co_varnames\n","    if var_names:\n","        if var_names[0] == \"self\":\n","            self_obj = frame.f_locals.get(\"self\")\n","            if self_obj is not None:\n","                return f\"{type(self_obj).__name__}.{fn_name}\" \n","        if var_names[0] == \"cls\":\n","            cls_obj = frame.f_locals.get(\"cls\")\n","            if cls_obj is not None:\n","                return f\"{cls_obj.__name__}.{fn_name}\"\n","    return fn_name"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Define Functions to Initialize Deep Learning Models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_constructors = {\n","  models.alexnet.__name__: models.alexnet, \n","  models.squeezenet1_1.__name__: models.squeezenet1_1,\n","  models.resnet50.__name__: models.resnet50, \n","  models.resnet101.__name__: models.resnet101,\n","  models.resnet152.__name__: models.resnet152, \n","  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n","  models.densenet201.__name__: models.densenet201, \n","  models.googlenet.__name__: models.googlenet, \n","  models.vgg16.__name__: models.vgg16, \n","  models.vgg19.__name__: models.vgg19, \n","  models.inception_v3.__name__: models.inception_v3, \n","  CovidnetModel.__name__: CovidnetModel,\n","}\n","\n","# This is only available in pytorch v0.13\n","# from torchvision.models import *\n","# model_weights = {\n","#   models.alexnet.__name__: models.AlexNet_Weights.DEFAULT,\n","#   models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n","#   models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n","#   models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n","#   models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n","#   models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n","#   models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n","#   models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n","#   models.vgg16.__name__: VGG16_Weights.DEFAULT,\n","#   models.vgg19.__name__: VGG19_Weights.DEFAULT,\n","#   models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n","# }\n","\n","# Experiment around dropout & Learning Rate & different optimizer (Adam)\n","def init_model(name):\n","  if not path.exists(RESULT_DIR):\n","    os.mkdir(RESULT_DIR)\n","\n","  clean_vram()\n","  seed_everything()\n","  \n","  # fine-tune pretrain models to our usecase\n","  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n","  NUM_CLASSES = len(class_names)\n","  DROPOUT = 0.5\n","  if name == CovidnetModel.__name__:\n","    model = model_constructors[name](NUM_CLASSES)\n","  else:\n","    model = model_constructors[name](True)\n","    if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n","      num_ftrs = model.classifier[6].in_features\n","      model.classifier[6] = Linear(num_ftrs, NUM_CLASSES)\n","      # model.classifier[6] = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Linear(num_ftrs, NUM_CLASSES)\n","      # )\n","    elif name == models.densenet201.__name__:\n","      num_ftrs = model.classifier.in_features\n","      model.classifier = Linear(num_ftrs, NUM_CLASSES)\n","      # model.classifier = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Linear(num_ftrs, NUM_CLASSES)\n","      # )\n","    elif name == models.squeezenet1_1.__name__:\n","      model.classifier = Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","      # model.classifier = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","      # )\n","      model.num_classes = NUM_CLASSES\n","    elif name == models.inception_v3.__name__:\n","      auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n","      model.AuxLogits.fc = Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","      # model.AuxLogits.fc = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","      # )\n","      primary_num_ftrs = model.fc.in_features\n","      model.fc = Linear(primary_num_ftrs, NUM_CLASSES)\n","      # model.fc = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Linear(primary_num_ftrs, NUM_CLASSES)\n","      # )\n","    else:\n","      # resnet, resnext & googlenet\n","      num_ftrs = model.fc.in_features\n","      model.fc= Linear(num_ftrs, NUM_CLASSES)\n","      # model.fc = Sequential(\n","      #   Dropout(DROPOUT),\n","      #   Linear(num_ftrs, NUM_CLASSES)\n","      # )\n","\n","  model = model.to(device)\n","  criterion = CrossEntropyLoss()\n","  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","  if is_gpu_avail():\n","    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n","    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n","    scaler = torch.cuda.amp.GradScaler()\n","  else:\n","    raise RuntimeError('This code only support machine with GPU.')\n","\n","  # print('=====================================')\n","  print(f'{name} is initialized')\n","  # print('=====================================')\n","  # print(model)\n","  return model, criterion, optimizer, scaler\n","\n","# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n","def save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n","  torch.save({\n","    'perf_metrics': perf_metrics,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    \"scaler_state_dict\": scaler.state_dict(),\n","    'history': history,\n","    }, model_path)\n","\n","def load_model(model, optimizer, scaler, model_path):\n","  if not os.path.exists(model_path):\n","    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n","    return None, model, optimizer, scaler, None, None\n","  checkpoint = torch.load(model_path)\n","  perf_metrics = checkpoint['perf_metrics']\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  # model.load_state_dict(checkpoint['model_state_dict'], strict=False) # https://stackoverflow.com/questions/54058256/runtimeerror-errors-in-loading-state-dict-for-resnet\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","  history = checkpoint['history']\n","  total_epoch = len(history) - 1\n","  del checkpoint\n","\n","  return perf_metrics, model, optimizer, scaler, history, total_epoch"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Define Function to Train Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:26.421026Z","iopub.status.busy":"2022-11-17T15:13:26.420649Z","iopub.status.idle":"2022-11-17T15:13:26.448404Z","shell.execute_reply":"2022-11-17T15:13:26.447170Z","shell.execute_reply.started":"2022-11-17T15:13:26.420992Z"},"trusted":true},"outputs":[],"source":["# training and validation loops\n","def train(model,\n","    criterion,\n","    optimizer,\n","    scaler,\n","    train_dataloader,\n","    valid_dataloader,\n","    model_path,\n","    max_epochs_stop=10,\n","    n_epochs=400,\n","    min_epoch=300,\n","    print_every=1):\n","    \n","    epochs_no_improve = 0\n","    perf = {\n","        'best_epoch': 0,\n","        'valid_loss_min': np.Inf,\n","        'valid_best_acc': 0,\n","    }\n","    total_epoch = 0\n","\n","    try:\n","        if os.path.exists(model_path):\n","            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n","            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n","            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n","        else:\n","            history = []\n","            log_to_file(f'Starting Training from Scratch.\\n')\n","    except:\n","        history = []\n","        log_to_file(f'exception: start from scratch.\\n')\n","\n","    overall_start = time.time()\n","    if total_epoch >= n_epochs:\n","        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n","        history = pd.DataFrame(\n","            history,\n","            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","        return model, history, perf\n","\n","    seed_everything()\n","\n","    # Main loop - continue training on where we left off if there's a saved model\n","    for epoch in range(total_epoch, n_epochs):\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","        start = time.time()\n","        for ii, (data, target) in enumerate (train_dataloader):\n","            data, target = data.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","\n","            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n","            with torch.cuda.amp.autocast():\n","              # output, aux_output = model(data)\n","              # loss1 = criterion(output, target)\n","              # loss2 = criterion(aux_output, target)\n","              # loss = loss1 + 0.4*loss2\n","              output = model(data)\n","              loss = criterion(output, target)\n","            # loss.backward()\n","            # optimizer.step()\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * data.size(0)\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","            train_acc += accuracy.item() * data.size(0)\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n","            \n","            # cleanup to save VRAM\n","            del data, target\n","#             clean_vram()\n","\n","        # After training loops ends, start validation\n","        else:\n","            with torch.no_grad():\n","                model.eval()\n","                for data, target in valid_dataloader:\n","                    if is_gpu_avail():\n","                        data, target = data.cuda(), target.cuda()\n","                    output = model(data)\n","                    loss = criterion(output, target)\n","                    valid_loss += loss.item() * data.size(0)\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                        correct_tensor.type(torch.FloatTensor))\n","                    valid_acc += accuracy.item() * data.size(0)\n","                    \n","                    # cleanup to save VRAM\n","                    del data, target\n","#                     clean_vram()\n","                train_loss = train_loss / train_data_size\n","                valid_loss = valid_loss / valid_data_size\n","                train_acc = train_acc / train_data_size\n","                valid_acc = valid_acc / valid_data_size\n","                history.append([train_loss, valid_loss,train_acc, valid_acc])\n","                if (epoch + 1) % print_every == 0:\n","                    log_to_file(f'Epoch: {epoch}', True)\n","                    log_to_file(\n","                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n","                        True\n","                    )\n","                    log_to_file(\n","                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n","                        True\n","                    )\n","          \n","                if valid_loss < perf['valid_loss_min']:\n","                    epochs_no_improve = 0\n","                    perf['best_epoch'] = epoch\n","                    perf['valid_loss_min'] = valid_loss\n","                    perf['valid_best_acc'] = valid_acc\n","                    save_model(perf, model, optimizer, scaler, history, model_path)\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n","                        log_to_file(\n","                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n","                        )\n","                        total_time = time.time() - overall_start\n","                        log_to_file(\n","                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n","                        )\n","                        log_to_file()\n","\n","                        # Load the best state from saved model\n","                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","                        # save the full history\n","                        save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history, perf\n","    \n","    total_time = time.time() - overall_start\n","    log_to_file(\n","        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n","    )\n","    log_to_file(\n","        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n","    )\n","    log_to_file()\n","\n","    # Load the best state from saved model\n","    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","    # save the full history\n","    save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","    \n","    return model, history, perf\n","\n","\n","def save_train_val_loss_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_loss', 'valid_loss']:\n","      plt.plot(\n","          history[c], label=c)\n","\n","  title = f'{curr_model} - Training and Validation Losses'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Losses')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def save_train_val_acc_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_acc', 'valid_acc']:\n","      plt.plot(\n","          100 * history[c], label=c)\n","      \n","  title = f'{curr_model} - Training and Validation Accuracy'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Accuracy')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Define Functions to Visualize Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    # mean = np.array([0.485, 0.456, 0.406])\n","    # std = np.array([0.229, 0.224, 0.225])\n","    # inp = std * inp + mean\n","    # inp = np.clip(inp, 0, 1)\n","    plt.figure(figsize=[15, 15])\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","# confusion matrix \n","def getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n","    model.eval()\n","    confusion_matrix=np.zeros((2,2),dtype=int)\n","    num_images=test_data_size\n","    \n","    with torch.no_grad():\n","        for i, (data,target) in enumerate(dataloader):\n","            data = data.to(device)\n","            target = target.to(device)\n","            \n","            output = model(data) \n","            _, pred = torch.max(output, 1)\n","            \n","            for j in range(data.size()[0]): \n","                if pred[j]==1 and target[j]==1:\n","                    term='TP'\n","                    confusion_matrix[0][0]+=1\n","                elif pred[j]==1 and target[j]==0:\n","                    term='FP'\n","                    confusion_matrix[1][0]+=1\n","                elif pred[j]==0 and target[j]==1:\n","                    term='FN'\n","                    confusion_matrix[0][1]+=1\n","                elif pred[j]==0 and target[j]==0:\n","                    term='TN'\n","                    confusion_matrix[1][1]+=1\n","            \n","                if show_image:\n","                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n","                    log_to_file(term, print_to_console_only)\n","                    imshow(data.cpu().data[j])\n","        \n","        log_to_file(None, print_to_console_only)\n","        category = 'Test' if is_test else 'Validation'\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file(f'{category} Results ', print_to_console_only)\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file('Confusion Matrix: ', print_to_console_only)\n","        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","\n","        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","    return confusion_matrix\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def save_test_acc_n_loss_graph(model, dataloader, criterion):\n","  pass\n","  # NOT NEEDED YET\n","  # with torch.no_grad():\n","  #   model.eval()\n","  #   for data, target in dataloader:\n","  #       if is_gpu_avail():\n","  #           data, target = data.cuda(), target.cuda()\n","  #       output = model(data)\n","  #       loss = criterion(output, target)\n","  #       test_loss += loss.item() * data.size(0)\n","  #       _, pred = torch.max(output, dim=1)\n","  #       correct_tensor = pred.eq(target.data.view_as(pred))\n","  #       accuracy = torch.mean(\n","  #           correct_tensor.type(torch.FloatTensor))\n","  #       test_acc += accuracy.item() * data.size(0)\n","  #   train_loss = train_loss / train_data_size\n","  #   test_loss = test_loss / test_data_size\n","  #   train_acc = train_acc / train_data_size\n","  #   test_acc = test_acc / test_data_size\n","\n","\n","# def visualize_test_prediction(model):\n","#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n","#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n","#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n","\n","#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n","#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n","\n","#   # log_to_file(img_list)\n","\n","#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n","#                               for img in img_list])\n","#   pred_logits_tensor = model(test_batch)\n","#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n","\n","#   row = 12\n","#   col = 3\n","#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n","#   r = 0\n","#   c = 0\n","#   for i, img in enumerate(img_list):\n","#       if c >= col:\n","#         r += 1\n","#         c = 0\n","#       ax = axs[r, c]\n","#       ax.axis('off')\n","#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n","#                                                               100*pred_probs[i,1]))\n","#       ax.imshow(img)\n","#       c +=1\n","\n","#   title = f'{curr_model} - Covid Image Prediction'\n","#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","#   plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def getPredProbs(model, datasetStr, count, isSeeded=True):\n","  if isSeeded:\n","    seed_everything()\n","  \n","  dataset = data[datasetStr].samples\n","  img_list = []\n","  for i, (img_path, cls_idx) in enumerate(dataset):\n","    if i >= count:\n","      break\n","    img_list.append(Image.open(img_path).convert('RGB'))\n","\n","  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n","                              for img in img_list])\n","  pred_logits_tensor = model(test_batch)\n","  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Run all models - Init Models + Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:34.458786Z","iopub.status.busy":"2022-11-17T15:13:34.458420Z"},"trusted":true},"outputs":[],"source":["model_list = [\n","    models.alexnet.__name__, # 0\n","    models.squeezenet1_1.__name__, #1\n","    models.resnet50.__name__, # 2\n","    models.resnet101.__name__, # 3\n","    models.resnet152.__name__, # 4\n","    models.resnext101_32x8d.__name__, # 5\n","    models.densenet201.__name__, # 6\n","    models.googlenet.__name__, # 7\n","    models.vgg16.__name__, # 8\n","    models.vgg19.__name__, #9\n","    models.inception_v3.__name__, #10\n","    CovidnetModel.__name__, #11\n","]\n","\n","for i in range(0, len(model_list)):\n","  # https://github.com/pytorch/pytorch/issues/50198\n","  # skipped these because cannot use deterministic algorithm\n","#   skip_model = [0, 1, 5, 8, 9, 10]\n","  skip_model = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","  if i in skip_model:\n","    continue\n","  curr_model = model_list[i]\n","\n","  # Initialize model, criterion and optimizer\n","  model, criterion, optimizer, scaler = init_model(curr_model)\n","\n","#   Training & Validation\n","  model, history, perf = train(\n","      model,\n","      criterion,\n","      optimizer,\n","      scaler,\n","      train_loader,\n","      val_loader,\n","      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n","      max_epochs_stop=5,  # Early stopping intialization\n","      n_epochs=1,\n","      min_epoch=1,\n","      print_every=10)\n","\n","  history\n","  save_train_val_loss_graph(history, perf)\n","  save_train_val_acc_graph(history, perf)\n","  getConfusionMatrix(model, val_loader)"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Test (Evaluation)\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Playground\n","\n","------------------------------------------------------------------------\n","\n","> Testbed that is not compulsary for any part of this notebook"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Models using PytorchViz\n","\n","-   https://pytorch.org/docs/stable/nn.html\n","-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n","-   [Mandrin explanation of pytorch resnet\n","    code](https://www.jianshu.com/p/90d61f53d15d)"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchviz import make_dot, make_dot_from_trace"]},{"cell_type":"code","execution_count":5,"metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating CovidNetBlock with layout length of 4\n","Creating CovidNetBlock with layout length of 4\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","         LeakyReLU-3         [-1, 64, 112, 112]               0\n","         Conv2d_BN-4         [-1, 64, 112, 112]               0\n","         MaxPool2d-5           [-1, 64, 56, 56]               0\n","            Conv2d-6           [-1, 80, 56, 56]           5,120\n","       BatchNorm2d-7           [-1, 80, 56, 56]             160\n","         LeakyReLU-8           [-1, 80, 56, 56]               0\n","         Conv2d_BN-9           [-1, 80, 56, 56]               0\n","           Conv2d-10          [-1, 192, 56, 56]         138,240\n","      BatchNorm2d-11          [-1, 192, 56, 56]             384\n","        LeakyReLU-12          [-1, 192, 56, 56]               0\n","        Conv2d_BN-13          [-1, 192, 56, 56]               0\n","        MaxPool2d-14          [-1, 192, 28, 28]               0\n","        StemBlock-15          [-1, 192, 28, 28]               0\n","           Conv2d-16          [-1, 128, 28, 28]          24,576\n","      BatchNorm2d-17          [-1, 128, 28, 28]             256\n","        LeakyReLU-18          [-1, 128, 28, 28]               0\n","        Conv2d_BN-19          [-1, 128, 28, 28]               0\n","           Conv2d-20          [-1, 128, 28, 28]           4,608\n","      BatchNorm2d-21          [-1, 128, 28, 28]             256\n","        LeakyReLU-22          [-1, 128, 28, 28]               0\n","        Conv2d_BN-23          [-1, 128, 28, 28]               0\n","           Conv2d-24          [-1, 256, 28, 28]          32,768\n","      BatchNorm2d-25          [-1, 256, 28, 28]             512\n","        Conv2d_BN-26          [-1, 256, 28, 28]               0\n","           Conv2d-27          [-1, 256, 28, 28]          49,152\n","      BatchNorm2d-28          [-1, 256, 28, 28]             512\n","        Conv2d_BN-29          [-1, 256, 28, 28]               0\n","        LeakyReLU-30          [-1, 256, 28, 28]               0\n","    ResidualBlock-31          [-1, 256, 28, 28]               0\n","           Conv2d-32          [-1, 256, 28, 28]          65,536\n","      BatchNorm2d-33          [-1, 256, 28, 28]             512\n","        LeakyReLU-34          [-1, 256, 28, 28]               0\n","        Conv2d_BN-35          [-1, 256, 28, 28]               0\n","           Conv2d-36          [-1, 256, 28, 28]          18,432\n","      BatchNorm2d-37          [-1, 256, 28, 28]             512\n","        LeakyReLU-38          [-1, 256, 28, 28]               0\n","        Conv2d_BN-39          [-1, 256, 28, 28]               0\n","           Conv2d-40          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n","        Conv2d_BN-42          [-1, 512, 28, 28]               0\n","           Conv2d-43          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n","        Conv2d_BN-45          [-1, 512, 28, 28]               0\n","        LeakyReLU-46          [-1, 512, 28, 28]               0\n","    ResidualBlock-47          [-1, 512, 28, 28]               0\n","           Conv2d-48          [-1, 512, 28, 28]         262,144\n","      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-50          [-1, 512, 28, 28]               0\n","        Conv2d_BN-51          [-1, 512, 28, 28]               0\n","           Conv2d-52          [-1, 512, 28, 28]          73,728\n","      BatchNorm2d-53          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-54          [-1, 512, 28, 28]               0\n","        Conv2d_BN-55          [-1, 512, 28, 28]               0\n","           Conv2d-56         [-1, 1024, 28, 28]         524,288\n","      BatchNorm2d-57         [-1, 1024, 28, 28]           2,048\n","        Conv2d_BN-58         [-1, 1024, 28, 28]               0\n","           Conv2d-59         [-1, 1024, 28, 28]         524,288\n","      BatchNorm2d-60         [-1, 1024, 28, 28]           2,048\n","        Conv2d_BN-61         [-1, 1024, 28, 28]               0\n","        LeakyReLU-62         [-1, 1024, 28, 28]               0\n","    ResidualBlock-63         [-1, 1024, 28, 28]               0\n","           Conv2d-64          [-1, 512, 28, 28]         524,288\n","      BatchNorm2d-65          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-66          [-1, 512, 28, 28]               0\n","        Conv2d_BN-67          [-1, 512, 28, 28]               0\n","           Conv2d-68          [-1, 512, 28, 28]          73,728\n","      BatchNorm2d-69          [-1, 512, 28, 28]           1,024\n","        LeakyReLU-70          [-1, 512, 28, 28]               0\n","        Conv2d_BN-71          [-1, 512, 28, 28]               0\n","           Conv2d-72         [-1, 1024, 28, 28]         524,288\n","      BatchNorm2d-73         [-1, 1024, 28, 28]           2,048\n","        Conv2d_BN-74         [-1, 1024, 28, 28]               0\n","        LeakyReLU-75         [-1, 1024, 28, 28]               0\n","    ResidualBlock-76         [-1, 1024, 28, 28]               0\n","           Conv2d-77           [-1, 72, 13, 13]         663,552\n","      BatchNorm2d-78           [-1, 72, 13, 13]             144\n","        LeakyReLU-79           [-1, 72, 13, 13]               0\n","        Conv2d_BN-80           [-1, 72, 13, 13]               0\n","           Conv2d-81           [-1, 36, 28, 28]          36,864\n","      BatchNorm2d-82           [-1, 36, 28, 28]              72\n","        LeakyReLU-83           [-1, 36, 28, 28]               0\n","        Conv2d_BN-84           [-1, 36, 28, 28]               0\n","           Conv2d-85           [-1, 42, 28, 28]          13,608\n","      BatchNorm2d-86           [-1, 42, 28, 28]              84\n","        LeakyReLU-87           [-1, 42, 28, 28]               0\n","        Conv2d_BN-88           [-1, 42, 28, 28]               0\n","           Conv2d-89           [-1, 48, 13, 13]          18,144\n","      BatchNorm2d-90           [-1, 48, 13, 13]              96\n","        LeakyReLU-91           [-1, 48, 13, 13]               0\n","        Conv2d_BN-92           [-1, 48, 13, 13]               0\n","        MaxPool2d-93         [-1, 1024, 13, 13]               0\n","           Conv2d-94           [-1, 72, 13, 13]          73,728\n","      BatchNorm2d-95           [-1, 72, 13, 13]             144\n","        LeakyReLU-96           [-1, 72, 13, 13]               0\n","        Conv2d_BN-97           [-1, 72, 13, 13]               0\n","   ReductionBlock-98          [-1, 192, 13, 13]               0\n","    CovidNetBlock-99          [-1, 192, 13, 13]               0\n","          Conv2d-100          [-1, 128, 13, 13]          24,576\n","     BatchNorm2d-101          [-1, 128, 13, 13]             256\n","       LeakyReLU-102          [-1, 128, 13, 13]               0\n","       Conv2d_BN-103          [-1, 128, 13, 13]               0\n","          Conv2d-104          [-1, 128, 13, 13]           4,608\n","     BatchNorm2d-105          [-1, 128, 13, 13]             256\n","       LeakyReLU-106          [-1, 128, 13, 13]               0\n","       Conv2d_BN-107          [-1, 128, 13, 13]               0\n","          Conv2d-108          [-1, 256, 13, 13]          32,768\n","     BatchNorm2d-109          [-1, 256, 13, 13]             512\n","       Conv2d_BN-110          [-1, 256, 13, 13]               0\n","          Conv2d-111          [-1, 256, 13, 13]          49,152\n","     BatchNorm2d-112          [-1, 256, 13, 13]             512\n","       Conv2d_BN-113          [-1, 256, 13, 13]               0\n","       LeakyReLU-114          [-1, 256, 13, 13]               0\n","   ResidualBlock-115          [-1, 256, 13, 13]               0\n","          Conv2d-116          [-1, 256, 13, 13]          65,536\n","     BatchNorm2d-117          [-1, 256, 13, 13]             512\n","       LeakyReLU-118          [-1, 256, 13, 13]               0\n","       Conv2d_BN-119          [-1, 256, 13, 13]               0\n","          Conv2d-120          [-1, 256, 13, 13]          18,432\n","     BatchNorm2d-121          [-1, 256, 13, 13]             512\n","       LeakyReLU-122          [-1, 256, 13, 13]               0\n","       Conv2d_BN-123          [-1, 256, 13, 13]               0\n","          Conv2d-124          [-1, 512, 13, 13]         131,072\n","     BatchNorm2d-125          [-1, 512, 13, 13]           1,024\n","       Conv2d_BN-126          [-1, 512, 13, 13]               0\n","          Conv2d-127          [-1, 512, 13, 13]         131,072\n","     BatchNorm2d-128          [-1, 512, 13, 13]           1,024\n","       Conv2d_BN-129          [-1, 512, 13, 13]               0\n","       LeakyReLU-130          [-1, 512, 13, 13]               0\n","   ResidualBlock-131          [-1, 512, 13, 13]               0\n","          Conv2d-132          [-1, 512, 13, 13]         262,144\n","     BatchNorm2d-133          [-1, 512, 13, 13]           1,024\n","       LeakyReLU-134          [-1, 512, 13, 13]               0\n","       Conv2d_BN-135          [-1, 512, 13, 13]               0\n","          Conv2d-136          [-1, 512, 13, 13]          73,728\n","     BatchNorm2d-137          [-1, 512, 13, 13]           1,024\n","       LeakyReLU-138          [-1, 512, 13, 13]               0\n","       Conv2d_BN-139          [-1, 512, 13, 13]               0\n","          Conv2d-140         [-1, 1024, 13, 13]         524,288\n","     BatchNorm2d-141         [-1, 1024, 13, 13]           2,048\n","       Conv2d_BN-142         [-1, 1024, 13, 13]               0\n","          Conv2d-143         [-1, 1024, 13, 13]         524,288\n","     BatchNorm2d-144         [-1, 1024, 13, 13]           2,048\n","       Conv2d_BN-145         [-1, 1024, 13, 13]               0\n","       LeakyReLU-146         [-1, 1024, 13, 13]               0\n","   ResidualBlock-147         [-1, 1024, 13, 13]               0\n","          Conv2d-148          [-1, 512, 13, 13]         524,288\n","     BatchNorm2d-149          [-1, 512, 13, 13]           1,024\n","       LeakyReLU-150          [-1, 512, 13, 13]               0\n","       Conv2d_BN-151          [-1, 512, 13, 13]               0\n","          Conv2d-152          [-1, 512, 13, 13]          73,728\n","     BatchNorm2d-153          [-1, 512, 13, 13]           1,024\n","       LeakyReLU-154          [-1, 512, 13, 13]               0\n","       Conv2d_BN-155          [-1, 512, 13, 13]               0\n","          Conv2d-156         [-1, 1024, 13, 13]         524,288\n","     BatchNorm2d-157         [-1, 1024, 13, 13]           2,048\n","       Conv2d_BN-158         [-1, 1024, 13, 13]               0\n","       LeakyReLU-159         [-1, 1024, 13, 13]               0\n","   ResidualBlock-160         [-1, 1024, 13, 13]               0\n","          Conv2d-161             [-1, 72, 6, 6]         663,552\n","     BatchNorm2d-162             [-1, 72, 6, 6]             144\n","       LeakyReLU-163             [-1, 72, 6, 6]               0\n","       Conv2d_BN-164             [-1, 72, 6, 6]               0\n","          Conv2d-165           [-1, 36, 13, 13]          36,864\n","     BatchNorm2d-166           [-1, 36, 13, 13]              72\n","       LeakyReLU-167           [-1, 36, 13, 13]               0\n","       Conv2d_BN-168           [-1, 36, 13, 13]               0\n","          Conv2d-169           [-1, 42, 13, 13]          13,608\n","     BatchNorm2d-170           [-1, 42, 13, 13]              84\n","       LeakyReLU-171           [-1, 42, 13, 13]               0\n","       Conv2d_BN-172           [-1, 42, 13, 13]               0\n","          Conv2d-173             [-1, 48, 6, 6]          18,144\n","     BatchNorm2d-174             [-1, 48, 6, 6]              96\n","       LeakyReLU-175             [-1, 48, 6, 6]               0\n","       Conv2d_BN-176             [-1, 48, 6, 6]               0\n","       MaxPool2d-177           [-1, 1024, 6, 6]               0\n","          Conv2d-178             [-1, 72, 6, 6]          73,728\n","     BatchNorm2d-179             [-1, 72, 6, 6]             144\n","       LeakyReLU-180             [-1, 72, 6, 6]               0\n","       Conv2d_BN-181             [-1, 72, 6, 6]               0\n","  ReductionBlock-182            [-1, 192, 6, 6]               0\n","   CovidNetBlock-183            [-1, 192, 6, 6]               0\n","AdaptiveAvgPool2d-184            [-1, 192, 1, 1]               0\n","          Linear-185                    [-1, 3]             579\n","================================================================\n","Total params: 7,724,523\n","Trainable params: 7,724,523\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 283.39\n","Params size (MB): 29.47\n","Estimated Total Size (MB): 313.43\n","----------------------------------------------------------------\n"]}],"source":["# from torchinfo import summary\n","from torchsummary import summary\n","\n","if is_gpu_avail():\n","    device = torch.device('cuda')\n","    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","else:\n","    device = torch.device('cpu')\n","\n","\n","# model = models.googlenet()\n","# summary(model, (3, 299, 299))\n","\n","# model = ResidualBlock(192, 192, BlockType.IDENTITY, 1)\n","# summary(model, (192, 28, 28))\n","\n","# model = ResidualBlock(192, 256, BlockType.CONV, 1) # also works if 192,192\n","# summary(model, (192, 28, 28))\n","\n","# model = ResidualBlock(256, 320, BlockType.CONV, 1) # also works if 192,192\n","# summary(model, (256, 28, 28))\n","\n","# model = ReductionBlock(1024, 192)\n","# summary(model, (1024, 28, 28))\n","\n","# residual_block_layout = {\n","#     BlockType.CONV:[\n","#         dict(in_chan=192, out_chan=256),\n","#         dict(in_chan=256, out_chan=512),\n","#         dict(in_chan=512, out_chan=1024),\n","#     ],\n","#     BlockType.IDENTITY:[dict(in_chan=1024, out_chan=1024)]\n","# }\n","# model = CovidNetBlock(residual_block_layout, 192) \n","# summary(model, (192, 28, 28))\n","\n","model = CovidnetModel()\n","summary(model, (3, 224, 224))\n","\n","# model = models.resnext50_32x4d()\n","# summary(model, (3, 224, 224))\n","\n","# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n","# y = model(x)\n","# dot = make_dot(y, params=dict(model.named_parameters()))\n","\n","# dot.format = \"png\"\n","# dot.render(render_model_pic_file)\n","# files.download(f\"{render_model_pic_file}.{dot.format}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigate Image in Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"outputs":[],"source":["import subprocess\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","DATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","\n","# count number of images\n","# !ls -Uba1 /content/data/3A_images | grep -c png\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\n","print(f\"number of images: {output.decode()}\")\n","\n","# list first 10 images\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\n","first_10_lines = output.decode()\n","# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\n","img_list = first_10_lines.split('\\n')\n","first_png = next(filter(lambda img: \"png\" in img, img_list), None)\n","print(f\"first_png: {first_png}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"outputs":[],"source":["# show the first test image, unbounded followed by bounded\n","import torch\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as torch_func_trans\n","from PIL import Image\n","\n","# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","def get_data_from_split_file(split_file):\n","    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","    files, classes, bboxes = [], [], []\n","    with open(split_file, 'r') as f:\n","        for line in f.readlines():\n","            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","            files.append(path.join(DATASET_DIR, fname))\n","            classes.append(int(cls))\n","            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","    return files, classes, bboxes\n","\n","def bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n","    top = ymax\n","    left = xmin\n","    height = ymax - ymin\n","    width = xmax - xmin\n","    return top, left, height, width\n","\n","files, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\n","first_file_tuple = (files[0], classes[0], bbox[0])\n","\n","print(f\"first image: {first_file_tuple[0]}\")\n","if 0:\n","    # this way of reading image is deprecated\n","    img = plt.imread(first_file_tuple[0])\n","\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"unbounded\")\n","    _ = plt.imshow(img)\n","    _ = plt.axis('off')\n","\n","    torch_img = torch.from_numpy(img)\n","    print(f\"torch_img size: {torch_img.size()}\")\n","    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n","    print(f\"pytorch_size: {pytorch_size}\")\n","\n","    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"bounded\")\n","    _ = plt.imshow(cropped_img)\n","    _ = plt.axis('off')\n","\n","# This is the recommended method for opening image\n","# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\n","with Image.open(first_file_tuple[0]) as im:\n","    print(\"\\n\")\n","    print(\"Unbounded image\")\n","    IMG = im\n","    display(im)\n","\n","    print(\"\\n\")\n","    print(\"Bounded image\")\n","    display(im.crop(first_file_tuple[2]))\n","\n","    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n","    print(\"\\n\")\n","    print(\"Out of Scope Unbounded image\")\n","    display(IMG)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Investigate metadata.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.expand_frame_repr', False)\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","meta_df = pd.read_csv(METADATA_CSV)\n","print(\"First 5 rows in metadata.csv0\")\n","print(meta_df.head(5))\n","print(f\"classes: {sorted(meta_df['finding'].unique())}\")\n","\n","unverified = meta_df['verified finding'].eq('No').sum()\n","verified = meta_df['verified finding'].eq('Yes').sum()\n","print(f\"Not verified: {unverified}\")\n","print(f\"Verified: {verified}\")\n","\n","unverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\n","assert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\n","print(f\"first unverified ID: {unverified_patient_ids[0]}\")\n","\n","verified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\n","assert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\n","print(f\"first verified ID: {verified_patient_ids[0]}\")\n","\n","imgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\n","print(f\"total images: {len(imgs)}\")\n","print(f\"first 10 images: {imgs[:10]}\")\n","imgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\n","print(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigating split dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"outputs":[],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = f\"{CURR_DIR}/data\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_SIZE)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","SEED = 18\n","seed_everything(SEED)\n","BATCH_SIZE = 128\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","# train_index, test_index = next(\n","#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n","#         dummy_X, full_dataset.targets\n","#     )\n","# )\n","\n","print('*'*50)\n","print(\"temp_train\")\n","print('*'*50)\n","print(f\"First 10 index: {temp_train_index[:10]}\")\n","print(f\"First 10 label: {y_remaining[:10]}\")\n","print(f\"length of index: {len(temp_train_index)}\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {y_remaining.count(0)}\")\n","print(f\"Pneunomia: {y_remaining.count(1)}\")\n","print(f\"Covid-19: {y_remaining.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"test\")\n","print('*'*50)\n","print(f\"First 10 index: {test_index[:10]}\")\n","print(f\"First 10 label: {Y_test[:10]}\")\n","print(f\"length of index: {len(test_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_test.count(0)}\")\n","print(f\"Pneunomia: {Y_test.count(1)}\")\n","print(f\"Covid-19: {Y_test.count(2)}\")\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","X_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","print()\n","print('*'*50)\n","print(\"train\")\n","print('*'*50)\n","print(f\"First 10 index: {train_index[:10]}\")\n","print(f\"First 10 label: {Y_train[:10]}\")\n","print(f\"length of index: {len(train_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_train.count(0)}\")\n","print(f\"Pneunomia: {Y_train.count(1)}\")\n","print(f\"Covid-19: {Y_train.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"val\")\n","print('*'*50)\n","print(f\"First 10 index: {val_index[:10]}\")\n","print(f\"First 10 label: {Y_val[:10]}\")\n","print(f\"length of index: {len(val_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_val.count(0)}\")\n","print(f\"Pneunomia: {Y_val.count(1)}\")\n","print(f\"Covid-19: {Y_val.count(2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"outputs":[],"source":["import pprint\n","def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    # mean = np.array([0.485, 0.456, 0.406])\n","    # std = np.array([0.229, 0.224, 0.225])\n","    # inp = std * inp + mean\n","    # inp = np.clip(inp, 0, 1)\n","    plt.figure(figsize=[15, 15])\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","seed_everything(19)\n","class_names, _ = full_dataset.find_classes()\n","data, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n","# out = torchvision.utils.make_grid(data)\n","# imshow(out)\n","# pp = pprint.PrettyPrinter(compact=True)\n","# pp.pprint([class_names[x] for x in classes])\n","\n","print(f\"Class: {classes[0]}\")\n","out = data[0]\n","imshow(out)"]},{"cell_type":"markdown","metadata":{},"source":["## Trying Avalance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://changhsinlee.com/colab-import-python/\n","!pip install requests\n","!pip install avalanche-lib"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","\n","# Save datagenerators as file to colab working directory\n","# If you are using GitHub, make sure you get the \"Raw\" version of the code\n","url = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\n","r = requests.get(url)\n","\n","# make sure your filename is the same as how you want to import \n","with open('pytorchcv_models.py', 'w') as f:\n","    f.write(r.text)\n","\n","# now we can import\n","import pytorchcv_models as pycv\n","from types import SimpleNamespace\n","\n","args = SimpleNamespace()\n","args.cuda = 0\n","pycv.main(args)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Old Conv and Identity Block"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Contructed by referring ResNext50 visualized in TorchStudio\n","class ConvBlock(Module):\n","    def __init__(self, in_channels, tower_height):\n","        super().__init__()\n","        self.conv_tower = ModuleList(\n","            Conv2d_BN(in_channels, out_channels=128, kernel=1),\n","            Conv2d_BN(in_channels=128, out_channels=128, kernel=3, padding=1, groups=tower_height),\n","            Conv2d_BN(in_channels=128, out_channels=256, kernel=1, acti=None),\n","            Conv2d_BN(in_channels, out_channels=256, kernel=1, acti=None)\n","        )\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower[0](x)\n","        out = self.conv_tower[1](out)\n","        out = torch.add(self.conv_tower[2](out), self.conv_tower[3](x))\n","        return self.relu(out)\n","\n","\n","# Contructed by referring to ConvBlock\n","class IdentityBlock(Module):\n","    def __init__(self, in_channels, tower_height):\n","        super().__init__()\n","        self.conv_tower = ModuleList(\n","            Conv2d_BN(in_channels, out_channels=128, kernel=1),\n","            Conv2d_BN(in_channels=128, out_channels=128, kernel=3, padding=1, groups=tower_height),\n","            Conv2d_BN(in_channels=128, out_channels=256, kernel=1, acti=None)\n","        )\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower[0](x)\n","        out = self.conv_tower[1](out)\n","        out = torch.add(self.conv_tower[2](out), x)\n","        return self.relu(out)"]},{"cell_type":"markdown","metadata":{},"source":["## Out of memory issue\n","\n","- References\n","    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n","    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n","    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n","        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n","    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n","    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n","- The fix\n","    - Delete unused tensor, force garbage collection and run `empty_cache()`\n","    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.memory_stats(device)\n","# print(torch.cuda.memory_summary(device))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.__version__"]}],"metadata":{"kernelspec":{"display_name":"pytorch","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"36aecb82383d45a5d90cd16d2a70e6def28d8cf6813a9e24d3e39991fa43b598"}}},"nbformat":4,"nbformat_minor":4}
