{"cells":[{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Summary\n","\n","------------------------------------------------------------------------\n","\n","> Expand to see summary and details"]},{"cell_type":"markdown","metadata":{},"source":["## Overview and Explanation\n","\n","1.  This notebook reuses a lot of the [original transfer learning\n","    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n","    -   Here the focus is on building the new custom model using the\n","        CovidNet-CT database.\n","2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n","    -   is not longer needed for notebook running in kaggle. Remained\n","        here for references only\n","    -   is where the dataset is being acquired.\n","    -   Explanation of various phases in the [CovidNet-CT ML\n","        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n","        -   train phase is train phase\n","        -   test phase is validation phase\n","        -   infer phase is test phase\n","3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n","    -   refers to the way [CovidNet-CT preprocess its\n","        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n","    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n","        to use PyTorch\n","    -   Two highlights\n","        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n","            used by the imagenet model\n","        -   the image is cropped to the bounding box provided with the\n","            dataset before resize to 512x512\n","4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n","    -   refers to [how CovidNet-CT\n","        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n","    -   This part is almost identical to the original transfer learning\n","        model notebook."]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Import and Deterministic Setup\n","\n","------------------------------------------------------------------------\n","\n","All modules will be imported here including modules used in the\n","[Playground](#playground) section"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:44.921196Z","iopub.status.busy":"2022-11-17T15:12:44.920745Z","iopub.status.idle":"2022-11-17T15:12:45.868415Z","shell.execute_reply":"2022-11-17T15:12:45.866381Z","shell.execute_reply.started":"2022-11-17T15:12:44.921088Z"},"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: CUDA_LAUNCH_BLOCKING=1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n","env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:22\n"]}],"source":["from __future__ import print_function, division\n","import os\n","%env CUDA_LAUNCH_BLOCKING=1\n","import random\n","import numpy as np\n","import torch\n","\n","from os import path\n","import zipfile\n","import math\n","import sys\n","\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from PIL import Image\n","from sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\n","\n","import torchvision\n","from torchvision import models, transforms #, datasets\n","import matplotlib.pyplot as plt\n","\n","from torch.nn import Module, Sequential, LeakyReLU, Conv2d, BatchNorm2d, ModuleList, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Linear\n","from torch import optim\n","from torch.optim import lr_scheduler\n","import time\n","import torch.nn.functional as F\n","\n","import gc\n","import pandas as pd\n","from enum import Enum\n","\n","# ensure reproducibility across different executions\n","# https://pytorch.org/docs/stable/notes/randomness.html\n","# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n","# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n","SEED = 18\n","def seed_everything(seed=18):\n","    random.seed(seed)\n","    %env PYTHONHASHSEED=$seed\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","#torch.set_deterministic(True)\n","torch.use_deterministic_algorithms(True)\n","%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n","%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:22\n","\n","def is_gpu_avail():\n","    GPU_DETECTED = False\n","    try:\n","        GPU_DETECTED = torch.cuda.is_available()\n","    except:\n","        pass\n","\n","    return GPU_DETECTED"]},{"cell_type":"markdown","metadata":{"id":"wMQLloEgzPol"},"source":["---\n","# Setup Kaggle data access to download dataset\n","---\n","\n","- [dataset source from kaggle](https://www.kaggle.com/datasets/hgunraj/covidxct)\n","- On 02-06-2022, CT-3A datasets with 425,024 CT images are uploaded. \n","    - The way of how CT-3A and CT-3B are constructured can be found in [this link](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md).\n","    - This notebook only uses CT-3A that is already constructed and ready for download from the kaggle source."]},{"cell_type":"markdown","metadata":{"id":"w_OfYhgp1V1z"},"source":["## Use Kaggle API to download dataset\n","\n","### Solving google drive not enough capacity to store data\n","- [Article explaining how to access Kaggle Data in colab](https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7)\n","- [An old reddit post that seems to be relevant](https://old.reddit.com/r/PiratedGames/comments/uypcw2/use_google_colab_to_mass_transfer_files_from/)\n","- Some rclone idea\n","    - [rclone gui](https://rclone.org/gui/)\n","    - [rclone lab](https://github.com/acamposxp/RcloneLab)\n","    - [rclone idea 1](https://github.com/ella-tj/Any-File-2-GDrive)\n","    - [rclone idea 2](https://github.com/eaustin6/Rclone-Setup-on-Google-Colab)\n","    - [rclone idea 2](https://towardsdatascience.com/why-you-should-try-rclone-with-google-drive-and-colab-753f3ec04ba1)\n","    - [reddit colab thread](https://old.reddit.com/r/DataHoarder/comments/g069dx/google_colab_is_like_running_code_from_your/)\n","    - [socialgrep on rclone and colab](https://socialgrep.com/search?query=rclone%2Ccolab)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"CZdji66q0Zpt","outputId":"b8a595be-ee84-4084-c706-e72b0528a3c4"},"outputs":[],"source":["CURR_DIR = os.getcwd()\n","dataset_zip = path.join(CURR_DIR, \"covidxct.zip\")\n","dataset_dir = path.join(CURR_DIR, \"data\")\n","\n","# manually download dataset from https://www.kaggle.com/datasets/c395fb339f210700ba392d81bf200f766418238c2734e5237b5dd0b6fc724fcb\n","# The kaggle command despite don't show error, not able to save/download the zip into local file system\n","# kaggle datasets download -d hgunraj/covidxct\n","\n","if not path.exists(dataset_dir):\n","    os.mkdir(dataset_dir)\n","    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n","        zip_ref.extractall(dataset_dir)\n","if path.exists(dataset_zip):\n","  print(dataset_zip)\n","    # os.remove(dataset_zip)"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Data Preprocessing\n","\n","------------------------------------------------------------------------\n","\n","-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n","-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n","    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n","-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset class to load CovidNet data\n","\n","- Various references used: \n","  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n","  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n","  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n","  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n","  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n","  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:50.079878Z","iopub.status.busy":"2022-11-17T15:12:50.079366Z","iopub.status.idle":"2022-11-17T15:12:50.092973Z","shell.execute_reply":"2022-11-17T15:12:50.091975Z","shell.execute_reply.started":"2022-11-17T15:12:50.079844Z"},"trusted":true},"outputs":[],"source":["class CovidNetDataset(Dataset):\n","    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n","        # don't seem to need the csv file\n","        # self.df = pd.read_csv(csv_path)\n","        # _, self.class_to_idx  = self.find_classes(csv_path);\n","\n","        self.img_dir = img_dir\n","        self.split_files = split_files\n","        \n","        self.size = 0\n","        self.limit_size = limit_size\n","        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n","        self.stradify_removal_based_on_limit()\n","        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, index):\n","        # filename = self.df[index, \"FILENAME\"]\n","        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n","        # image = Image.open(os.path.join(self.img_dir, filename))\n","\n","        label = self.targets[index]\n","        with open(self.imgs[index], \"rb\") as f:\n","            image = Image.open(f)\n","            image = image.crop(self.bboxes[index])\n","            image = image.copy()\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","    \n","\n","        return image, label\n","\n","    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n","    def find_classes(self, csv_path=None):\n","        \"\"\"Returns class name array and class_to_idx.\n","        See :class:`CovidNetDataset` for details.\n","        \"\"\"\n","        # class_col = \"finding\"\n","        # classes = sorted(self.df[class_col].unique())\n","        # if not classes:\n","        #     raise FileNotFoundError(f\"Couldn't find any class from '{class_col}' column in {csv_path}.\")\n","\n","        # hard code classes as the order are not alphabetic\n","        classes = ['Normal', 'Pneumonia', 'COVID-19']\n","\n","        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n","        return classes, class_to_idx\n","\n","    def get_all_split_file_data(self):\n","        files, classes, bboxes = [], [], []\n","        for split_file in self.split_files:\n","            f, cls, bb = self.get_data_from_split_file(split_file)\n","            files.extend(f)\n","            classes.extend(cls)\n","            bboxes.extend(bb)\n","        return files, classes, bboxes\n","\n","    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","    def get_data_from_split_file(self, split_file):\n","        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","        files, classes, bboxes = [], [], []\n","        with open(split_file, 'r') as f:\n","            for line in f.readlines():\n","                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","                files.append(path.join(self.img_dir, fname))\n","                classes.append(int(cls))\n","                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","                self.size += 1\n","        return files, classes, bboxes\n","    \n","    '''Try to do stratified removal based on limit count if it is specified'''\n","    def stradify_removal_based_on_limit(self):\n","        _, class_to_idx = self.find_classes()\n","        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n","        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n","            return\n","        \n","        total_remove_count = self.size - self.limit_size\n","        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n","        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n","        for idx, count in occurrence.items():\n","            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n","        \n","        print(occurrence)\n","        print(target_remove_count)\n","        \n","        for i in reversed(range(len(self.targets))):\n","            idx = self.targets[i]\n","            if target_remove_count[idx] > 0:\n","                del self.targets[i]\n","                del self.imgs[i]\n","                del self.bboxes[i]\n","                target_remove_count[idx] -= 1\n","                self.size -= 1"]},{"cell_type":"markdown","metadata":{},"source":["## Spliting dataset into train, val, test\n","\n","-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n","    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n","    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n","-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n","-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n","-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n","-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:54.991228Z","iopub.status.busy":"2022-11-17T15:12:54.990831Z","iopub.status.idle":"2022-11-17T15:12:57.295385Z","shell.execute_reply":"2022-11-17T15:12:57.294226Z","shell.execute_reply.started":"2022-11-17T15:12:54.991172Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 71488, 1: 42943, 2: 310593}\n","{0: 69806, 1: 41932, 2: 303285}\n","Length of full dataset: 10001\n","First 10 train_index: [6373 6939 9338  105 7573 4773 9125 4210 4499 1754]\n","length of train_index: 7999\n","\n","First 10 val_index: [6942 5767  536 6017 6502 1373 8798 3052 4665 9964]\n","length of val_index: 1001\n","\n","First 10 test_index: [9022 9017 9868 1724  795 4754 4568 1985 9612 9534]\n","length of test_index: 1001\n","\n"]}],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = path.join(CURR_DIR, \"data\")\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, 10000)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","_, _, _, _, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","# dataset size\n","train_data_size = len(train_index)\n","valid_data_size = len(val_index)\n","test_data_size = len(test_index)\n","\n","print(f\"First 10 train_index: {train_index[:10]}\")\n","print(f\"length of train_index: {train_data_size}\\n\")\n","print(f\"First 10 val_index: {val_index[:10]}\")\n","print(f\"length of val_index: {valid_data_size}\\n\")\n","print(f\"First 10 test_index: {test_index[:10]}\")\n","print(f\"length of test_index: {test_data_size}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Applying transforms to dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Define a wrapper dataset\n","\n","- This is to have the flexibility of applying different transforms to each of the splitted dataset \n","- References \n","    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n","    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n","    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:05.619690Z","iopub.status.busy":"2022-11-17T15:13:05.619009Z","iopub.status.idle":"2022-11-17T15:13:05.626266Z","shell.execute_reply":"2022-11-17T15:13:05.625249Z","shell.execute_reply.started":"2022-11-17T15:13:05.619650Z"},"trusted":true},"outputs":[],"source":["class WrapperDataset:\n","    def __init__(self, dataset, transform=None, target_transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        image, label = self.dataset[index]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        if self.target_transform is not None:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.dataset)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining the transforms\n","\n","- References for mean and std of images \n","    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n","    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n","    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n","    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n","    - Bounding box causing issue when batching as stacking donâ€™t work with\n","    different size \n","        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n","        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:08.902356Z","iopub.status.busy":"2022-11-17T15:13:08.901940Z","iopub.status.idle":"2022-11-17T15:13:08.915480Z","shell.execute_reply":"2022-11-17T15:13:08.914234Z","shell.execute_reply.started":"2022-11-17T15:13:08.902324Z"},"trusted":true},"outputs":[],"source":["covidnet_std_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n","])\n","\n","covidnet_train_transform = transforms.Compose([\n","    transforms.RandomChoice(transforms=[\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=.3, hue=.3),\n","        transforms.RandomPerspective(distortion_scale=0.4),\n","        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n","    ])\n","\n","image_transforms = {\n","    'train': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ]),\n","    'val': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'test': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'playground': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ])\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Dataset Loader"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:11.477712Z","iopub.status.busy":"2022-11-17T15:13:11.477114Z","iopub.status.idle":"2022-11-17T15:13:11.529327Z","shell.execute_reply":"2022-11-17T15:13:11.528231Z","shell.execute_reply.started":"2022-11-17T15:13:11.477670Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: PYTHONHASHSEED=18\n","['Normal', 'Pneumonia', 'COVID-19']\n","{'Normal': 0, 'Pneumonia': 1, 'COVID-19': 2}\n","Using device: cuda\n","train size:7999; validation size:1001; test size:1001\n"]}],"source":["seed_everything(SEED)\n","BATCH_SIZE = 2\n","\n","train_sampler = SubsetRandomSampler(train_index)\n","val_sampler = SubsetRandomSampler(val_index)\n","test_sampler = SubsetRandomSampler(test_index)\n","\n","train_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\n","val_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\n","test_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n","\n","class_names, class_to_idx = full_dataset.find_classes()\n","print(class_names)\n","print(class_to_idx)\n","\n","if is_gpu_avail():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","    \n","print(f'Using device: {device}')\n","print(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Custom Model\n","\n","------------------------------------------------------------------------\n","\n","- The design of this custom model is illustrated in a draw.io diagram \n","    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n","    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)"]},{"cell_type":"markdown","metadata":{},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["### Links\n","\n","-   [10 CNN Architecture\n","    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n","    -   [Visualizing pytorch\n","        models](https://github.com/szagoruyko/pytorchviz)\n","-   Main model building references\n","    -   The [CT-3A github\n","        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n","        -   [tensorflow pretrained\n","            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n","        -   How to [convert tensorflow checkpoints into pytorch\n","            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n","            -   [pytorch\n","                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n","        -   [pytorch\n","            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n","            about combining two existing models\n","    -   [Pytorch resnext50\n","        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n","    -   [pytorch beginner tutorial on building\n","        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n","-   Other model building references\n","    -   [Custom\n","        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n","    -   [Resnest convolution block\n","        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n","    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram\n","-   How to determine ConvNet output channel/size\n","    -   [How to determine output channel ref 1](https://datascience.stackexchange.com/questions/47328/how-to-choose-the-number-of-output-channels-in-a-convolutional-layer)\n","    -   [How to determine output channel ref 2](https://stats.stackexchange.com/questions/380996/convolutional-network-how-to-choose-output-channels-number-stride-and-padding)\n","    -   TLDR.: The exact number of output channel don't matter, experiment around the common range and don't use values that are too extreme\n","    -   For output size, it can be easier calculated using [this calcuator](https://asiltureli.github.io/Convolution-Layer-Calculator) or even manually."]},{"cell_type":"markdown","metadata":{},"source":["## Components"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# modified from https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py#L46\n","class Conv2d_BN(Module):\n","    def __init__(self, in_channels, out_channels, kernel, stride=1, padding=0, groups=1, acti=LeakyReLU(0.2, inplace=True)):\n","        super().__init__() # same as super(Conv2d_BN, self).__init__()\n","        if acti:\n","            self.conv2d_bn = Sequential(\n","                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n","                BatchNorm2d(out_channels),\n","                acti\n","            )\n","        else:\n","            self.conv2d_bn = Sequential(\n","                Conv2d(in_channels, out_channels, kernel, stride, padding, groups=groups, bias=False),\n","                BatchNorm2d(out_channels)\n","            )\n","\n","    def forward(self, x):\n","        return self.conv2d_bn(x)\n","\n","\n","# Taken from https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py#L63\n","class StemBlock(Module):\n","    def __init__(self, in_channels=3):\n","        super().__init__()\n","        # For simplicity Sequential module can be used here, explicitly name every layer for practise and readibility\n","        self.conv1 = Conv2d_BN(in_channels, 64, kernel=7, stride=2, padding=3)\n","        self.maxpool1 = MaxPool2d(in_channels, stride=2, ceil_mode=True)\n","        self.conv2 = Conv2d_BN(64, 64, kernel=1)\n","        self.conv3 = Conv2d_BN(64, 192, kernel=3, padding=1)\n","        self.maxpool2 = MaxPool2d(3, stride=2, ceil_mode=True)\n","\n","    def forward(self, x):\n","        # N x 3 x 224 x 224\n","        x = self.conv1(x)\n","        # N x 64 x 112 x 112\n","        x = self.maxpool1(x)\n","        # N x 64 x 56 x 56\n","        x = self.conv2(x)\n","        # N x 64 x 56 x 56\n","        x = self.conv3(x)\n","        # N x 192 x 56 x 56\n","        return self.maxpool2(x)\n","\n","\n","class BlockType(Enum):\n","    CONV = Enum.auto()\n","    IDENTITY = Enum.auto()\n","\n","\n","# Contructed by referring ResNext50 visualized in TorchStudio\n","class ConvBlock(Module):\n","    def __init__(self, in_channels, tower_height):\n","        super().__init__()\n","        self.conv_tower = ModuleList(\n","            Conv2d_BN(in_channels, out_channels=128, kernel=1),\n","            Conv2d_BN(in_channels=128, out_channels=128, kernel=3, padding=1, groups=tower_height),\n","            Conv2d_BN(in_channels=128, out_channels=256, kernel=1, acti=None),\n","            Conv2d_BN(in_channels, out_channels=256, kernel=1, acti=None)\n","        )\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower[0](x)\n","        out = self.conv_tower[1](out)\n","        out = torch.add(self.conv_tower[2](out), self.conv_tower[3](x))\n","        return self.relu(out)\n","\n","\n","# Contructed by referring to ConvBlock\n","class IdentityBlock(Module):\n","    def __init__(self, in_channels, tower_height):\n","        super().__init__()\n","        self.conv_tower = ModuleList(\n","            Conv2d_BN(in_channels, out_channels=128, kernel=1),\n","            Conv2d_BN(in_channels=128, out_channels=128, kernel=3, padding=1, groups=tower_height),\n","            Conv2d_BN(in_channels=128, out_channels=256, kernel=1, acti=None)\n","        )\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower[0](x)\n","        out = self.conv_tower[1](out)\n","        out = torch.add(self.conv_tower[2](out), x)\n","        return self.relu(out)\n","\n","\n","# taken from https://github.com/reppertj/earworm/blob/a2d8a70085748da5db378f7f5f68ad8c2926a274/modeling/music_metric_learning/modules/inception.py#L93\n","class ReductionBlock(Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        assert out_channels % 16 == 0\n","        conv5_out = out_channels // 4\n","        conv3_pooling_out = conv5_out * 3 // 2\n","        self.conv3 = Conv2d_BN(in_channels, conv3_pooling_out, kernel=3, stride=2)\n","        self.conv5 = Sequential(\n","            Conv2d_BN(in_channels       , conv5_out * 3 // 4, kernel=1),\n","            Conv2d_BN(conv5_out * 3 // 4, conv5_out * 7 // 8, kernel=3, padding=1),\n","            Conv2d_BN(conv5_out * 7 // 8, conv5_out         , kernel=3, stride=2),\n","        )\n","        self.pooling = Sequential(\n","            MaxPool2d(3, stride=2),\n","            Conv2d_BN(in_channels, conv3_pooling_out, kernel=1),\n","        )\n","\n","    def forward(self, x):\n","        return torch.cat((self.conv3(x), self.conv5(x), self.pooling(x)), dim=1)\n","\n","\n","# Generalize ConvBlock and Identity block as ResidualBlock:\n","# https://github.com/maciejbalawejder/Deep-Learning-Collection/blob/main/ConvNets/ResNeXt/resnext_pytorch.py\n","class ResidualBlock(Module):\n","    def __init__(self, in_channels, out_channels, block_type: BlockType, stride, cardinatlity=32):\n","        super().__init__()\n","        assert out_channels % 32 == 0\n","        self.C = cardinatlity\n","        self.block_type = block_type\n","        inner_channels = out_channels // 2\n","        self.conv_tower = Sequential(\n","            Conv2d_BN(in_channels, inner_channels, kernel=1),\n","            Conv2d_BN(inner_channels, inner_channels, kernel=3, stride=stride, padding=1, groups=self.C),\n","            Conv2d_BN(inner_channels, out_channels=256, kernel=1, acti=None)\n","        )\n","        if self.block_type is BlockType.CONV:\n","            self.downsample = Conv2d_BN(in_channels, out_channels, 1, stride, 0)\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv_tower(x)\n","        if self.block_type is BlockType.CONV:\n","            x = self.downsample(x)\n","        out = self.relu(torch.add(out,x))\n","        return out\n","\n","\n","class CovidnetBlock(Module):\n","    def __init__(self, block_type_dict: dict):\n","        super().__init__()\n","        self.blocks = Sequential()\n","        assert [k for k, v in block_type_dict.items() if isinstance(k, BlockType) and isinstance(v, dict)].count() == len(block_type_dict)\n","        for k,v in block_type_dict:\n","            for value in v.items():\n","                self.blocks.append(ResidualBlock(value.in_chan, value.out_chan, k, 1))\n","\n","    def forward(self, x):\n","        out = self.conv_tower(x)\n","        if self.block_type is BlockType.CONV:\n","            x = self.downsample(x)\n","        out = self.relu(torch.add(out,x))\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Full Model"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa"},"outputs":[],"source":["class CovidNetModel(Module):\n","    def __init__(self, channels_in):\n","        super().__init__()\n","        self.inplanes = 64\n","\n","        self.conv1 = Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=5, bias=False)\n","        self.bn1 = BatchNorm2d(self.inplanes)\n","        self.relu = LeakyReLU(0.2, inplace=True)\n","        self.maxpool1 = MaxPool2d(3, 1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool1(x)\n","        return x\n","\n","    def _make_block():\n","        pass"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Training & Validation\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Test (Evaluation)\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Playground\n","\n","------------------------------------------------------------------------\n","\n","> Testbed that is not compulsary for any part of this notebook"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Models using PytorchViz\n","\n","-   https://pytorch.org/docs/stable/nn.html\n","-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n","-   [Mandrin explanation of pytorch resnet\n","    code](https://www.jianshu.com/p/90d61f53d15d)"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchviz import make_dot, make_dot_from_trace"]},{"cell_type":"code","execution_count":8,"metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\googlenet.py:47: FutureWarning: The default weight initialization of GoogleNet will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 150, 150]           9,408\n","       BatchNorm2d-2         [-1, 64, 150, 150]             128\n","       BasicConv2d-3         [-1, 64, 150, 150]               0\n","         MaxPool2d-4           [-1, 64, 75, 75]               0\n","            Conv2d-5           [-1, 64, 75, 75]           4,096\n","       BatchNorm2d-6           [-1, 64, 75, 75]             128\n","       BasicConv2d-7           [-1, 64, 75, 75]               0\n","            Conv2d-8          [-1, 192, 75, 75]         110,592\n","       BatchNorm2d-9          [-1, 192, 75, 75]             384\n","      BasicConv2d-10          [-1, 192, 75, 75]               0\n","        MaxPool2d-11          [-1, 192, 37, 37]               0\n","           Conv2d-12           [-1, 64, 37, 37]          12,288\n","      BatchNorm2d-13           [-1, 64, 37, 37]             128\n","      BasicConv2d-14           [-1, 64, 37, 37]               0\n","           Conv2d-15           [-1, 96, 37, 37]          18,432\n","      BatchNorm2d-16           [-1, 96, 37, 37]             192\n","      BasicConv2d-17           [-1, 96, 37, 37]               0\n","           Conv2d-18          [-1, 128, 37, 37]         110,592\n","      BatchNorm2d-19          [-1, 128, 37, 37]             256\n","      BasicConv2d-20          [-1, 128, 37, 37]               0\n","           Conv2d-21           [-1, 16, 37, 37]           3,072\n","      BatchNorm2d-22           [-1, 16, 37, 37]              32\n","      BasicConv2d-23           [-1, 16, 37, 37]               0\n","           Conv2d-24           [-1, 32, 37, 37]           4,608\n","      BatchNorm2d-25           [-1, 32, 37, 37]              64\n","      BasicConv2d-26           [-1, 32, 37, 37]               0\n","        MaxPool2d-27          [-1, 192, 37, 37]               0\n","           Conv2d-28           [-1, 32, 37, 37]           6,144\n","      BatchNorm2d-29           [-1, 32, 37, 37]              64\n","      BasicConv2d-30           [-1, 32, 37, 37]               0\n","        Inception-31          [-1, 256, 37, 37]               0\n","           Conv2d-32          [-1, 128, 37, 37]          32,768\n","      BatchNorm2d-33          [-1, 128, 37, 37]             256\n","      BasicConv2d-34          [-1, 128, 37, 37]               0\n","           Conv2d-35          [-1, 128, 37, 37]          32,768\n","      BatchNorm2d-36          [-1, 128, 37, 37]             256\n","      BasicConv2d-37          [-1, 128, 37, 37]               0\n","           Conv2d-38          [-1, 192, 37, 37]         221,184\n","      BatchNorm2d-39          [-1, 192, 37, 37]             384\n","      BasicConv2d-40          [-1, 192, 37, 37]               0\n","           Conv2d-41           [-1, 32, 37, 37]           8,192\n","      BatchNorm2d-42           [-1, 32, 37, 37]              64\n","      BasicConv2d-43           [-1, 32, 37, 37]               0\n","           Conv2d-44           [-1, 96, 37, 37]          27,648\n","      BatchNorm2d-45           [-1, 96, 37, 37]             192\n","      BasicConv2d-46           [-1, 96, 37, 37]               0\n","        MaxPool2d-47          [-1, 256, 37, 37]               0\n","           Conv2d-48           [-1, 64, 37, 37]          16,384\n","      BatchNorm2d-49           [-1, 64, 37, 37]             128\n","      BasicConv2d-50           [-1, 64, 37, 37]               0\n","        Inception-51          [-1, 480, 37, 37]               0\n","        MaxPool2d-52          [-1, 480, 18, 18]               0\n","           Conv2d-53          [-1, 192, 18, 18]          92,160\n","      BatchNorm2d-54          [-1, 192, 18, 18]             384\n","      BasicConv2d-55          [-1, 192, 18, 18]               0\n","           Conv2d-56           [-1, 96, 18, 18]          46,080\n","      BatchNorm2d-57           [-1, 96, 18, 18]             192\n","      BasicConv2d-58           [-1, 96, 18, 18]               0\n","           Conv2d-59          [-1, 208, 18, 18]         179,712\n","      BatchNorm2d-60          [-1, 208, 18, 18]             416\n","      BasicConv2d-61          [-1, 208, 18, 18]               0\n","           Conv2d-62           [-1, 16, 18, 18]           7,680\n","      BatchNorm2d-63           [-1, 16, 18, 18]              32\n","      BasicConv2d-64           [-1, 16, 18, 18]               0\n","           Conv2d-65           [-1, 48, 18, 18]           6,912\n","      BatchNorm2d-66           [-1, 48, 18, 18]              96\n","      BasicConv2d-67           [-1, 48, 18, 18]               0\n","        MaxPool2d-68          [-1, 480, 18, 18]               0\n","           Conv2d-69           [-1, 64, 18, 18]          30,720\n","      BatchNorm2d-70           [-1, 64, 18, 18]             128\n","      BasicConv2d-71           [-1, 64, 18, 18]               0\n","        Inception-72          [-1, 512, 18, 18]               0\n","           Conv2d-73            [-1, 128, 4, 4]          65,536\n","      BatchNorm2d-74            [-1, 128, 4, 4]             256\n","      BasicConv2d-75            [-1, 128, 4, 4]               0\n","           Linear-76                 [-1, 1024]       2,098,176\n","          Dropout-77                 [-1, 1024]               0\n","           Linear-78                 [-1, 1000]       1,025,000\n","     InceptionAux-79                 [-1, 1000]               0\n","           Conv2d-80          [-1, 160, 18, 18]          81,920\n","      BatchNorm2d-81          [-1, 160, 18, 18]             320\n","      BasicConv2d-82          [-1, 160, 18, 18]               0\n","           Conv2d-83          [-1, 112, 18, 18]          57,344\n","      BatchNorm2d-84          [-1, 112, 18, 18]             224\n","      BasicConv2d-85          [-1, 112, 18, 18]               0\n","           Conv2d-86          [-1, 224, 18, 18]         225,792\n","      BatchNorm2d-87          [-1, 224, 18, 18]             448\n","      BasicConv2d-88          [-1, 224, 18, 18]               0\n","           Conv2d-89           [-1, 24, 18, 18]          12,288\n","      BatchNorm2d-90           [-1, 24, 18, 18]              48\n","      BasicConv2d-91           [-1, 24, 18, 18]               0\n","           Conv2d-92           [-1, 64, 18, 18]          13,824\n","      BatchNorm2d-93           [-1, 64, 18, 18]             128\n","      BasicConv2d-94           [-1, 64, 18, 18]               0\n","        MaxPool2d-95          [-1, 512, 18, 18]               0\n","           Conv2d-96           [-1, 64, 18, 18]          32,768\n","      BatchNorm2d-97           [-1, 64, 18, 18]             128\n","      BasicConv2d-98           [-1, 64, 18, 18]               0\n","        Inception-99          [-1, 512, 18, 18]               0\n","          Conv2d-100          [-1, 128, 18, 18]          65,536\n","     BatchNorm2d-101          [-1, 128, 18, 18]             256\n","     BasicConv2d-102          [-1, 128, 18, 18]               0\n","          Conv2d-103          [-1, 128, 18, 18]          65,536\n","     BatchNorm2d-104          [-1, 128, 18, 18]             256\n","     BasicConv2d-105          [-1, 128, 18, 18]               0\n","          Conv2d-106          [-1, 256, 18, 18]         294,912\n","     BatchNorm2d-107          [-1, 256, 18, 18]             512\n","     BasicConv2d-108          [-1, 256, 18, 18]               0\n","          Conv2d-109           [-1, 24, 18, 18]          12,288\n","     BatchNorm2d-110           [-1, 24, 18, 18]              48\n","     BasicConv2d-111           [-1, 24, 18, 18]               0\n","          Conv2d-112           [-1, 64, 18, 18]          13,824\n","     BatchNorm2d-113           [-1, 64, 18, 18]             128\n","     BasicConv2d-114           [-1, 64, 18, 18]               0\n","       MaxPool2d-115          [-1, 512, 18, 18]               0\n","          Conv2d-116           [-1, 64, 18, 18]          32,768\n","     BatchNorm2d-117           [-1, 64, 18, 18]             128\n","     BasicConv2d-118           [-1, 64, 18, 18]               0\n","       Inception-119          [-1, 512, 18, 18]               0\n","          Conv2d-120          [-1, 112, 18, 18]          57,344\n","     BatchNorm2d-121          [-1, 112, 18, 18]             224\n","     BasicConv2d-122          [-1, 112, 18, 18]               0\n","          Conv2d-123          [-1, 144, 18, 18]          73,728\n","     BatchNorm2d-124          [-1, 144, 18, 18]             288\n","     BasicConv2d-125          [-1, 144, 18, 18]               0\n","          Conv2d-126          [-1, 288, 18, 18]         373,248\n","     BatchNorm2d-127          [-1, 288, 18, 18]             576\n","     BasicConv2d-128          [-1, 288, 18, 18]               0\n","          Conv2d-129           [-1, 32, 18, 18]          16,384\n","     BatchNorm2d-130           [-1, 32, 18, 18]              64\n","     BasicConv2d-131           [-1, 32, 18, 18]               0\n","          Conv2d-132           [-1, 64, 18, 18]          18,432\n","     BatchNorm2d-133           [-1, 64, 18, 18]             128\n","     BasicConv2d-134           [-1, 64, 18, 18]               0\n","       MaxPool2d-135          [-1, 512, 18, 18]               0\n","          Conv2d-136           [-1, 64, 18, 18]          32,768\n","     BatchNorm2d-137           [-1, 64, 18, 18]             128\n","     BasicConv2d-138           [-1, 64, 18, 18]               0\n","       Inception-139          [-1, 528, 18, 18]               0\n","          Conv2d-140            [-1, 128, 4, 4]          67,584\n","     BatchNorm2d-141            [-1, 128, 4, 4]             256\n","     BasicConv2d-142            [-1, 128, 4, 4]               0\n","          Linear-143                 [-1, 1024]       2,098,176\n","         Dropout-144                 [-1, 1024]               0\n","          Linear-145                 [-1, 1000]       1,025,000\n","    InceptionAux-146                 [-1, 1000]               0\n","          Conv2d-147          [-1, 256, 18, 18]         135,168\n","     BatchNorm2d-148          [-1, 256, 18, 18]             512\n","     BasicConv2d-149          [-1, 256, 18, 18]               0\n","          Conv2d-150          [-1, 160, 18, 18]          84,480\n","     BatchNorm2d-151          [-1, 160, 18, 18]             320\n","     BasicConv2d-152          [-1, 160, 18, 18]               0\n","          Conv2d-153          [-1, 320, 18, 18]         460,800\n","     BatchNorm2d-154          [-1, 320, 18, 18]             640\n","     BasicConv2d-155          [-1, 320, 18, 18]               0\n","          Conv2d-156           [-1, 32, 18, 18]          16,896\n","     BatchNorm2d-157           [-1, 32, 18, 18]              64\n","     BasicConv2d-158           [-1, 32, 18, 18]               0\n","          Conv2d-159          [-1, 128, 18, 18]          36,864\n","     BatchNorm2d-160          [-1, 128, 18, 18]             256\n","     BasicConv2d-161          [-1, 128, 18, 18]               0\n","       MaxPool2d-162          [-1, 528, 18, 18]               0\n","          Conv2d-163          [-1, 128, 18, 18]          67,584\n","     BatchNorm2d-164          [-1, 128, 18, 18]             256\n","     BasicConv2d-165          [-1, 128, 18, 18]               0\n","       Inception-166          [-1, 832, 18, 18]               0\n","       MaxPool2d-167            [-1, 832, 9, 9]               0\n","          Conv2d-168            [-1, 256, 9, 9]         212,992\n","     BatchNorm2d-169            [-1, 256, 9, 9]             512\n","     BasicConv2d-170            [-1, 256, 9, 9]               0\n","          Conv2d-171            [-1, 160, 9, 9]         133,120\n","     BatchNorm2d-172            [-1, 160, 9, 9]             320\n","     BasicConv2d-173            [-1, 160, 9, 9]               0\n","          Conv2d-174            [-1, 320, 9, 9]         460,800\n","     BatchNorm2d-175            [-1, 320, 9, 9]             640\n","     BasicConv2d-176            [-1, 320, 9, 9]               0\n","          Conv2d-177             [-1, 32, 9, 9]          26,624\n","     BatchNorm2d-178             [-1, 32, 9, 9]              64\n","     BasicConv2d-179             [-1, 32, 9, 9]               0\n","          Conv2d-180            [-1, 128, 9, 9]          36,864\n","     BatchNorm2d-181            [-1, 128, 9, 9]             256\n","     BasicConv2d-182            [-1, 128, 9, 9]               0\n","       MaxPool2d-183            [-1, 832, 9, 9]               0\n","          Conv2d-184            [-1, 128, 9, 9]         106,496\n","     BatchNorm2d-185            [-1, 128, 9, 9]             256\n","     BasicConv2d-186            [-1, 128, 9, 9]               0\n","       Inception-187            [-1, 832, 9, 9]               0\n","          Conv2d-188            [-1, 384, 9, 9]         319,488\n","     BatchNorm2d-189            [-1, 384, 9, 9]             768\n","     BasicConv2d-190            [-1, 384, 9, 9]               0\n","          Conv2d-191            [-1, 192, 9, 9]         159,744\n","     BatchNorm2d-192            [-1, 192, 9, 9]             384\n","     BasicConv2d-193            [-1, 192, 9, 9]               0\n","          Conv2d-194            [-1, 384, 9, 9]         663,552\n","     BatchNorm2d-195            [-1, 384, 9, 9]             768\n","     BasicConv2d-196            [-1, 384, 9, 9]               0\n","          Conv2d-197             [-1, 48, 9, 9]          39,936\n","     BatchNorm2d-198             [-1, 48, 9, 9]              96\n","     BasicConv2d-199             [-1, 48, 9, 9]               0\n","          Conv2d-200            [-1, 128, 9, 9]          55,296\n","     BatchNorm2d-201            [-1, 128, 9, 9]             256\n","     BasicConv2d-202            [-1, 128, 9, 9]               0\n","       MaxPool2d-203            [-1, 832, 9, 9]               0\n","          Conv2d-204            [-1, 128, 9, 9]         106,496\n","     BatchNorm2d-205            [-1, 128, 9, 9]             256\n","     BasicConv2d-206            [-1, 128, 9, 9]               0\n","       Inception-207           [-1, 1024, 9, 9]               0\n","AdaptiveAvgPool2d-208           [-1, 1024, 1, 1]               0\n","         Dropout-209                 [-1, 1024]               0\n","          Linear-210                 [-1, 1000]       1,025,000\n","================================================================\n","Total params: 13,004,888\n","Trainable params: 13,004,888\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 1.02\n","Forward/backward pass size (MB): 163.54\n","Params size (MB): 49.61\n","Estimated Total Size (MB): 214.17\n","----------------------------------------------------------------\n"]}],"source":["# from torchinfo import summary\n","from torchsummary import summary\n","\n","if is_gpu_avail():\n","    device = torch.device('cuda')\n","    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","else:\n","    device = torch.device('cpu')\n","\n","# model = models.resnext50_32x4d()\n","# summary(model, (3, 224, 224))\n","\n","model = models.googlenet()\n","summary(model, (3, 299, 299))\n","\n","# model = ReductionBlock(256, 256)\n","# summary(model, (3, 224, 224))\n","\n","# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n","# y = model(x)\n","# dot = make_dot(y, params=dict(model.named_parameters()))\n","\n","# dot.format = \"png\"\n","# dot.render(render_model_pic_file)\n","# files.download(f\"{render_model_pic_file}.{dot.format}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigate Image in Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"outputs":[],"source":["import subprocess\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","DATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","\n","# count number of images\n","# !ls -Uba1 /content/data/3A_images | grep -c png\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\n","print(f\"number of images: {output.decode()}\")\n","\n","# list first 10 images\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\n","first_10_lines = output.decode()\n","# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\n","img_list = first_10_lines.split('\\n')\n","first_png = next(filter(lambda img: \"png\" in img, img_list), None)\n","print(f\"first_png: {first_png}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"outputs":[],"source":["# show the first test image, unbounded followed by bounded\n","import torch\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as torch_func_trans\n","from PIL import Image\n","\n","# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","def get_data_from_split_file(split_file):\n","    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","    files, classes, bboxes = [], [], []\n","    with open(split_file, 'r') as f:\n","        for line in f.readlines():\n","            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","            files.append(path.join(DATASET_DIR, fname))\n","            classes.append(int(cls))\n","            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","    return files, classes, bboxes\n","\n","def bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n","    top = ymax\n","    left = xmin\n","    height = ymax - ymin\n","    width = xmax - xmin\n","    return top, left, height, width\n","\n","files, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\n","first_file_tuple = (files[0], classes[0], bbox[0])\n","\n","print(f\"first image: {first_file_tuple[0]}\")\n","if 0:\n","    # this way of reading image is deprecated\n","    img = plt.imread(first_file_tuple[0])\n","\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"unbounded\")\n","    _ = plt.imshow(img)\n","    _ = plt.axis('off')\n","\n","    torch_img = torch.from_numpy(img)\n","    print(f\"torch_img size: {torch_img.size()}\")\n","    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n","    print(f\"pytorch_size: {pytorch_size}\")\n","\n","    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"bounded\")\n","    _ = plt.imshow(cropped_img)\n","    _ = plt.axis('off')\n","\n","# This is the recommended method for opening image\n","# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\n","with Image.open(first_file_tuple[0]) as im:\n","    print(\"\\n\")\n","    print(\"Unbounded image\")\n","    IMG = im\n","    display(im)\n","\n","    print(\"\\n\")\n","    print(\"Bounded image\")\n","    display(im.crop(first_file_tuple[2]))\n","\n","    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n","    print(\"\\n\")\n","    print(\"Out of Scope Unbounded image\")\n","    display(IMG)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Investigate metadata.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.expand_frame_repr', False)\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","meta_df = pd.read_csv(METADATA_CSV)\n","print(\"First 5 rows in metadata.csv0\")\n","print(meta_df.head(5))\n","print(f\"classes: {sorted(meta_df['finding'].unique())}\")\n","\n","unverified = meta_df['verified finding'].eq('No').sum()\n","verified = meta_df['verified finding'].eq('Yes').sum()\n","print(f\"Not verified: {unverified}\")\n","print(f\"Verified: {verified}\")\n","\n","unverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\n","assert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\n","print(f\"first unverified ID: {unverified_patient_ids[0]}\")\n","\n","verified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\n","assert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\n","print(f\"first verified ID: {verified_patient_ids[0]}\")\n","\n","imgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\n","print(f\"total images: {len(imgs)}\")\n","print(f\"first 10 images: {imgs[:10]}\")\n","imgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\n","print(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigating split dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"outputs":[],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = f\"{CURR_DIR}/data\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_SIZE)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","SEED = 18\n","seed_everything(SEED)\n","BATCH_SIZE = 128\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","# train_index, test_index = next(\n","#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n","#         dummy_X, full_dataset.targets\n","#     )\n","# )\n","\n","print('*'*50)\n","print(\"temp_train\")\n","print('*'*50)\n","print(f\"First 10 index: {temp_train_index[:10]}\")\n","print(f\"First 10 label: {y_remaining[:10]}\")\n","print(f\"length of index: {len(temp_train_index)}\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {y_remaining.count(0)}\")\n","print(f\"Pneunomia: {y_remaining.count(1)}\")\n","print(f\"Covid-19: {y_remaining.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"test\")\n","print('*'*50)\n","print(f\"First 10 index: {test_index[:10]}\")\n","print(f\"First 10 label: {Y_test[:10]}\")\n","print(f\"length of index: {len(test_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_test.count(0)}\")\n","print(f\"Pneunomia: {Y_test.count(1)}\")\n","print(f\"Covid-19: {Y_test.count(2)}\")\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","X_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","print()\n","print('*'*50)\n","print(\"train\")\n","print('*'*50)\n","print(f\"First 10 index: {train_index[:10]}\")\n","print(f\"First 10 label: {Y_train[:10]}\")\n","print(f\"length of index: {len(train_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_train.count(0)}\")\n","print(f\"Pneunomia: {Y_train.count(1)}\")\n","print(f\"Covid-19: {Y_train.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"val\")\n","print('*'*50)\n","print(f\"First 10 index: {val_index[:10]}\")\n","print(f\"First 10 label: {Y_val[:10]}\")\n","print(f\"length of index: {len(val_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_val.count(0)}\")\n","print(f\"Pneunomia: {Y_val.count(1)}\")\n","print(f\"Covid-19: {Y_val.count(2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"outputs":[],"source":["import pprint\n","def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    # mean = np.array([0.485, 0.456, 0.406])\n","    # std = np.array([0.229, 0.224, 0.225])\n","    # inp = std * inp + mean\n","    # inp = np.clip(inp, 0, 1)\n","    plt.figure(figsize=[15, 15])\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","seed_everything(19)\n","class_names, _ = full_dataset.find_classes()\n","data, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n","# out = torchvision.utils.make_grid(data)\n","# imshow(out)\n","# pp = pprint.PrettyPrinter(compact=True)\n","# pp.pprint([class_names[x] for x in classes])\n","\n","print(f\"Class: {classes[0]}\")\n","out = data[0]\n","imshow(out)"]},{"cell_type":"markdown","metadata":{},"source":["## Trying Avalance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://changhsinlee.com/colab-import-python/\n","!pip install requests\n","!pip install avalanche-lib"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","\n","# Save datagenerators as file to colab working directory\n","# If you are using GitHub, make sure you get the \"Raw\" version of the code\n","url = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\n","r = requests.get(url)\n","\n","# make sure your filename is the same as how you want to import \n","with open('pytorchcv_models.py', 'w') as f:\n","    f.write(r.text)\n","\n","# now we can import\n","import pytorchcv_models as pycv\n","from types import SimpleNamespace\n","\n","args = SimpleNamespace()\n","args.cuda = 0\n","pycv.main(args)"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained Model"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:19.895641Z","iopub.status.busy":"2022-11-17T15:13:19.895051Z","iopub.status.idle":"2022-11-17T15:13:19.903105Z","shell.execute_reply":"2022-11-17T15:13:19.901963Z","shell.execute_reply.started":"2022-11-17T15:13:19.895607Z"},"trusted":true},"outputs":[],"source":["LOG_DIR = os.path.join(CURR_DIR, \"log\")\n","RESULT_DIR = os.path.join(CURR_DIR, 'result')\n","curr_model = \"\"\n","\n","def log_to_file(txt=None, print_to_console_only=False):\n","  if txt is None:\n","    txt = ''\n","  txt += '\\n'\n","  print(txt)\n","  if print_to_console_only:\n","    return\n","  if not path.exists(LOG_DIR):\n","    os.mkdir(LOG_DIR)\n","  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n","  with open(full_path, mode='a') as f:\n","    f.write(txt)\n","    \n","# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n","# Make sure to delete any references to tensor. Else this function will not have significant effect\n","def clean_vram():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\n","def name_of_caller(frame=1):\n","    \"\"\"\n","    Return \"class.function_name\" of the caller or just \"function_name\".\n","    \"\"\"\n","    frame = sys._getframe(frame)\n","    fn_name = frame.f_code.co_name\n","    var_names = frame.f_code.co_varnames\n","    if var_names:\n","        if var_names[0] == \"self\":\n","            self_obj = frame.f_locals.get(\"self\")\n","            if self_obj is not None:\n","                return f\"{type(self_obj).__name__}.{fn_name}\" \n","        if var_names[0] == \"cls\":\n","            cls_obj = frame.f_locals.get(\"cls\")\n","            if cls_obj is not None:\n","                return f\"{cls_obj.__name__}.{fn_name}\"\n","    return fn_name"]},{"cell_type":"markdown","metadata":{},"source":["### Define Function to Initialize Deep Learning Models"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:22.599863Z","iopub.status.busy":"2022-11-17T15:13:22.599496Z","iopub.status.idle":"2022-11-17T15:13:22.618257Z","shell.execute_reply":"2022-11-17T15:13:22.617089Z","shell.execute_reply.started":"2022-11-17T15:13:22.599831Z"},"trusted":true},"outputs":[],"source":["model_constructors = {\n","  models.alexnet.__name__: models.alexnet, \n","  models.squeezenet1_1.__name__: models.squeezenet1_1,\n","  models.resnet50.__name__: models.resnet50, \n","  models.resnet101.__name__: models.resnet101,\n","  models.resnet152.__name__: models.resnet152, \n","  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n","  models.densenet201.__name__: models.densenet201, \n","  models.googlenet.__name__: models.googlenet, \n","  models.vgg16.__name__: models.vgg16, \n","  models.vgg19.__name__: models.vgg19, \n","  models.inception_v3.__name__: models.inception_v3, \n","}\n","\n","from torchvision.models import *\n","model_weights = {\n","  models.alexnet.__name__: AlexNet_Weights.DEFAULT,\n","  models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n","  models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n","  models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n","  models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n","  models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n","  models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n","  models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n","  models.vgg16.__name__: VGG16_Weights.DEFAULT,\n","  models.vgg19.__name__: VGG19_Weights.DEFAULT,\n","  models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n","}\n","\n","# Experiment around dropout & Learning Rate & different optimizer (Adam)\n","def init_model(name):\n","  if not path.exists(RESULT_DIR):\n","    os.mkdir(RESULT_DIR)\n","\n","  clean_vram()\n","  seed_everything()\n","  model = model_constructors[name](weights=model_weights[name])\n","  \n","  # fine-tune pretrain models to our usecase\n","  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n","  NUM_CLASSES = len(class_names)\n","  DROPOUT = 0.5\n","  if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n","    num_ftrs = model.classifier[6].in_features\n","    model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.classifier[6] = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","  elif name == models.densenet201.__name__:\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.classifier = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","  elif name == models.squeezenet1_1.__name__:\n","    model.classifier = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","    # model.classifier = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","    # )\n","    model.num_classes = NUM_CLASSES\n","  elif name == models.inception_v3.__name__:\n","    auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n","    model.AuxLogits.fc = nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","    # model.AuxLogits.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","    # )\n","    primary_num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(primary_num_ftrs, NUM_CLASSES)\n","    # model.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(primary_num_ftrs, NUM_CLASSES)\n","    # )\n","  else:\n","    # resnet, resnext & googlenet\n","    num_ftrs = model.fc.in_features\n","    model.fc= nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","  if is_gpu_avail():\n","    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n","    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n","    scaler = torch.cuda.amp.GradScaler()\n","  else:\n","    raise RuntimeError('This code only support machine with GPU.')\n","\n","  # print('=====================================')\n","  print(f'{name} is initialized')\n","  # print('=====================================')\n","  # print(model)\n","  return model, criterion, optimizer, scaler\n","\n","# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n","def save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n","  torch.save({\n","    'perf_metrics': perf_metrics,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    \"scaler_state_dict\": scaler.state_dict(),\n","    'history': history,\n","    }, model_path)\n","\n","def load_model(model, optimizer, scaler, model_path):\n","  if not os.path.exists(model_path):\n","    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n","    return None, model, optimizer, scaler, None, None\n","  checkpoint = torch.load(model_path)\n","  perf_metrics = checkpoint['perf_metrics']\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  # model.load_state_dict(checkpoint['model_state_dict'], strict=False) # https://stackoverflow.com/questions/54058256/runtimeerror-errors-in-loading-state-dict-for-resnet\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","  history = checkpoint['history']\n","  total_epoch = len(history) - 1\n","  del checkpoint\n","\n","  return perf_metrics, model, optimizer, scaler, history, total_epoch"]},{"cell_type":"markdown","metadata":{},"source":["### Define Function to Train Models"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:26.421026Z","iopub.status.busy":"2022-11-17T15:13:26.420649Z","iopub.status.idle":"2022-11-17T15:13:26.448404Z","shell.execute_reply":"2022-11-17T15:13:26.447170Z","shell.execute_reply.started":"2022-11-17T15:13:26.420992Z"},"trusted":true},"outputs":[],"source":["# training and validation loops\n","def train(model,\n","    criterion,\n","    optimizer,\n","    scaler,\n","    train_dataloader,\n","    valid_dataloader,\n","    model_path,\n","    max_epochs_stop=10,\n","    n_epochs=400,\n","    min_epoch=300,\n","    print_every=1):\n","    \n","    epochs_no_improve = 0\n","    perf = {\n","        'best_epoch': 0,\n","        'valid_loss_min': np.Inf,\n","        'valid_best_acc': 0,\n","    }\n","    total_epoch = 0\n","\n","    try:\n","        if os.path.exists(model_path):\n","            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n","            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n","            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n","        else:\n","            history = []\n","            log_to_file(f'Starting Training from Scratch.\\n')\n","    except:\n","        history = []\n","        log_to_file(f'exception: start from scratch.\\n')\n","\n","    overall_start = time.time()\n","    if total_epoch >= n_epochs:\n","        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n","        history = pd.DataFrame(\n","            history,\n","            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","        return model, history, perf\n","\n","    seed_everything()\n","\n","    # Main loop - continue training on where we left off if there's a saved model\n","    for epoch in range(total_epoch, n_epochs):\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","        start = time.time()\n","        for ii, (data, target) in enumerate (train_dataloader):\n","            data, target = data.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","\n","            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n","            with torch.cuda.amp.autocast():\n","              # output, aux_output = model(data)\n","              # loss1 = criterion(output, target)\n","              # loss2 = criterion(aux_output, target)\n","              # loss = loss1 + 0.4*loss2\n","              output = model(data)\n","              loss = criterion(output, target)\n","            # loss.backward()\n","            # optimizer.step()\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * data.size(0)\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","            train_acc += accuracy.item() * data.size(0)\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n","            \n","            # cleanup to save VRAM\n","            del data, target\n","#             clean_vram()\n","\n","        # After training loops ends, start validation\n","        else:\n","            with torch.no_grad():\n","                model.eval()\n","                for data, target in valid_dataloader:\n","                    if is_gpu_avail():\n","                        data, target = data.cuda(), target.cuda()\n","                    output = model(data)\n","                    loss = criterion(output, target)\n","                    valid_loss += loss.item() * data.size(0)\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                        correct_tensor.type(torch.FloatTensor))\n","                    valid_acc += accuracy.item() * data.size(0)\n","                    \n","                    # cleanup to save VRAM\n","                    del data, target\n","#                     clean_vram()\n","                train_loss = train_loss / train_data_size\n","                valid_loss = valid_loss / valid_data_size\n","                train_acc = train_acc / train_data_size\n","                valid_acc = valid_acc / valid_data_size\n","                history.append([train_loss, valid_loss,train_acc, valid_acc])\n","                if (epoch + 1) % print_every == 0:\n","                    log_to_file(f'Epoch: {epoch}', True)\n","                    log_to_file(\n","                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n","                        True\n","                    )\n","                    log_to_file(\n","                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n","                        True\n","                    )\n","          \n","                if valid_loss < perf['valid_loss_min']:\n","                    epochs_no_improve = 0\n","                    perf['best_epoch'] = epoch\n","                    perf['valid_loss_min'] = valid_loss\n","                    perf['valid_best_acc'] = valid_acc\n","                    save_model(perf, model, optimizer, scaler, history, model_path)\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n","                        log_to_file(\n","                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n","                        )\n","                        total_time = time.time() - overall_start\n","                        log_to_file(\n","                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n","                        )\n","                        log_to_file()\n","\n","                        # Load the best state from saved model\n","                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","                        # save the full history\n","                        save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history, perf\n","    \n","    total_time = time.time() - overall_start\n","    log_to_file(\n","        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n","    )\n","    log_to_file(\n","        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n","    )\n","    log_to_file()\n","\n","    # Load the best state from saved model\n","    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","    # save the full history\n","    save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","    \n","    return model, history, perf\n","\n","\n","def save_train_val_loss_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_loss', 'valid_loss']:\n","      plt.plot(\n","          history[c], label=c)\n","\n","  title = f'{curr_model} - Training and Validation Losses'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Losses')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def save_train_val_acc_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_acc', 'valid_acc']:\n","      plt.plot(\n","          100 * history[c], label=c)\n","      \n","  title = f'{curr_model} - Training and Validation Accuracy'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Accuracy')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')"]},{"cell_type":"markdown","metadata":{},"source":["### Define Functions to Visualize Prediction"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:30.921413Z","iopub.status.busy":"2022-11-17T15:13:30.921028Z","iopub.status.idle":"2022-11-17T15:13:30.939941Z","shell.execute_reply":"2022-11-17T15:13:30.939041Z","shell.execute_reply.started":"2022-11-17T15:13:30.921378Z"},"trusted":true},"outputs":[],"source":["# confusion matrix \n","def getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n","    model.eval()\n","    confusion_matrix=np.zeros((2,2),dtype=int)\n","    num_images=test_data_size\n","    \n","    with torch.no_grad():\n","        for i, (data,target) in enumerate(dataloader):\n","            data = data.to(device)\n","            target = target.to(device)\n","            \n","            output = model(data) \n","            _, pred = torch.max(output, 1)\n","            \n","            for j in range(data.size()[0]): \n","                if pred[j]==1 and target[j]==1:\n","                    term='TP'\n","                    confusion_matrix[0][0]+=1\n","                elif pred[j]==1 and target[j]==0:\n","                    term='FP'\n","                    confusion_matrix[1][0]+=1\n","                elif pred[j]==0 and target[j]==1:\n","                    term='FN'\n","                    confusion_matrix[0][1]+=1\n","                elif pred[j]==0 and target[j]==0:\n","                    term='TN'\n","                    confusion_matrix[1][1]+=1\n","            \n","                if show_image:\n","                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n","                    log_to_file(term, print_to_console_only)\n","                    imshow(data.cpu().data[j])\n","        \n","        log_to_file(None, print_to_console_only)\n","        category = 'Test' if is_test else 'Validation'\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file(f'{category} Results ', print_to_console_only)\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file('Confusion Matrix: ', print_to_console_only)\n","        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","\n","        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","    return confusion_matrix\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def save_test_acc_n_loss_graph(model, dataloader, criterion):\n","  pass\n","  # NOT NEEDED YET\n","  # with torch.no_grad():\n","  #   model.eval()\n","  #   for data, target in dataloader:\n","  #       if is_gpu_avail():\n","  #           data, target = data.cuda(), target.cuda()\n","  #       output = model(data)\n","  #       loss = criterion(output, target)\n","  #       test_loss += loss.item() * data.size(0)\n","  #       _, pred = torch.max(output, dim=1)\n","  #       correct_tensor = pred.eq(target.data.view_as(pred))\n","  #       accuracy = torch.mean(\n","  #           correct_tensor.type(torch.FloatTensor))\n","  #       test_acc += accuracy.item() * data.size(0)\n","  #   train_loss = train_loss / train_data_size\n","  #   test_loss = test_loss / test_data_size\n","  #   train_acc = train_acc / train_data_size\n","  #   test_acc = test_acc / test_data_size\n","\n","\n","# def visualize_test_prediction(model):\n","#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n","#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n","#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n","\n","#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n","#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n","\n","#   # log_to_file(img_list)\n","\n","#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n","#                               for img in img_list])\n","#   pred_logits_tensor = model(test_batch)\n","#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n","\n","#   row = 12\n","#   col = 3\n","#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n","#   r = 0\n","#   c = 0\n","#   for i, img in enumerate(img_list):\n","#       if c >= col:\n","#         r += 1\n","#         c = 0\n","#       ax = axs[r, c]\n","#       ax.axis('off')\n","#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n","#                                                               100*pred_probs[i,1]))\n","#       ax.imshow(img)\n","#       c +=1\n","\n","#   title = f'{curr_model} - Covid Image Prediction'\n","#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","#   plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def getPredProbs(model, datasetStr, count, isSeeded=True):\n","  if isSeeded:\n","    seed_everything()\n","  \n","  dataset = data[datasetStr].samples\n","  img_list = []\n","  for i, (img_path, cls_idx) in enumerate(dataset):\n","    if i >= count:\n","      break\n","    img_list.append(Image.open(img_path).convert('RGB'))\n","\n","  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n","                              for img in img_list])\n","  pred_logits_tensor = model(test_batch)\n","  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### Run all models - Init Models + Training"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:34.458786Z","iopub.status.busy":"2022-11-17T15:13:34.458420Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: PYTHONHASHSEED=18\n","resnet152 is initialized\n","Starting Training from Scratch.\n","\n","\n","env: PYTHONHASHSEED=18\n","Epoch: 0\t100.00% complete. 1067.34 seconds elapsed in epoch.\n","Best epoch: 0 with loss: 0.5814 and acc: 81.7183%\n","\n","1114.7765 total seconds elapsed. 1114.7765 seconds per epoch.\n","\n","\n","\n","\n","\n","=====================\n","\n","Validation Results \n","\n","=====================\n","\n","Confusion Matrix: \n","\n","[[ 0  5]\n"," [ 0 96]]\n","\n","\n","\n","Sensitivity: 0.0\n","\n","Specificity: 100.0\n","\n","PPV: nan\n","\n","NPV: 95.04950495049505\n","\n","Accuracy: 95.04950495049505\n","\n","F1-Score: 0.0\n","\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\wizlee\\AppData\\Local\\Temp\\ipykernel_6364\\2045143499.py:45: RuntimeWarning: invalid value encountered in long_scalars\n","  log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"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","text/plain":["<Figure size 800x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["model_list = [\n","    models.alexnet.__name__, # 0\n","    models.squeezenet1_1.__name__, #1\n","    models.resnet50.__name__, # 2\n","    models.resnet101.__name__, # 3\n","    models.resnet152.__name__, # 4\n","    models.resnext101_32x8d.__name__, # 5\n","    models.densenet201.__name__, # 6\n","    models.googlenet.__name__, # 7\n","    models.vgg16.__name__, # 8\n","    models.vgg19.__name__, #9\n","    models.inception_v3.__name__, #10\n","]\n","\n","for i in range(0,11):\n","  # https://github.com/pytorch/pytorch/issues/50198\n","  # skipped these because cannot use deterministic algorithm\n","#   skip_model = [0, 1, 5, 8, 9, 10]\n","  skip_model = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n","  if i in skip_model:\n","    continue\n","  curr_model = model_list[i]\n","\n","  # Initialize model, criterion and optimizer\n","  model, criterion, optimizer, scaler = init_model(curr_model)\n","\n","#   Training & Validation\n","  model, history, perf = train(\n","      model,\n","      criterion,\n","      optimizer,\n","      scaler,\n","      train_loader,\n","      val_loader,\n","      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n","      max_epochs_stop=5,  # Early stopping intialization\n","      n_epochs=1,\n","      min_epoch=1,\n","      print_every=10)\n","\n","  history\n","  save_train_val_loss_graph(history, perf)\n","  save_train_val_acc_graph(history, perf)\n","  getConfusionMatrix(model, val_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Out of memory issue\n","\n","- References\n","    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n","    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n","    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n","        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n","    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n","    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n","- The fix\n","    - Delete unused tensor, force garbage collection and run `empty_cache()`\n","    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.cuda.memory_stats(device)\n","# print(torch.cuda.memory_summary(device))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.__version__"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"a13c2ffa9cca1a4271ffd42a538cdd8499349a8341576b85c507ed439a507f24"}}},"nbformat":4,"nbformat_minor":4}
