{"cells":[{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Summary\n","\n","------------------------------------------------------------------------\n","\n","> Expand to see summary and details"]},{"cell_type":"markdown","metadata":{},"source":["## Overview and Explanation\n","\n","1.  This notebook reuses a lot of the [original transfer learning\n","    notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)\n","    -   Here the focus is on building the new custom model using the\n","        CovidNet-CT database.\n","2.  The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:\n","    -   is not longer needed for notebook running in kaggle. Remained\n","        here for references only\n","    -   is where the dataset is being acquired.\n","    -   Explanation of various phases in the [CovidNet-CT ML\n","        code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):\n","        -   train phase is train phase\n","        -   test phase is validation phase\n","        -   infer phase is test phase\n","3.  The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:\n","    -   refers to the way [CovidNet-CT preprocess its\n","        data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)\n","    -   CovidNet-CT uses TensorFlow while this notebook adapts the code\n","        to use PyTorch\n","    -   Two highlights\n","        -   input shape is (512, 512, 3) instead of the (224, 224, 3)\n","            used by the imagenet model\n","        -   the image is cropped to the bounding box provided with the\n","            dataset before resize to 512x512\n","4.  The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:\n","    -   refers to [how CovidNet-CT\n","        trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)\n","    -   This part is almost identical to the original transfer learning\n","        model notebook."]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Import and Deterministic Setup\n","\n","------------------------------------------------------------------------\n","\n","All modules will be imported here including modules used in the\n","[Playground](#playground) section"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:44.921196Z","iopub.status.busy":"2022-11-17T15:12:44.920745Z","iopub.status.idle":"2022-11-17T15:12:45.868415Z","shell.execute_reply":"2022-11-17T15:12:45.866381Z","shell.execute_reply.started":"2022-11-17T15:12:44.921088Z"},"outputId":"9f9e4454-b1ca-47d8-b3c0-873d01043bf2","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: CUDA_LAUNCH_BLOCKING=1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n","env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n"]}],"source":["from __future__ import print_function, division\n","import os\n","%env CUDA_LAUNCH_BLOCKING=1\n","import random\n","import numpy as np\n","import torch\n","\n","from os import path\n","import zipfile\n","import math\n","import sys\n","\n","from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n","from PIL import Image\n","from sklearn.model_selection import train_test_split #, StratifiedShuffleSplit\n","\n","import torchvision\n","from torchvision import models, transforms #, datasets\n","import matplotlib.pyplot as plt\n","\n","import torch.nn as nn\n","from torch import optim\n","from torch.optim import lr_scheduler\n","import time\n","import torch.nn.functional as F\n","\n","import gc\n","import pandas as pd\n","\n","# ensure reproducibility across different executions\n","# https://pytorch.org/docs/stable/notes/randomness.html\n","# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch\n","# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html\n","SEED = 18\n","def seed_everything(seed=18):\n","    random.seed(seed)\n","    %env PYTHONHASHSEED=$seed\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = True\n","#torch.set_deterministic(True)\n","torch.use_deterministic_algorithms(True)\n","%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n","%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32\n","\n","def is_gpu_avail():\n","    GPU_DETECTED = False\n","    try:\n","        GPU_DETECTED = torch.cuda.is_available()\n","    except:\n","        pass\n","\n","    return GPU_DETECTED"]},{"cell_type":"markdown","metadata":{"id":"wMQLloEgzPol"},"source":["---\n","# Setup Kaggle data access to download dataset\n","---\n","\n","- [dataset source from kaggle](https://www.kaggle.com/datasets/hgunraj/covidxct)\n","- On 02-06-2022, CT-3A datasets with 425,024 CT images are uploaded. \n","    - The way of how CT-3A and CT-3B are constructured can be found in [this link](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md).\n","    - This notebook only uses CT-3A that is already constructed and ready for download from the kaggle source."]},{"cell_type":"markdown","metadata":{"id":"w_OfYhgp1V1z"},"source":["## Use Kaggle API to download dataset\n","\n","### Solving google drive not enough capacity to store data\n","- [Article explaining how to access Kaggle Data in colab](https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7)\n","- [An old reddit post that seems to be relevant](https://old.reddit.com/r/PiratedGames/comments/uypcw2/use_google_colab_to_mass_transfer_files_from/)\n","- Some rclone idea\n","    - [rclone gui](https://rclone.org/gui/)\n","    - [rclone lab](https://github.com/acamposxp/RcloneLab)\n","    - [rclone idea 1](https://github.com/ella-tj/Any-File-2-GDrive)\n","    - [rclone idea 2](https://github.com/eaustin6/Rclone-Setup-on-Google-Colab)\n","    - [rclone idea 2](https://towardsdatascience.com/why-you-should-try-rclone-with-google-drive-and-colab-753f3ec04ba1)\n","    - [reddit colab thread](https://old.reddit.com/r/DataHoarder/comments/g069dx/google_colab_is_like_running_code_from_your/)\n","    - [socialgrep on rclone and colab](https://socialgrep.com/search?query=rclone%2Ccolab)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"CZdji66q0Zpt","outputId":"b8a595be-ee84-4084-c706-e72b0528a3c4"},"outputs":[],"source":["CURR_DIR = os.getcwd()\n","dataset_zip = path.join(CURR_DIR, \"covidxct.zip\")\n","dataset_dir = path.join(CURR_DIR, \"data\")\n","\n","# manually download dataset from https://www.kaggle.com/datasets/c395fb339f210700ba392d81bf200f766418238c2734e5237b5dd0b6fc724fcb\n","# The kaggle command despite don't show error, not able to save/download the zip into local file system\n","# kaggle datasets download -d hgunraj/covidxct\n","\n","if not path.exists(dataset_dir):\n","    os.mkdir(dataset_dir)\n","    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n","        zip_ref.extractall(dataset_dir)\n","if path.exists(dataset_zip):\n","  print(dataset_zip)\n","    # os.remove(dataset_zip)"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Data Preprocessing\n","\n","------------------------------------------------------------------------\n","\n","-   [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)\n","-   [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)\n","    -   [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example\n","-   [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)"]},{"cell_type":"markdown","metadata":{},"source":["## Custom Dataset class to load CovidNet data\n","\n","- Various references used: \n","  - https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh\\_#scrollTo=H9doKmx1TXK1 \n","  - https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72 \n","  - https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md \n","  - https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv \n","  - https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887 \n","  - https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py \n","  - https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:50.079878Z","iopub.status.busy":"2022-11-17T15:12:50.079366Z","iopub.status.idle":"2022-11-17T15:12:50.092973Z","shell.execute_reply":"2022-11-17T15:12:50.091975Z","shell.execute_reply.started":"2022-11-17T15:12:50.079844Z"},"trusted":true},"outputs":[],"source":["class CovidNetDataset(Dataset):\n","    def __init__(self, img_dir, split_files, limit_size = 0, transform = None):\n","        # don't seem to need the csv file\n","        # self.df = pd.read_csv(csv_path)\n","        # _, self.class_to_idx  = self.find_classes(csv_path);\n","\n","        self.img_dir = img_dir\n","        self.split_files = split_files\n","        \n","        self.size = 0\n","        self.limit_size = limit_size\n","        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()\n","        self.stradify_removal_based_on_limit()\n","        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.imgs)\n","\n","    def __getitem__(self, index):\n","        # filename = self.df[index, \"FILENAME\"]\n","        # label = self.class_to_idx [self.df[index, \"LABEL\"]]\n","        # image = Image.open(os.path.join(self.img_dir, filename))\n","\n","        label = self.targets[index]\n","        with open(self.imgs[index], \"rb\") as f:\n","            image = Image.open(f)\n","            image = image.crop(self.bboxes[index])\n","            image = image.copy()\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","    \n","\n","        return image, label\n","\n","    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36\n","    def find_classes(self, csv_path=None):\n","        \"\"\"Returns class name array and class_to_idx.\n","        See :class:`CovidNetDataset` for details.\n","        \"\"\"\n","        # class_col = \"finding\"\n","        # classes = sorted(self.df[class_col].unique())\n","        # if not classes:\n","        #     raise FileNotFoundError(f\"Couldn't find any class from '{class_col}' column in {csv_path}.\")\n","\n","        # hard code classes as the order are not alphabetic\n","        classes = ['Normal', 'Pneumonia', 'COVID-19']\n","\n","        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n","        return classes, class_to_idx\n","\n","    def get_all_split_file_data(self):\n","        files, classes, bboxes = [], [], []\n","        for split_file in self.split_files:\n","            f, cls, bb = self.get_data_from_split_file(split_file)\n","            files.extend(f)\n","            classes.extend(cls)\n","            bboxes.extend(bb)\n","        return files, classes, bboxes\n","\n","    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","    def get_data_from_split_file(self, split_file):\n","        \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","        files, classes, bboxes = [], [], []\n","        with open(split_file, 'r') as f:\n","            for line in f.readlines():\n","                fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","                files.append(path.join(self.img_dir, fname))\n","                classes.append(int(cls))\n","                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","                self.size += 1\n","        return files, classes, bboxes\n","    \n","    '''Try to do stratified removal based on limit count if it is specified'''\n","    def stradify_removal_based_on_limit(self):\n","        _, class_to_idx = self.find_classes()\n","        MIN_SIZE = len(class_to_idx) * 10 # allow for some buffer to work with\n","        if self.limit_size <= 0 or self.limit_size <= MIN_SIZE or self.limit_size >= self.size:\n","            return\n","        \n","        total_remove_count = self.size - self.limit_size\n","        occurrence = {idx: self.targets.count(idx) for _, idx in class_to_idx.items()}\n","        target_remove_count = {idx: 0 for _, idx in class_to_idx.items()}\n","        for idx, count in occurrence.items():\n","            target_remove_count[idx] = math.floor(total_remove_count * count / self.size)\n","        \n","        print(occurrence)\n","        print(target_remove_count)\n","        \n","        for i in reversed(range(len(self.targets))):\n","            idx = self.targets[i]\n","            if target_remove_count[idx] > 0:\n","                del self.targets[i]\n","                del self.imgs[i]\n","                del self.bboxes[i]\n","                target_remove_count[idx] -= 1\n","                self.size -= 1"]},{"cell_type":"markdown","metadata":{},"source":["## Spliting dataset into train, val, test\n","\n","-   [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)\n","    -   [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)\n","    -   [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)\n","-   [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)\n","-   [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)\n","-   [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)\n","-   [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:12:54.991228Z","iopub.status.busy":"2022-11-17T15:12:54.990831Z","iopub.status.idle":"2022-11-17T15:12:57.295385Z","shell.execute_reply":"2022-11-17T15:12:57.294226Z","shell.execute_reply.started":"2022-11-17T15:12:54.991172Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 71488, 1: 42943, 2: 310593}\n","{0: 69806, 1: 41932, 2: 303285}\n","Length of full dataset: 10001\n","First 10 train_index: [6373 6939 9338  105 7573 4773 9125 4210 4499 1754]\n","length of train_index: 7999\n","\n","First 10 val_index: [6942 5767  536 6017 6502 1373 8798 3052 4665 9964]\n","length of val_index: 1001\n","\n","First 10 test_index: [9022 9017 9868 1724  795 4754 4568 1985 9612 9534]\n","length of test_index: 1001\n","\n"]}],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = path.join(CURR_DIR, \"data\")\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, 10000)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","_, _, _, _, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","# dataset size\n","train_data_size = len(train_index)\n","valid_data_size = len(val_index)\n","test_data_size = len(test_index)\n","\n","print(f\"First 10 train_index: {train_index[:10]}\")\n","print(f\"length of train_index: {train_data_size}\\n\")\n","print(f\"First 10 val_index: {val_index[:10]}\")\n","print(f\"length of val_index: {valid_data_size}\\n\")\n","print(f\"First 10 test_index: {test_index[:10]}\")\n","print(f\"length of test_index: {test_data_size}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["## Applying transforms to dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Define a wrapper dataset\n","\n","- This is to have the flexibility of applying different transforms to each of the splitted dataset \n","- References \n","    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)\n","    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)\n","    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:05.619690Z","iopub.status.busy":"2022-11-17T15:13:05.619009Z","iopub.status.idle":"2022-11-17T15:13:05.626266Z","shell.execute_reply":"2022-11-17T15:13:05.625249Z","shell.execute_reply.started":"2022-11-17T15:13:05.619650Z"},"trusted":true},"outputs":[],"source":["class WrapperDataset:\n","    def __init__(self, dataset, transform=None, target_transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __getitem__(self, index):\n","        image, label = self.dataset[index]\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        if self.target_transform is not None:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","    def __len__(self):\n","        return len(self.dataset)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining the transforms\n","\n","- References for mean and std of images \n","    - [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit) \n","    - [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1) \n","    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2) \n","    - [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a) \n","    - Bounding box causing issue when batching as stacking don’t work with\n","    different size \n","        - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()` \n","        - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:08.902356Z","iopub.status.busy":"2022-11-17T15:13:08.901940Z","iopub.status.idle":"2022-11-17T15:13:08.915480Z","shell.execute_reply":"2022-11-17T15:13:08.914234Z","shell.execute_reply.started":"2022-11-17T15:13:08.902324Z"},"trusted":true},"outputs":[],"source":["covidnet_std_transform = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE\n","])\n","\n","covidnet_train_transform = transforms.Compose([\n","    transforms.RandomChoice(transforms=[\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomVerticalFlip(),\n","        transforms.RandomRotation(10),\n","        transforms.ColorJitter(brightness=.3, hue=.3),\n","        transforms.RandomPerspective(distortion_scale=0.4),\n","        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])\n","    ])\n","\n","image_transforms = {\n","    'train': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ]),\n","    'val': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'test': transforms.Compose([\n","        covidnet_std_transform\n","    ]),\n","    'playground': transforms.Compose([\n","        covidnet_train_transform,\n","        covidnet_std_transform\n","    ])\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Creating the Dataset Loader"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:11.477712Z","iopub.status.busy":"2022-11-17T15:13:11.477114Z","iopub.status.idle":"2022-11-17T15:13:11.529327Z","shell.execute_reply":"2022-11-17T15:13:11.528231Z","shell.execute_reply.started":"2022-11-17T15:13:11.477670Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: PYTHONHASHSEED=18\n","['Normal', 'Pneumonia', 'COVID-19']\n","{'Normal': 0, 'Pneumonia': 1, 'COVID-19': 2}\n","Using device: cuda\n","train size:7999; validation size:1001; test size:1001\n"]}],"source":["seed_everything(SEED)\n","BATCH_SIZE = 1\n","\n","train_sampler = SubsetRandomSampler(train_index)\n","val_sampler = SubsetRandomSampler(val_index)\n","test_sampler = SubsetRandomSampler(test_index)\n","\n","train_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)\n","val_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)\n","test_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)\n","\n","class_names, class_to_idx = full_dataset.find_classes()\n","print(class_names)\n","print(class_to_idx)\n","\n","if is_gpu_avail():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","    \n","print(f'Using device: {device}')\n","print(f'train size:{train_data_size}; validation size:{valid_data_size}; test size:{test_data_size}')"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Custom Model\n","\n","------------------------------------------------------------------------\n","\n","- The design of this custom model is illustrated in a draw.io diagram \n","    - [Onedrive shared file of all-cnn-diagram.drawio diagram](https://onedrive.live.com/?authkey=%21AL6NGGK0%5FDdNURY&cid=10930FD9F7DD82DD&id=10930FD9F7DD82DD%21226797&parId=10930FD9F7DD82DD%21226791&o=OneUp) \n","    - [link to draw.io of the model](https://app.diagrams.net/#W10930fd9f7dd82dd%2F10930FD9F7DD82DD!226797)"]},{"cell_type":"markdown","metadata":{},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["### Links\n","\n","-   [10 CNN Architecture\n","    Illustrations](https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#bca5)\n","    -   [Visualizing pytorch\n","        models](https://github.com/szagoruyko/pytorchviz)\n","-   Main model building references\n","    -   The [CT-3A github\n","        repo](https://github.com/haydengunraj/COVIDNet-CT/search?q=model)\n","        -   [tensorflow pretrained\n","            models](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/models.md)\n","        -   How to [convert tensorflow checkpoints into pytorch\n","            format](https://github.com/lernapparat/lernapparat/blob/master/style_gan/pytorch_style_gan.ipynb)\n","            -   [pytorch\n","                thread](https://discuss.pytorch.org/t/loading-tensorflow-checkpoints-with-pytorch/151750)\n","        -   [pytorch\n","            thread](https://discuss.pytorch.org/t/combining-trained-models-in-pytorch/28383/44)\n","            about combining two existing models\n","    -   [Pytorch resnext50\n","        implementation](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L792)\n","    -   [pytorch beginner tutorial on building\n","        model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n","-   Other model building references\n","    -   [Custom\n","        Resnet](https://github.com/Arijit-datascience/pytorch_cifar10/blob/main/model/custom_resnet.py)\n","    -   [Resnest convolution block\n","        code](https://github.com/CVHuber/Convolution/blob/main/ResNeSt%20Block.py)\n","    -   [A very clear implementation of InceptionV3](https://github.com/Moeo3/GoogLeNet-Inception-V3-pytorch/blob/master/googlenet_v3.py) that follows the naming of blocks in the diagram"]},{"cell_type":"markdown","metadata":{},"source":["## First try"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CovidNetModel(nn.Module):\n","    def __init__(self):\n","        super().__init__() # same as super(CovidNetModel, self).__init__()\n","        self.inplanes = 64\n","\n","        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=5, bias=False)\n","        self.bn1 = nn.BatchNorm2d(self.inplanes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool1 = nn.MaxPool2d(3, 1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool1(x)\n","        return x\n","\n","    def _make_block():\n","        pass"]},{"cell_type":"markdown","metadata":{},"source":["## Full Model"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"631e7380-b393-4786-be6a-6166b9b785aa"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Using {device} device\")\n","\n","class CovidNetModel(nn.Module):\n","    def __init__(self):\n","        super().__init__() # same as super(CovidNetModel, self).__init__()"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Training & Validation\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Test (Evaluation)\n","\n","------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{},"source":["------------------------------------------------------------------------\n","\n","# Playground\n","\n","------------------------------------------------------------------------\n","\n","> Testbed that is not compulsary for any part of this notebook"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Models using PytorchViz\n","\n","-   https://pytorch.org/docs/stable/nn.html\n","-   https://discuss.pytorch.org/t/combining-multiple-models-and-datasets/82623\n","-   [Mandrin explanation of pytorch resnet\n","    code](https://www.jianshu.com/p/90d61f53d15d)"]},{"cell_type":"code","execution_count":17,"metadata":{"outputId":"9d20f8af-0489-4225-cdd1-4f8747a7c053"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torchviz import make_dot, make_dot_from_trace"]},{"cell_type":"code","execution_count":20,"metadata":{"outputId":"374f5af4-8588-4f75-cf55-3c659c652ca1"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]           4,096\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-12          [-1, 256, 56, 56]             512\n","           Conv2d-13          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-14          [-1, 256, 56, 56]             512\n","             ReLU-15          [-1, 256, 56, 56]               0\n","       Bottleneck-16          [-1, 256, 56, 56]               0\n","           Conv2d-17           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-18           [-1, 64, 56, 56]             128\n","             ReLU-19           [-1, 64, 56, 56]               0\n","           Conv2d-20           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-21           [-1, 64, 56, 56]             128\n","             ReLU-22           [-1, 64, 56, 56]               0\n","           Conv2d-23          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-24          [-1, 256, 56, 56]             512\n","             ReLU-25          [-1, 256, 56, 56]               0\n","       Bottleneck-26          [-1, 256, 56, 56]               0\n","           Conv2d-27           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-28           [-1, 64, 56, 56]             128\n","             ReLU-29           [-1, 64, 56, 56]               0\n","           Conv2d-30           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-31           [-1, 64, 56, 56]             128\n","             ReLU-32           [-1, 64, 56, 56]               0\n","           Conv2d-33          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-34          [-1, 256, 56, 56]             512\n","             ReLU-35          [-1, 256, 56, 56]               0\n","       Bottleneck-36          [-1, 256, 56, 56]               0\n","           Conv2d-37          [-1, 128, 56, 56]          32,768\n","      BatchNorm2d-38          [-1, 128, 56, 56]             256\n","             ReLU-39          [-1, 128, 56, 56]               0\n","           Conv2d-40          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-41          [-1, 128, 28, 28]             256\n","             ReLU-42          [-1, 128, 28, 28]               0\n","           Conv2d-43          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n","           Conv2d-45          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n","             ReLU-47          [-1, 512, 28, 28]               0\n","       Bottleneck-48          [-1, 512, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","           Conv2d-55          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n","             ReLU-57          [-1, 512, 28, 28]               0\n","       Bottleneck-58          [-1, 512, 28, 28]               0\n","           Conv2d-59          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-60          [-1, 128, 28, 28]             256\n","             ReLU-61          [-1, 128, 28, 28]               0\n","           Conv2d-62          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-63          [-1, 128, 28, 28]             256\n","             ReLU-64          [-1, 128, 28, 28]               0\n","           Conv2d-65          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n","             ReLU-67          [-1, 512, 28, 28]               0\n","       Bottleneck-68          [-1, 512, 28, 28]               0\n","           Conv2d-69          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-70          [-1, 128, 28, 28]             256\n","             ReLU-71          [-1, 128, 28, 28]               0\n","           Conv2d-72          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-73          [-1, 128, 28, 28]             256\n","             ReLU-74          [-1, 128, 28, 28]               0\n","           Conv2d-75          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n","             ReLU-77          [-1, 512, 28, 28]               0\n","       Bottleneck-78          [-1, 512, 28, 28]               0\n","           Conv2d-79          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-80          [-1, 128, 28, 28]             256\n","             ReLU-81          [-1, 128, 28, 28]               0\n","           Conv2d-82          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-83          [-1, 128, 28, 28]             256\n","             ReLU-84          [-1, 128, 28, 28]               0\n","           Conv2d-85          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n","             ReLU-87          [-1, 512, 28, 28]               0\n","       Bottleneck-88          [-1, 512, 28, 28]               0\n","           Conv2d-89          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-90          [-1, 128, 28, 28]             256\n","             ReLU-91          [-1, 128, 28, 28]               0\n","           Conv2d-92          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-93          [-1, 128, 28, 28]             256\n","             ReLU-94          [-1, 128, 28, 28]               0\n","           Conv2d-95          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n","             ReLU-97          [-1, 512, 28, 28]               0\n","       Bottleneck-98          [-1, 512, 28, 28]               0\n","           Conv2d-99          [-1, 128, 28, 28]          65,536\n","     BatchNorm2d-100          [-1, 128, 28, 28]             256\n","            ReLU-101          [-1, 128, 28, 28]               0\n","          Conv2d-102          [-1, 128, 28, 28]         147,456\n","     BatchNorm2d-103          [-1, 128, 28, 28]             256\n","            ReLU-104          [-1, 128, 28, 28]               0\n","          Conv2d-105          [-1, 512, 28, 28]          65,536\n","     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n","            ReLU-107          [-1, 512, 28, 28]               0\n","      Bottleneck-108          [-1, 512, 28, 28]               0\n","          Conv2d-109          [-1, 128, 28, 28]          65,536\n","     BatchNorm2d-110          [-1, 128, 28, 28]             256\n","            ReLU-111          [-1, 128, 28, 28]               0\n","          Conv2d-112          [-1, 128, 28, 28]         147,456\n","     BatchNorm2d-113          [-1, 128, 28, 28]             256\n","            ReLU-114          [-1, 128, 28, 28]               0\n","          Conv2d-115          [-1, 512, 28, 28]          65,536\n","     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n","            ReLU-117          [-1, 512, 28, 28]               0\n","      Bottleneck-118          [-1, 512, 28, 28]               0\n","          Conv2d-119          [-1, 256, 28, 28]         131,072\n","     BatchNorm2d-120          [-1, 256, 28, 28]             512\n","            ReLU-121          [-1, 256, 28, 28]               0\n","          Conv2d-122          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-123          [-1, 256, 14, 14]             512\n","            ReLU-124          [-1, 256, 14, 14]               0\n","          Conv2d-125         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n","          Conv2d-127         [-1, 1024, 14, 14]         524,288\n","     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n","            ReLU-129         [-1, 1024, 14, 14]               0\n","      Bottleneck-130         [-1, 1024, 14, 14]               0\n","          Conv2d-131          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-132          [-1, 256, 14, 14]             512\n","            ReLU-133          [-1, 256, 14, 14]               0\n","          Conv2d-134          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-135          [-1, 256, 14, 14]             512\n","            ReLU-136          [-1, 256, 14, 14]               0\n","          Conv2d-137         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n","            ReLU-139         [-1, 1024, 14, 14]               0\n","      Bottleneck-140         [-1, 1024, 14, 14]               0\n","          Conv2d-141          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-142          [-1, 256, 14, 14]             512\n","            ReLU-143          [-1, 256, 14, 14]               0\n","          Conv2d-144          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-145          [-1, 256, 14, 14]             512\n","            ReLU-146          [-1, 256, 14, 14]               0\n","          Conv2d-147         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n","            ReLU-149         [-1, 1024, 14, 14]               0\n","      Bottleneck-150         [-1, 1024, 14, 14]               0\n","          Conv2d-151          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-152          [-1, 256, 14, 14]             512\n","            ReLU-153          [-1, 256, 14, 14]               0\n","          Conv2d-154          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-155          [-1, 256, 14, 14]             512\n","            ReLU-156          [-1, 256, 14, 14]               0\n","          Conv2d-157         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n","            ReLU-159         [-1, 1024, 14, 14]               0\n","      Bottleneck-160         [-1, 1024, 14, 14]               0\n","          Conv2d-161          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-162          [-1, 256, 14, 14]             512\n","            ReLU-163          [-1, 256, 14, 14]               0\n","          Conv2d-164          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-165          [-1, 256, 14, 14]             512\n","            ReLU-166          [-1, 256, 14, 14]               0\n","          Conv2d-167         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n","            ReLU-169         [-1, 1024, 14, 14]               0\n","      Bottleneck-170         [-1, 1024, 14, 14]               0\n","          Conv2d-171          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-172          [-1, 256, 14, 14]             512\n","            ReLU-173          [-1, 256, 14, 14]               0\n","          Conv2d-174          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-175          [-1, 256, 14, 14]             512\n","            ReLU-176          [-1, 256, 14, 14]               0\n","          Conv2d-177         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n","            ReLU-179         [-1, 1024, 14, 14]               0\n","      Bottleneck-180         [-1, 1024, 14, 14]               0\n","          Conv2d-181          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-182          [-1, 256, 14, 14]             512\n","            ReLU-183          [-1, 256, 14, 14]               0\n","          Conv2d-184          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-185          [-1, 256, 14, 14]             512\n","            ReLU-186          [-1, 256, 14, 14]               0\n","          Conv2d-187         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n","            ReLU-189         [-1, 1024, 14, 14]               0\n","      Bottleneck-190         [-1, 1024, 14, 14]               0\n","          Conv2d-191          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-192          [-1, 256, 14, 14]             512\n","            ReLU-193          [-1, 256, 14, 14]               0\n","          Conv2d-194          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-195          [-1, 256, 14, 14]             512\n","            ReLU-196          [-1, 256, 14, 14]               0\n","          Conv2d-197         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n","            ReLU-199         [-1, 1024, 14, 14]               0\n","      Bottleneck-200         [-1, 1024, 14, 14]               0\n","          Conv2d-201          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-202          [-1, 256, 14, 14]             512\n","            ReLU-203          [-1, 256, 14, 14]               0\n","          Conv2d-204          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-205          [-1, 256, 14, 14]             512\n","            ReLU-206          [-1, 256, 14, 14]               0\n","          Conv2d-207         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n","            ReLU-209         [-1, 1024, 14, 14]               0\n","      Bottleneck-210         [-1, 1024, 14, 14]               0\n","          Conv2d-211          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-212          [-1, 256, 14, 14]             512\n","            ReLU-213          [-1, 256, 14, 14]               0\n","          Conv2d-214          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-215          [-1, 256, 14, 14]             512\n","            ReLU-216          [-1, 256, 14, 14]               0\n","          Conv2d-217         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n","            ReLU-219         [-1, 1024, 14, 14]               0\n","      Bottleneck-220         [-1, 1024, 14, 14]               0\n","          Conv2d-221          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-222          [-1, 256, 14, 14]             512\n","            ReLU-223          [-1, 256, 14, 14]               0\n","          Conv2d-224          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-225          [-1, 256, 14, 14]             512\n","            ReLU-226          [-1, 256, 14, 14]               0\n","          Conv2d-227         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n","            ReLU-229         [-1, 1024, 14, 14]               0\n","      Bottleneck-230         [-1, 1024, 14, 14]               0\n","          Conv2d-231          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-232          [-1, 256, 14, 14]             512\n","            ReLU-233          [-1, 256, 14, 14]               0\n","          Conv2d-234          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-235          [-1, 256, 14, 14]             512\n","            ReLU-236          [-1, 256, 14, 14]               0\n","          Conv2d-237         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n","            ReLU-239         [-1, 1024, 14, 14]               0\n","      Bottleneck-240         [-1, 1024, 14, 14]               0\n","          Conv2d-241          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-242          [-1, 256, 14, 14]             512\n","            ReLU-243          [-1, 256, 14, 14]               0\n","          Conv2d-244          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-245          [-1, 256, 14, 14]             512\n","            ReLU-246          [-1, 256, 14, 14]               0\n","          Conv2d-247         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n","            ReLU-249         [-1, 1024, 14, 14]               0\n","      Bottleneck-250         [-1, 1024, 14, 14]               0\n","          Conv2d-251          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-252          [-1, 256, 14, 14]             512\n","            ReLU-253          [-1, 256, 14, 14]               0\n","          Conv2d-254          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-255          [-1, 256, 14, 14]             512\n","            ReLU-256          [-1, 256, 14, 14]               0\n","          Conv2d-257         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n","            ReLU-259         [-1, 1024, 14, 14]               0\n","      Bottleneck-260         [-1, 1024, 14, 14]               0\n","          Conv2d-261          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-262          [-1, 256, 14, 14]             512\n","            ReLU-263          [-1, 256, 14, 14]               0\n","          Conv2d-264          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-265          [-1, 256, 14, 14]             512\n","            ReLU-266          [-1, 256, 14, 14]               0\n","          Conv2d-267         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n","            ReLU-269         [-1, 1024, 14, 14]               0\n","      Bottleneck-270         [-1, 1024, 14, 14]               0\n","          Conv2d-271          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-272          [-1, 256, 14, 14]             512\n","            ReLU-273          [-1, 256, 14, 14]               0\n","          Conv2d-274          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-275          [-1, 256, 14, 14]             512\n","            ReLU-276          [-1, 256, 14, 14]               0\n","          Conv2d-277         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n","            ReLU-279         [-1, 1024, 14, 14]               0\n","      Bottleneck-280         [-1, 1024, 14, 14]               0\n","          Conv2d-281          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-282          [-1, 256, 14, 14]             512\n","            ReLU-283          [-1, 256, 14, 14]               0\n","          Conv2d-284          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-285          [-1, 256, 14, 14]             512\n","            ReLU-286          [-1, 256, 14, 14]               0\n","          Conv2d-287         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n","            ReLU-289         [-1, 1024, 14, 14]               0\n","      Bottleneck-290         [-1, 1024, 14, 14]               0\n","          Conv2d-291          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-292          [-1, 256, 14, 14]             512\n","            ReLU-293          [-1, 256, 14, 14]               0\n","          Conv2d-294          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-295          [-1, 256, 14, 14]             512\n","            ReLU-296          [-1, 256, 14, 14]               0\n","          Conv2d-297         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n","            ReLU-299         [-1, 1024, 14, 14]               0\n","      Bottleneck-300         [-1, 1024, 14, 14]               0\n","          Conv2d-301          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-302          [-1, 256, 14, 14]             512\n","            ReLU-303          [-1, 256, 14, 14]               0\n","          Conv2d-304          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-305          [-1, 256, 14, 14]             512\n","            ReLU-306          [-1, 256, 14, 14]               0\n","          Conv2d-307         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n","            ReLU-309         [-1, 1024, 14, 14]               0\n","      Bottleneck-310         [-1, 1024, 14, 14]               0\n","          Conv2d-311          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-312          [-1, 256, 14, 14]             512\n","            ReLU-313          [-1, 256, 14, 14]               0\n","          Conv2d-314          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-315          [-1, 256, 14, 14]             512\n","            ReLU-316          [-1, 256, 14, 14]               0\n","          Conv2d-317         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n","            ReLU-319         [-1, 1024, 14, 14]               0\n","      Bottleneck-320         [-1, 1024, 14, 14]               0\n","          Conv2d-321          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-322          [-1, 256, 14, 14]             512\n","            ReLU-323          [-1, 256, 14, 14]               0\n","          Conv2d-324          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-325          [-1, 256, 14, 14]             512\n","            ReLU-326          [-1, 256, 14, 14]               0\n","          Conv2d-327         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n","            ReLU-329         [-1, 1024, 14, 14]               0\n","      Bottleneck-330         [-1, 1024, 14, 14]               0\n","          Conv2d-331          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-332          [-1, 256, 14, 14]             512\n","            ReLU-333          [-1, 256, 14, 14]               0\n","          Conv2d-334          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-335          [-1, 256, 14, 14]             512\n","            ReLU-336          [-1, 256, 14, 14]               0\n","          Conv2d-337         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n","            ReLU-339         [-1, 1024, 14, 14]               0\n","      Bottleneck-340         [-1, 1024, 14, 14]               0\n","          Conv2d-341          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-342          [-1, 256, 14, 14]             512\n","            ReLU-343          [-1, 256, 14, 14]               0\n","          Conv2d-344          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-345          [-1, 256, 14, 14]             512\n","            ReLU-346          [-1, 256, 14, 14]               0\n","          Conv2d-347         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n","            ReLU-349         [-1, 1024, 14, 14]               0\n","      Bottleneck-350         [-1, 1024, 14, 14]               0\n","          Conv2d-351          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-352          [-1, 256, 14, 14]             512\n","            ReLU-353          [-1, 256, 14, 14]               0\n","          Conv2d-354          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-355          [-1, 256, 14, 14]             512\n","            ReLU-356          [-1, 256, 14, 14]               0\n","          Conv2d-357         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n","            ReLU-359         [-1, 1024, 14, 14]               0\n","      Bottleneck-360         [-1, 1024, 14, 14]               0\n","          Conv2d-361          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-362          [-1, 256, 14, 14]             512\n","            ReLU-363          [-1, 256, 14, 14]               0\n","          Conv2d-364          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-365          [-1, 256, 14, 14]             512\n","            ReLU-366          [-1, 256, 14, 14]               0\n","          Conv2d-367         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n","            ReLU-369         [-1, 1024, 14, 14]               0\n","      Bottleneck-370         [-1, 1024, 14, 14]               0\n","          Conv2d-371          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-372          [-1, 256, 14, 14]             512\n","            ReLU-373          [-1, 256, 14, 14]               0\n","          Conv2d-374          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-375          [-1, 256, 14, 14]             512\n","            ReLU-376          [-1, 256, 14, 14]               0\n","          Conv2d-377         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n","            ReLU-379         [-1, 1024, 14, 14]               0\n","      Bottleneck-380         [-1, 1024, 14, 14]               0\n","          Conv2d-381          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-382          [-1, 256, 14, 14]             512\n","            ReLU-383          [-1, 256, 14, 14]               0\n","          Conv2d-384          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-385          [-1, 256, 14, 14]             512\n","            ReLU-386          [-1, 256, 14, 14]               0\n","          Conv2d-387         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n","            ReLU-389         [-1, 1024, 14, 14]               0\n","      Bottleneck-390         [-1, 1024, 14, 14]               0\n","          Conv2d-391          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-392          [-1, 256, 14, 14]             512\n","            ReLU-393          [-1, 256, 14, 14]               0\n","          Conv2d-394          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-395          [-1, 256, 14, 14]             512\n","            ReLU-396          [-1, 256, 14, 14]               0\n","          Conv2d-397         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n","            ReLU-399         [-1, 1024, 14, 14]               0\n","      Bottleneck-400         [-1, 1024, 14, 14]               0\n","          Conv2d-401          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-402          [-1, 256, 14, 14]             512\n","            ReLU-403          [-1, 256, 14, 14]               0\n","          Conv2d-404          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-405          [-1, 256, 14, 14]             512\n","            ReLU-406          [-1, 256, 14, 14]               0\n","          Conv2d-407         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n","            ReLU-409         [-1, 1024, 14, 14]               0\n","      Bottleneck-410         [-1, 1024, 14, 14]               0\n","          Conv2d-411          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-412          [-1, 256, 14, 14]             512\n","            ReLU-413          [-1, 256, 14, 14]               0\n","          Conv2d-414          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-415          [-1, 256, 14, 14]             512\n","            ReLU-416          [-1, 256, 14, 14]               0\n","          Conv2d-417         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n","            ReLU-419         [-1, 1024, 14, 14]               0\n","      Bottleneck-420         [-1, 1024, 14, 14]               0\n","          Conv2d-421          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-422          [-1, 256, 14, 14]             512\n","            ReLU-423          [-1, 256, 14, 14]               0\n","          Conv2d-424          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-425          [-1, 256, 14, 14]             512\n","            ReLU-426          [-1, 256, 14, 14]               0\n","          Conv2d-427         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n","            ReLU-429         [-1, 1024, 14, 14]               0\n","      Bottleneck-430         [-1, 1024, 14, 14]               0\n","          Conv2d-431          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-432          [-1, 256, 14, 14]             512\n","            ReLU-433          [-1, 256, 14, 14]               0\n","          Conv2d-434          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-435          [-1, 256, 14, 14]             512\n","            ReLU-436          [-1, 256, 14, 14]               0\n","          Conv2d-437         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n","            ReLU-439         [-1, 1024, 14, 14]               0\n","      Bottleneck-440         [-1, 1024, 14, 14]               0\n","          Conv2d-441          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-442          [-1, 256, 14, 14]             512\n","            ReLU-443          [-1, 256, 14, 14]               0\n","          Conv2d-444          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-445          [-1, 256, 14, 14]             512\n","            ReLU-446          [-1, 256, 14, 14]               0\n","          Conv2d-447         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n","            ReLU-449         [-1, 1024, 14, 14]               0\n","      Bottleneck-450         [-1, 1024, 14, 14]               0\n","          Conv2d-451          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-452          [-1, 256, 14, 14]             512\n","            ReLU-453          [-1, 256, 14, 14]               0\n","          Conv2d-454          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-455          [-1, 256, 14, 14]             512\n","            ReLU-456          [-1, 256, 14, 14]               0\n","          Conv2d-457         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n","            ReLU-459         [-1, 1024, 14, 14]               0\n","      Bottleneck-460         [-1, 1024, 14, 14]               0\n","          Conv2d-461          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-462          [-1, 256, 14, 14]             512\n","            ReLU-463          [-1, 256, 14, 14]               0\n","          Conv2d-464          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-465          [-1, 256, 14, 14]             512\n","            ReLU-466          [-1, 256, 14, 14]               0\n","          Conv2d-467         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n","            ReLU-469         [-1, 1024, 14, 14]               0\n","      Bottleneck-470         [-1, 1024, 14, 14]               0\n","          Conv2d-471          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-472          [-1, 256, 14, 14]             512\n","            ReLU-473          [-1, 256, 14, 14]               0\n","          Conv2d-474          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-475          [-1, 256, 14, 14]             512\n","            ReLU-476          [-1, 256, 14, 14]               0\n","          Conv2d-477         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n","            ReLU-479         [-1, 1024, 14, 14]               0\n","      Bottleneck-480         [-1, 1024, 14, 14]               0\n","          Conv2d-481          [-1, 512, 14, 14]         524,288\n","     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n","            ReLU-483          [-1, 512, 14, 14]               0\n","          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n","            ReLU-486            [-1, 512, 7, 7]               0\n","          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n","          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n","     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n","            ReLU-491           [-1, 2048, 7, 7]               0\n","      Bottleneck-492           [-1, 2048, 7, 7]               0\n","          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n","            ReLU-495            [-1, 512, 7, 7]               0\n","          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n","            ReLU-498            [-1, 512, 7, 7]               0\n","          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n","            ReLU-501           [-1, 2048, 7, 7]               0\n","      Bottleneck-502           [-1, 2048, 7, 7]               0\n","          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n","            ReLU-505            [-1, 512, 7, 7]               0\n","          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n","            ReLU-508            [-1, 512, 7, 7]               0\n","          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n","            ReLU-511           [-1, 2048, 7, 7]               0\n","      Bottleneck-512           [-1, 2048, 7, 7]               0\n","AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n","         Dropout-514                 [-1, 2048]               0\n","          Linear-515                    [-1, 3]           6,147\n","================================================================\n","Total params: 58,149,955\n","Trainable params: 58,149,955\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 606.60\n","Params size (MB): 221.82\n","Estimated Total Size (MB): 829.00\n","----------------------------------------------------------------\n"]}],"source":["from torchvision import models\n","# from torchinfo import summary\n","from torchsummary import summary\n","\n","# model = CovidNetModel()\n","# print(model)\n","# summary(model, (3, 224, 224))\n","\n","# render_model_pic_file = \"resnext50_32x4d\"\n","# model = models.resnext50_32x4d()\n","summary(model, (3, 224, 224))\n","\n","# x = torch.randn(1, 3, 224, 224).requires_grad_(True)\n","# y = model(x)\n","# dot = make_dot(y, params=dict(model.named_parameters()))\n","\n","# dot.format = \"png\"\n","# dot.render(render_model_pic_file)\n","# files.download(f\"{render_model_pic_file}.{dot.format}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigate Image in Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"dee59970-4dda-4616-d934-7638ab68dc23"},"outputs":[],"source":["import subprocess\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","DATASET_DIR = path.join(DATA_DIR, \"3A_images/\")\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","\n","# count number of images\n","# !ls -Uba1 /content/data/3A_images | grep -c png\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"grep\", \"-c\", \"png\"), stdin=ls.stdout)\n","print(f\"number of images: {output.decode()}\")\n","\n","# list first 10 images\n","ls = subprocess.Popen((\"ls\", \"-Uba1\", DATASET_DIR), stdout=subprocess.PIPE)\n","output = subprocess.check_output((\"head\", \"-10\"), stdin=ls.stdout)\n","first_10_lines = output.decode()\n","# print(f\"first 10 images in {DATASET_DIR}:\\n{first_10_lines}\")\n","img_list = first_10_lines.split('\\n')\n","first_png = next(filter(lambda img: \"png\" in img, img_list), None)\n","print(f\"first_png: {first_png}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"d0ccff4a-152e-48aa-8b8f-dadafcc7b42b"},"outputs":[],"source":["# show the first test image, unbounded followed by bounded\n","import torch\n","import matplotlib.pyplot as plt\n","import torchvision.transforms.functional as torch_func_trans\n","from PIL import Image\n","\n","# from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108\n","def get_data_from_split_file(split_file):\n","    \"\"\"Gets image filenames, classes and bboxes\"\"\"\n","    files, classes, bboxes = [], [], []\n","    with open(split_file, 'r') as f:\n","        for line in f.readlines():\n","            fname, cls, xmin, ymin, xmax, ymax = line.strip('\\n').split()\n","            files.append(path.join(DATASET_DIR, fname))\n","            classes.append(int(cls))\n","            bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n","    return files, classes, bboxes\n","\n","def bbox_to_topLeftOrigin_size(xmin, ymin, xmax, ymax):\n","    top = ymax\n","    left = xmin\n","    height = ymax - ymin\n","    width = xmax - xmin\n","    return top, left, height, width\n","\n","files, classes, bbox = get_data_from_split_file(TEST_SPLIT_FILE)\n","first_file_tuple = (files[0], classes[0], bbox[0])\n","\n","print(f\"first image: {first_file_tuple[0]}\")\n","if 0:\n","    # this way of reading image is deprecated\n","    img = plt.imread(first_file_tuple[0])\n","\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"unbounded\")\n","    _ = plt.imshow(img)\n","    _ = plt.axis('off')\n","\n","    torch_img = torch.from_numpy(img)\n","    print(f\"torch_img size: {torch_img.size()}\")\n","    pytorch_size = bbox_to_topLeftOrigin_size(*first_file_tuple[2])\n","    print(f\"pytorch_size: {pytorch_size}\")\n","\n","    cropped_img = torch_func_trans.crop(torch_img, *pytorch_size)\n","    fig = plt.figure(figsize=(15,15))\n","    plt.title(\"bounded\")\n","    _ = plt.imshow(cropped_img)\n","    _ = plt.axis('off')\n","\n","# This is the recommended method for opening image\n","# https://pillow.readthedocs.io/en/stable/reference/Image.html#examples\n","with Image.open(first_file_tuple[0]) as im:\n","    print(\"\\n\")\n","    print(\"Unbounded image\")\n","    IMG = im\n","    display(im)\n","\n","    print(\"\\n\")\n","    print(\"Bounded image\")\n","    display(im.crop(first_file_tuple[2]))\n","\n","    # https://pillow.readthedocs.io/en/latest/reference/open_files.html#file-handling\n","    print(\"\\n\")\n","    print(\"Out of Scope Unbounded image\")\n","    display(IMG)\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Investigate metadata.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"f442a9a0-5034-4330-c596-c460d7f389a9"},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.expand_frame_repr', False)\n","\n","# Specify all the filepath of the dataset\n","DATA_DIR = \"/kaggle/input/covidxct\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","meta_df = pd.read_csv(METADATA_CSV)\n","print(\"First 5 rows in metadata.csv0\")\n","print(meta_df.head(5))\n","print(f\"classes: {sorted(meta_df['finding'].unique())}\")\n","\n","unverified = meta_df['verified finding'].eq('No').sum()\n","verified = meta_df['verified finding'].eq('Yes').sum()\n","print(f\"Not verified: {unverified}\")\n","print(f\"Verified: {verified}\")\n","\n","unverified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'No', 'patient id'].unique()\n","assert len(unverified_patient_ids) == unverified # patient id is expected to be unique in metadata.csv\n","print(f\"first unverified ID: {unverified_patient_ids[0]}\")\n","\n","verified_patient_ids = meta_df.loc[meta_df['verified finding'] == 'Yes', 'patient id'].unique()\n","assert len(verified_patient_ids) == verified # patient id is expected to be unique in metadata.csv\n","print(f\"first verified ID: {verified_patient_ids[0]}\")\n","\n","imgs = [entry.name for entry in os.scandir(DATASET_DIR) if entry.is_file()]\n","print(f\"total images: {len(imgs)}\")\n","print(f\"first 10 images: {imgs[:10]}\")\n","imgs_of_1_patient = [img for img in imgs if verified_patient_ids[0] in img]\n","print(f\"Images of patient ID {verified_patient_ids[0]}: {imgs_of_1_patient}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Investigating split dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"outputId":"6b8fad54-930b-4452-a1a6-aff1ba381edf"},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 71488, 1: 42943, 2: 310593}\n","{0: 69806, 1: 41932, 2: 303285}\n","Length of full dataset: 10001\n","env: PYTHONHASHSEED=18\n","**************************************************\n","temp_train\n","**************************************************\n","First 10 index: [2460 2473 5079 1711 8967 4549 7073   98 1770 3621]\n","First 10 label: [2, 2, 2, 2, 0, 2, 2, 2, 2, 2]\n","length of index: 9000\n",">>>Distribution of labels:\n","Normal: 1514\n","Pneunomia: 910\n","Covid-19: 6576\n","\n","**************************************************\n","test\n","**************************************************\n","First 10 index: [9022 9017 9868 1724  795 4754 4568 1985 9612 9534]\n","First 10 label: [0, 0, 0, 2, 2, 2, 2, 2, 0, 0]\n","length of index: 1001\n","\n",">>>Distribution of labels:\n","Normal: 168\n","Pneunomia: 101\n","Covid-19: 732\n","\n","**************************************************\n","train\n","**************************************************\n","First 10 index: [6373 6939 9338  105 7573 4773 9125 4210 4499 1754]\n","First 10 label: [2, 2, 0, 2, 1, 2, 0, 2, 2, 2]\n","length of index: 7999\n","\n",">>>Distribution of labels:\n","Normal: 1346\n","Pneunomia: 809\n","Covid-19: 5844\n","\n","**************************************************\n","val\n","**************************************************\n","First 10 index: [6942 5767  536 6017 6502 1373 8798 3052 4665 9964]\n","First 10 label: [2, 2, 2, 2, 2, 2, 0, 2, 2, 0]\n","length of index: 1001\n","\n",">>>Distribution of labels:\n","Normal: 168\n","Pneunomia: 101\n","Covid-19: 732\n"]}],"source":["# Specify all the filepath of the dataset\n","DATA_DIR = f\"{CURR_DIR}/data\"\n","dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]\n","assert len(dirs) == 1 # expects to only have 1 folder that contains all the images\n","\n","DATASET_DIR = path.join(DATA_DIR, dirs[0])\n","METADATA_CSV = path.join(DATA_DIR, \"metadata.csv\")\n","\n","DATASET_NAME = \"COVIDx_CT-3A\"\n","TRAIN_SPLIT_FILE = path.join(DATA_DIR, f\"train_{DATASET_NAME}.txt\")\n","VAL_SPLIT_FILE = path.join(DATA_DIR, f\"val_{DATASET_NAME}.txt\")\n","TEST_SPLIT_FILE = path.join(DATA_DIR, f\"test_{DATASET_NAME}.txt\")\n","SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]\n","\n","MAX_SIZE = 10000\n","full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES, MAX_SIZE)\n","full_data_len = len(full_dataset)\n","print(f\"Length of full dataset: {full_data_len}\")\n","\n","SEED = 18\n","seed_everything(SEED)\n","BATCH_SIZE = 128\n","\n","# # Defines ratios, w.r.t. whole dataset.\n","ratio_train = 0.8\n","ratio_val = 0.1\n","ratio_test = 0.1\n","dummy_X = np.zeros(full_data_len)\n","indexes = np.arange(full_data_len)\n","\n","# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining\n","# to be used in the next step. \n","# Note that an additional indexes array is provided\n","x_remaining, X_test, y_remaining, Y_test, temp_train_index, test_index = train_test_split(\n","    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)\n","# train_index, test_index = next(\n","#     StratifiedShuffleSplit(n_splits=1, test_size=ratio_test, random_state=SEED).split(\n","#         dummy_X, full_dataset.targets\n","#     )\n","# )\n","\n","print('*'*50)\n","print(\"temp_train\")\n","print('*'*50)\n","print(f\"First 10 index: {temp_train_index[:10]}\")\n","print(f\"First 10 label: {y_remaining[:10]}\")\n","print(f\"length of index: {len(temp_train_index)}\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {y_remaining.count(0)}\")\n","print(f\"Pneunomia: {y_remaining.count(1)}\")\n","print(f\"Covid-19: {y_remaining.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"test\")\n","print('*'*50)\n","print(f\"First 10 index: {test_index[:10]}\")\n","print(f\"First 10 label: {Y_test[:10]}\")\n","print(f\"length of index: {len(test_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_test.count(0)}\")\n","print(f\"Pneunomia: {Y_test.count(1)}\")\n","print(f\"Covid-19: {Y_test.count(2)}\")\n","\n","# Adjusts val ratio, w.r.t. remaining dataset.\n","ratio_remaining = 1 - ratio_test\n","ratio_val_adjusted = ratio_val / ratio_remaining\n","\n","# Produces train and val splits.\n","X_train, X_val, Y_train, Y_val, train_index, val_index = train_test_split(\n","    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)\n","\n","print()\n","print('*'*50)\n","print(\"train\")\n","print('*'*50)\n","print(f\"First 10 index: {train_index[:10]}\")\n","print(f\"First 10 label: {Y_train[:10]}\")\n","print(f\"length of index: {len(train_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_train.count(0)}\")\n","print(f\"Pneunomia: {Y_train.count(1)}\")\n","print(f\"Covid-19: {Y_train.count(2)}\")\n","\n","print()\n","print('*'*50)\n","print(\"val\")\n","print('*'*50)\n","print(f\"First 10 index: {val_index[:10]}\")\n","print(f\"First 10 label: {Y_val[:10]}\")\n","print(f\"length of index: {len(val_index)}\\n\")\n","print(\">>>Distribution of labels:\")\n","print(f\"Normal: {Y_val.count(0)}\")\n","print(f\"Pneunomia: {Y_val.count(1)}\")\n","print(f\"Covid-19: {Y_val.count(2)}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Visualizing Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"outputId":"151b91d7-9c07-41f3-ff2f-ac0139aaca5f"},"outputs":[],"source":["import pprint\n","def imshow(inp, title=None):\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    # mean = np.array([0.485, 0.456, 0.406])\n","    # std = np.array([0.229, 0.224, 0.225])\n","    # inp = std * inp + mean\n","    # inp = np.clip(inp, 0, 1)\n","    plt.figure(figsize=[15, 15])\n","    plt.imshow(inp)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","seed_everything(19)\n","class_names, _ = full_dataset.find_classes()\n","data, classes = next(iter(train_loader)) # note that it is normal for warning about clipping here if the image has been normalized\n","# out = torchvision.utils.make_grid(data)\n","# imshow(out)\n","# pp = pprint.PrettyPrinter(compact=True)\n","# pp.pprint([class_names[x] for x in classes])\n","\n","print(f\"Class: {classes[0]}\")\n","out = data[0]\n","imshow(out)"]},{"cell_type":"markdown","metadata":{},"source":["## Trying Avalance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# https://changhsinlee.com/colab-import-python/\n","!pip install requests\n","!pip install avalanche-lib"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import requests\n","\n","# Save datagenerators as file to colab working directory\n","# If you are using GitHub, make sure you get the \"Raw\" version of the code\n","url = 'https://raw.githubusercontent.com/ContinualAI/avalanche/master/examples/pytorchcv_models.py'\n","r = requests.get(url)\n","\n","# make sure your filename is the same as how you want to import \n","with open('pytorchcv_models.py', 'w') as f:\n","    f.write(r.text)\n","\n","# now we can import\n","import pytorchcv_models as pycv\n","from types import SimpleNamespace\n","\n","args = SimpleNamespace()\n","args.cuda = 0\n","pycv.main(args)"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained Model"]},{"cell_type":"markdown","metadata":{},"source":["### Helper Functions"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:19.895641Z","iopub.status.busy":"2022-11-17T15:13:19.895051Z","iopub.status.idle":"2022-11-17T15:13:19.903105Z","shell.execute_reply":"2022-11-17T15:13:19.901963Z","shell.execute_reply.started":"2022-11-17T15:13:19.895607Z"},"trusted":true},"outputs":[],"source":["LOG_DIR = os.path.join(CURR_DIR, \"log\")\n","RESULT_DIR = os.path.join(CURR_DIR, 'result')\n","curr_model = \"\"\n","\n","def log_to_file(txt=None, print_to_console_only=False):\n","  if txt is None:\n","    txt = ''\n","  txt += '\\n'\n","  print(txt)\n","  if print_to_console_only:\n","    return\n","  if not path.exists(LOG_DIR):\n","    os.mkdir(LOG_DIR)\n","  full_path = os.path.join(LOG_DIR, f'{curr_model}.txt')\n","  with open(full_path, mode='a') as f:\n","    f.write(txt)\n","    \n","# https://discuss.pytorch.org/t/clearing-the-gpu-is-a-headache/84762\n","# Make sure to delete any references to tensor. Else this function will not have significant effect\n","def clean_vram():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","# https://stackoverflow.com/questions/33162319/get-current-function-name-inside-that-function-using-python\n","def name_of_caller(frame=1):\n","    \"\"\"\n","    Return \"class.function_name\" of the caller or just \"function_name\".\n","    \"\"\"\n","    frame = sys._getframe(frame)\n","    fn_name = frame.f_code.co_name\n","    var_names = frame.f_code.co_varnames\n","    if var_names:\n","        if var_names[0] == \"self\":\n","            self_obj = frame.f_locals.get(\"self\")\n","            if self_obj is not None:\n","                return f\"{type(self_obj).__name__}.{fn_name}\" \n","        if var_names[0] == \"cls\":\n","            cls_obj = frame.f_locals.get(\"cls\")\n","            if cls_obj is not None:\n","                return f\"{cls_obj.__name__}.{fn_name}\"\n","    return fn_name"]},{"cell_type":"markdown","metadata":{},"source":["### Define Function to Initialize Deep Learning Models"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:22.599863Z","iopub.status.busy":"2022-11-17T15:13:22.599496Z","iopub.status.idle":"2022-11-17T15:13:22.618257Z","shell.execute_reply":"2022-11-17T15:13:22.617089Z","shell.execute_reply.started":"2022-11-17T15:13:22.599831Z"},"trusted":true},"outputs":[],"source":["model_constructors = {\n","  models.alexnet.__name__: models.alexnet, \n","  models.squeezenet1_1.__name__: models.squeezenet1_1,\n","  models.resnet50.__name__: models.resnet50, \n","  models.resnet101.__name__: models.resnet101,\n","  models.resnet152.__name__: models.resnet152, \n","  models.resnext101_32x8d.__name__: models.resnext101_32x8d, \n","  models.densenet201.__name__: models.densenet201, \n","  models.googlenet.__name__: models.googlenet, \n","  models.vgg16.__name__: models.vgg16, \n","  models.vgg19.__name__: models.vgg19, \n","  models.inception_v3.__name__: models.inception_v3, \n","}\n","\n","from torchvision.models import *\n","model_weights = {\n","  models.alexnet.__name__: AlexNet_Weights.DEFAULT,\n","  models.squeezenet1_1.__name__: SqueezeNet1_1_Weights.DEFAULT,\n","  models.resnet50.__name__: ResNet50_Weights.DEFAULT,\n","  models.resnet101.__name__: ResNet101_Weights.DEFAULT,\n","  models.resnet152.__name__: ResNet152_Weights.DEFAULT,\n","  models.resnext101_32x8d.__name__: ResNeXt101_32X8D_Weights.DEFAULT,\n","  models.densenet201.__name__: DenseNet201_Weights.DEFAULT,\n","  models.googlenet.__name__: GoogLeNet_Weights.DEFAULT,\n","  models.vgg16.__name__: VGG16_Weights.DEFAULT,\n","  models.vgg19.__name__: VGG19_Weights.DEFAULT,\n","  models.inception_v3.__name__: Inception_V3_Weights.DEFAULT,\n","}\n","\n","# Experiment around dropout & Learning Rate & different optimizer (Adam)\n","def init_model(name):\n","  if not path.exists(RESULT_DIR):\n","    os.mkdir(RESULT_DIR)\n","\n","  clean_vram()\n","  seed_everything()\n","  model = model_constructors[name](weight=model_weights[name])\n","  \n","  # fine-tune pretrain models to our usecase\n","  # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#initialize-and-reshape-the-networks\n","  NUM_CLASSES = len(class_names)\n","  DROPOUT = 0.5\n","  if name == models.alexnet.__name__ or name == models.vgg16.__name__ or name == models.vgg19.__name__:\n","    num_ftrs = model.classifier[6].in_features\n","    model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.classifier[6] = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","  elif name == models.densenet201.__name__:\n","    num_ftrs = model.classifier.in_features\n","    model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.classifier = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","  elif name == models.squeezenet1_1.__name__:\n","    model.classifier = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","    # model.classifier = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Conv2d(512, NUM_CLASSES, kernel_size=(1,1), stride=(1,1))\n","    # )\n","    model.num_classes = NUM_CLASSES\n","  elif name == models.inception_v3.__name__:\n","    auxLogits_num_ftrs = model.AuxLogits.fc.in_features\n","    model.AuxLogits.fc = nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","    # model.AuxLogits.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(auxLogits_num_ftrs, NUM_CLASSES)\n","    # )\n","    primary_num_ftrs = model.fc.in_features\n","    model.fc = nn.Linear(primary_num_ftrs, NUM_CLASSES)\n","    # model.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(primary_num_ftrs, NUM_CLASSES)\n","    # )\n","  else:\n","    # resnet, resnext & googlenet\n","    num_ftrs = model.fc.in_features\n","    model.fc= nn.Linear(num_ftrs, NUM_CLASSES)\n","    # model.fc = nn.Sequential(\n","    #   nn.Dropout(DROPOUT),\n","    #   nn.Linear(num_ftrs, NUM_CLASSES)\n","    # )\n","\n","  model = model.to(device)\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer= optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n","  exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","  if is_gpu_avail():\n","    # Use Automatic Mixed Precision as an attempt to solve CUDA out of memory and to speed things up\n","    # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n","    scaler = torch.cuda.amp.GradScaler()\n","  else:\n","    raise RuntimeError('This code only support machine with GPU.')\n","\n","  # print('=====================================')\n","  print(f'{name} is initialized')\n","  # print('=====================================')\n","  # print(model)\n","  return model, criterion, optimizer, scaler\n","\n","# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n","# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n","def save_model(perf_metrics, model, optimizer, scaler, history, model_path):\n","  torch.save({\n","    'perf_metrics': perf_metrics,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","    \"scaler_state_dict\": scaler.state_dict(),\n","    'history': history,\n","    }, model_path)\n","\n","def load_model(model, optimizer, scaler, model_path):\n","  if not os.path.exists(model_path):\n","    log_to_file(f\">>> WARN: {name_of_caller()}() model path '{model_path}' don't exist!\")\n","    return None, model, optimizer, scaler, None, None\n","  checkpoint = torch.load(model_path)\n","  perf_metrics = checkpoint['perf_metrics']\n","  model.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  scaler.load_state_dict(checkpoint['scaler_state_dict'])\n","  history = checkpoint['history']\n","  total_epoch = len(history) - 1\n","  del checkpoint\n","\n","  return perf_metrics, model, optimizer, scaler, history, total_epoch"]},{"cell_type":"markdown","metadata":{},"source":["### Define Function to Train Models"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:26.421026Z","iopub.status.busy":"2022-11-17T15:13:26.420649Z","iopub.status.idle":"2022-11-17T15:13:26.448404Z","shell.execute_reply":"2022-11-17T15:13:26.447170Z","shell.execute_reply.started":"2022-11-17T15:13:26.420992Z"},"trusted":true},"outputs":[],"source":["# training and validation loops\n","def train(model,\n","    criterion,\n","    optimizer,\n","    scaler,\n","    train_dataloader,\n","    valid_dataloader,\n","    model_path,\n","    max_epochs_stop=10,\n","    n_epochs=400,\n","    min_epoch=300,\n","    print_every=1):\n","    \n","    epochs_no_improve = 0\n","    perf = {\n","        'best_epoch': 0,\n","        'valid_loss_min': np.Inf,\n","        'valid_best_acc': 0,\n","    }\n","    total_epoch = 0\n","\n","    try:\n","        if os.path.exists(model_path):\n","            perf, model, optimizer, scaler, history, total_epoch = load_model(model, optimizer, scaler, model_path)\n","            log_to_file(f'Model has been trained for: {total_epoch} epochs.')\n","            log_to_file(f\"Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\\n\")\n","        else:\n","            history = []\n","            log_to_file(f'Starting Training from Scratch.\\n')\n","    except:\n","        history = []\n","        log_to_file(f'exception: start from scratch.\\n')\n","\n","    overall_start = time.time()\n","    if total_epoch >= n_epochs:\n","        log_to_file(f'Model has been fully trained. n_epochs specified is: {n_epochs} epochs.')\n","        history = pd.DataFrame(\n","            history,\n","            columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","        return model, history, perf\n","\n","    seed_everything()\n","\n","    # Main loop - continue training on where we left off if there's a saved model\n","    for epoch in range(total_epoch, n_epochs):\n","        # keep track of training and validation loss each epoch\n","        train_loss = 0.0\n","        valid_loss = 0.0\n","\n","        train_acc = 0\n","        valid_acc = 0\n","\n","        # Set to training\n","        model.train()\n","        start = time.time()\n","        for ii, (data, target) in enumerate (train_dataloader):\n","            data, target = data.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","\n","            # only for inception_v3 - https://discuss.pytorch.org/t/why-auxiliary-logits-set-to-false-in-train-mode/40705/15\n","            with torch.cuda.amp.autocast():\n","              # output, aux_output = model(data)\n","              # loss1 = criterion(output, target)\n","              # loss2 = criterion(aux_output, target)\n","              # loss = loss1 + 0.4*loss2\n","              output = model(data)\n","              loss = criterion(output, target)\n","            # loss.backward()\n","            # optimizer.step()\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * data.size(0)\n","            _, pred = torch.max(output, dim=1)\n","            correct_tensor = pred.eq(target.data.view_as(pred))\n","            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n","            train_acc += accuracy.item() * data.size(0)\n","            print(\n","                f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_dataloader):.2f}% complete. {time.time() - start:.2f} seconds elapsed in epoch.', end=\"\\r\")\n","            \n","            # cleanup to save VRAM\n","            del data, target\n","#             clean_vram()\n","\n","        # After training loops ends, start validation\n","        else:\n","            with torch.no_grad():\n","                model.eval()\n","                for data, target in valid_dataloader:\n","                    if is_gpu_avail():\n","                        data, target = data.cuda(), target.cuda()\n","                    output = model(data)\n","                    loss = criterion(output, target)\n","                    valid_loss += loss.item() * data.size(0)\n","                    _, pred = torch.max(output, dim=1)\n","                    correct_tensor = pred.eq(target.data.view_as(pred))\n","                    accuracy = torch.mean(\n","                        correct_tensor.type(torch.FloatTensor))\n","                    valid_acc += accuracy.item() * data.size(0)\n","                    \n","                    # cleanup to save VRAM\n","                    del data, target\n","#                     clean_vram()\n","                train_loss = train_loss / train_data_size\n","                valid_loss = valid_loss / valid_data_size\n","                train_acc = train_acc / train_data_size\n","                valid_acc = valid_acc / valid_data_size\n","                history.append([train_loss, valid_loss,train_acc, valid_acc])\n","                if (epoch + 1) % print_every == 0:\n","                    log_to_file(f'Epoch: {epoch}', True)\n","                    log_to_file(\n","                        f'Training Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}',\n","                        True\n","                    )\n","                    log_to_file(\n","                        f'Training Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}% \\n',\n","                        True\n","                    )\n","          \n","                if valid_loss < perf['valid_loss_min']:\n","                    epochs_no_improve = 0\n","                    perf['best_epoch'] = epoch\n","                    perf['valid_loss_min'] = valid_loss\n","                    perf['valid_best_acc'] = valid_acc\n","                    save_model(perf, model, optimizer, scaler, history, model_path)\n","                else:\n","                    epochs_no_improve += 1\n","                    # Trigger early stopping\n","                    if epoch > min_epoch and epochs_no_improve >= max_epochs_stop:\n","                        log_to_file(\n","                            f\"\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.2f}%\"\n","                        )\n","                        total_time = time.time() - overall_start\n","                        log_to_file(\n","                            f'{total_time:.4f} total seconds elapsed. {total_time / (epoch+1):.4f} seconds per epoch.'\n","                        )\n","                        log_to_file()\n","\n","                        # Load the best state from saved model\n","                        _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","                        # save the full history\n","                        save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","                        # Format history\n","                        history = pd.DataFrame(\n","                            history,\n","                            columns=[\n","                                'train_loss', 'valid_loss', 'train_acc',\n","                                'valid_acc'\n","                            ])\n","                        return model, history, perf\n","    \n","    total_time = time.time() - overall_start\n","    log_to_file(\n","        f\"\\nBest epoch: {perf['best_epoch']} with loss: {perf['valid_loss_min']:.4f} and acc: {100 * perf['valid_best_acc']:.4f}%\"\n","    )\n","    log_to_file(\n","        f\"{total_time:.4f} total seconds elapsed. {total_time / (perf['best_epoch']+1):.4f} seconds per epoch.\"\n","    )\n","    log_to_file()\n","\n","    # Load the best state from saved model\n","    _, model, optimizer, scaler, _, _ = load_model(model, optimizer, scaler, model_path)\n","    # save the full history\n","    save_model(perf, model, optimizer, scaler, history, model_path)\n","\n","    # Format history\n","    history = pd.DataFrame(\n","        history,\n","        columns=['train_loss', 'valid_loss','train_acc', 'valid_acc'])\n","    \n","    return model, history, perf\n","\n","\n","def save_train_val_loss_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_loss', 'valid_loss']:\n","      plt.plot(\n","          history[c], label=c)\n","\n","  title = f'{curr_model} - Training and Validation Losses'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Losses')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def save_train_val_acc_graph(history, perf):\n","  plt.figure(figsize=(8, 6))\n","  for c in ['train_acc', 'valid_acc']:\n","      plt.plot(\n","          100 * history[c], label=c)\n","      \n","  title = f'{curr_model} - Training and Validation Accuracy'\n","  full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","  plt.xlabel('Epochs')\n","  plt.ylabel('Average Accuracy')\n","  plt.title(title)\n","  plt.axvline(x=perf['best_epoch'], color='r', label='best_epoch')\n","  plt.legend()\n","  plt.savefig(full_path, bbox_inches='tight')"]},{"cell_type":"markdown","metadata":{},"source":["### Define Functions to Visualize Prediction"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:30.921413Z","iopub.status.busy":"2022-11-17T15:13:30.921028Z","iopub.status.idle":"2022-11-17T15:13:30.939941Z","shell.execute_reply":"2022-11-17T15:13:30.939041Z","shell.execute_reply.started":"2022-11-17T15:13:30.921378Z"},"trusted":true},"outputs":[],"source":["# confusion matrix \n","def getConfusionMatrix(model, dataloader, is_test=False, show_image=False, print_to_console_only=False):\n","    model.eval()\n","    confusion_matrix=np.zeros((2,2),dtype=int)\n","    num_images=test_data_size\n","    \n","    with torch.no_grad():\n","        for i, (data,target) in enumerate(dataloader):\n","            data = data.to(device)\n","            target = target.to(device)\n","            \n","            output = model(data) \n","            _, pred = torch.max(output, 1)\n","            \n","            for j in range(data.size()[0]): \n","                if pred[j]==1 and target[j]==1:\n","                    term='TP'\n","                    confusion_matrix[0][0]+=1\n","                elif pred[j]==1 and target[j]==0:\n","                    term='FP'\n","                    confusion_matrix[1][0]+=1\n","                elif pred[j]==0 and target[j]==1:\n","                    term='FN'\n","                    confusion_matrix[0][1]+=1\n","                elif pred[j]==0 and target[j]==0:\n","                    term='TN'\n","                    confusion_matrix[1][1]+=1\n","            \n","                if show_image:\n","                    log_to_file(f'predicted: {class_names[pred[j]]}', print_to_console_only)\n","                    log_to_file(term, print_to_console_only)\n","                    imshow(data.cpu().data[j])\n","        \n","        log_to_file(None, print_to_console_only)\n","        category = 'Test' if is_test else 'Validation'\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file(f'{category} Results ', print_to_console_only)\n","        log_to_file('=====================', print_to_console_only)\n","        log_to_file('Confusion Matrix: ', print_to_console_only)\n","        log_to_file(np.array2string(confusion_matrix), print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","\n","        log_to_file(f'Sensitivity: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Specificity: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'PPV: {100*confusion_matrix[0][0]/(confusion_matrix[0][0]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'NPV: {100*confusion_matrix[1][1]/(confusion_matrix[1][1]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(f'Accuracy: {100*(confusion_matrix[0][0]+confusion_matrix[1][1])/(confusion_matrix[0][0]+confusion_matrix[0][1]+confusion_matrix[1][1]+confusion_matrix[1][0])}', print_to_console_only)\n","        log_to_file(f'F1-Score: {(2*confusion_matrix[0][0])/(2*confusion_matrix[0][0]+confusion_matrix[1][0]+confusion_matrix[0][1])}', print_to_console_only)\n","        log_to_file(None, print_to_console_only)\n","    return confusion_matrix\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def save_test_acc_n_loss_graph(model, dataloader, criterion):\n","  pass\n","  # NOT NEEDED YET\n","  # with torch.no_grad():\n","  #   model.eval()\n","  #   for data, target in dataloader:\n","  #       if is_gpu_avail():\n","  #           data, target = data.cuda(), target.cuda()\n","  #       output = model(data)\n","  #       loss = criterion(output, target)\n","  #       test_loss += loss.item() * data.size(0)\n","  #       _, pred = torch.max(output, dim=1)\n","  #       correct_tensor = pred.eq(target.data.view_as(pred))\n","  #       accuracy = torch.mean(\n","  #           correct_tensor.type(torch.FloatTensor))\n","  #       test_acc += accuracy.item() * data.size(0)\n","  #   train_loss = train_loss / train_data_size\n","  #   test_loss = test_loss / test_data_size\n","  #   train_acc = train_acc / train_data_size\n","  #   test_acc = test_acc / test_data_size\n","\n","\n","# def visualize_test_prediction(model):\n","#   covid_test_img_dir = '/content/drive/My Drive/data/test/covid/'\n","#   img_list = [Image.open(os.path.join(pth, f)).convert('RGB')\n","#       for pth, dirs, files in os.walk(covid_test_img_dir) for f in files]\n","\n","#   # test_img_paths = ['/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%3.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%4.png',\n","#   #                         '/content/drive/My Drive/data/test/covid/2020.02.22.20024927-p19-68%5.png']\n","#   # img_list = [Image.open( img_path) for img_path in test_img_paths]\n","\n","#   # log_to_file(img_list)\n","\n","#   test_batch = torch.stack([image_transforms['test'](img).to(device)\n","#                               for img in img_list])\n","#   pred_logits_tensor = model(test_batch)\n","#   pred_probs = F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n","\n","#   row = 12\n","#   col = 3\n","#   fig, axs = plt.subplots(row, col, figsize=(20, 50))\n","#   r = 0\n","#   c = 0\n","#   for i, img in enumerate(img_list):\n","#       if c >= col:\n","#         r += 1\n","#         c = 0\n","#       ax = axs[r, c]\n","#       ax.axis('off')\n","#       ax.set_title(\"{:.4f}% Covid, {:.4f}% NonCovid\".format(100*pred_probs[i,0],\n","#                                                               100*pred_probs[i,1]))\n","#       ax.imshow(img)\n","#       c +=1\n","\n","#   title = f'{curr_model} - Covid Image Prediction'\n","#   full_path = os.path.join(RESULT_DIR, f'{title}.png')\n","#   plt.savefig(full_path, bbox_inches='tight')\n","\n","\n","def getPredProbs(model, datasetStr, count, isSeeded=True):\n","  if isSeeded:\n","    seed_everything()\n","  \n","  dataset = data[datasetStr].samples\n","  img_list = []\n","  for i, (img_path, cls_idx) in enumerate(dataset):\n","    if i >= count:\n","      break\n","    img_list.append(Image.open(img_path).convert('RGB'))\n","\n","  test_batch = torch.stack([image_transforms[datasetStr](img).to(device)\n","                              for img in img_list])\n","  pred_logits_tensor = model(test_batch)\n","  return F.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()"]},{"cell_type":"markdown","metadata":{},"source":["### Run all models - Init Models + Training"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-11-17T15:13:34.458786Z","iopub.status.busy":"2022-11-17T15:13:34.458420Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: PYTHONHASHSEED=18\n"]},{"name":"stderr","output_type":"stream","text":["c:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  warnings.warn(\n","c:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["resnet152 is initialized\n","exception: start from scratch.\n","\n","\n","env: PYTHONHASHSEED=18\n","Epoch: 0\t100.00% complete. 1364.18 seconds elapsed in epoch.\n","Best epoch: 0 with loss: inf and acc: 0.0000%\n","\n","1418.1037 total seconds elapsed. 1418.1037 seconds per epoch.\n","\n","\n","\n"]},{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc.1.weight\", \"fc.1.bias\". ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn [12], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m   model, criterion, optimizer, scaler \u001b[39m=\u001b[39m init_model(curr_model)\n\u001b[0;32m     27\u001b[0m \u001b[39m#   Training & Validation\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m   model, history, perf \u001b[39m=\u001b[39m train(\n\u001b[0;32m     29\u001b[0m       model,\n\u001b[0;32m     30\u001b[0m       criterion,\n\u001b[0;32m     31\u001b[0m       optimizer,\n\u001b[0;32m     32\u001b[0m       scaler,\n\u001b[0;32m     33\u001b[0m       train_loader,\n\u001b[0;32m     34\u001b[0m       val_loader,\n\u001b[0;32m     35\u001b[0m       model_path\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mpath\u001b[39m.\u001b[39;49mjoin(RESULT_DIR, curr_model)\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     36\u001b[0m       max_epochs_stop\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,  \u001b[39m# Early stopping intialization\u001b[39;49;00m\n\u001b[0;32m     37\u001b[0m       n_epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     38\u001b[0m       min_epoch\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m     39\u001b[0m       print_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m   history\n\u001b[0;32m     42\u001b[0m   save_train_val_loss_graph(history, perf)\n","Cell \u001b[1;32mIn [10], line 164\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, scaler, train_dataloader, valid_dataloader, model_path, max_epochs_stop, n_epochs, min_epoch, print_every)\u001b[0m\n\u001b[0;32m    161\u001b[0m log_to_file()\n\u001b[0;32m    163\u001b[0m \u001b[39m# Load the best state from saved model\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m _, model, optimizer, scaler, _, _ \u001b[39m=\u001b[39m load_model(model, optimizer, scaler, model_path)\n\u001b[0;32m    165\u001b[0m \u001b[39m# save the full history\u001b[39;00m\n\u001b[0;32m    166\u001b[0m save_model(perf, model, optimizer, scaler, history, model_path)\n","Cell \u001b[1;32mIn [9], line 105\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model, optimizer, scaler, model_path)\u001b[0m\n\u001b[0;32m    103\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(model_path)\n\u001b[0;32m    104\u001b[0m perf_metrics \u001b[39m=\u001b[39m checkpoint[\u001b[39m'\u001b[39m\u001b[39mperf_metrics\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> 105\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    106\u001b[0m optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39moptimizer_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    107\u001b[0m scaler\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mscaler_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n","File \u001b[1;32mc:\\tools\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"fc.1.weight\", \"fc.1.bias\". "]}],"source":["model_list = [\n","    models.alexnet.__name__, # 0\n","    models.squeezenet1_1.__name__, #1\n","    models.resnet50.__name__, # 2\n","    models.resnet101.__name__, # 3\n","    models.resnet152.__name__, # 4\n","    models.resnext101_32x8d.__name__, # 5\n","    models.densenet201.__name__, # 6\n","    models.googlenet.__name__, # 7\n","    models.vgg16.__name__, # 8\n","    models.vgg19.__name__, #9\n","    models.inception_v3.__name__, #10\n","]\n","\n","for i in range(0,11):\n","  # https://github.com/pytorch/pytorch/issues/50198\n","  # skipped these because cannot use deterministic algorithm\n","#   skip_model = [0, 1, 5, 8, 9, 10]\n","  skip_model = [0, 1, 2, 3, 5, 6, 7, 8, 9, 10]\n","  if i in skip_model:\n","    continue\n","  curr_model = model_list[i]\n","\n","  # Initialize model, criterion and optimizer\n","  model, criterion, optimizer, scaler = init_model(curr_model)\n","\n","#   Training & Validation\n","  model, history, perf = train(\n","      model,\n","      criterion,\n","      optimizer,\n","      scaler,\n","      train_loader,\n","      val_loader,\n","      model_path=f'{path.join(RESULT_DIR, curr_model)}.pt',\n","      max_epochs_stop=5,  # Early stopping intialization\n","      n_epochs=1,\n","      min_epoch=1,\n","      print_every=10)\n","\n","  history\n","  save_train_val_loss_graph(history, perf)\n","  save_train_val_acc_graph(history, perf)\n","  getConfusionMatrix(model, val_loader)"]},{"cell_type":"markdown","metadata":{},"source":["## Out of memory issue\n","\n","- References\n","    - https://discuss.pytorch.org/t/using-main-ram-instead-of-vram/59344/3 \n","    - https://duckduckgo.com/?q=pytorch+colab+use+system+ram+instead+of+gpu+ram&ia=web\n","    - https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch\n","        - [CUDA Out of Memory discussion in kaggle forum](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/91081)\n","    - https://pytorch.org/docs/stable/notes/cuda.html#memory-management\n","    - [trick to debug tensor memory](https://forum.pyro.ai/t/a-trick-to-debug-tensor-memory/556)\n","- The fix\n","    - Delete unused tensor, force garbage collection and run `empty_cache()`\n","    - Set PYTORCH_CUDA_ALLOC_CONF to `max_split_size_mb:512`. This prevents the allocator to split block large than 512MB"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmemory_stats(device)\n","\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"]}],"source":["torch.cuda.memory_stats(device)\n","# print(torch.cuda.memory_summary(device))"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('pytorch')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"36aecb82383d45a5d90cd16d2a70e6def28d8cf6813a9e24d3e39991fa43b598"}}},"nbformat":4,"nbformat_minor":4}
