# -*- coding: utf-8 -*-
"""custom-model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQ7kAtQLg2IpFtWZbvquy-T43FHH405M

---
# Summary
---
> Expand to see summary and details

## Overview and Explanation

1. This notebook reuses a lot of the [original transfer learning notebook](https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=QgZD08Q-YXXH)
    - Here the focus is on building the new custom model using the CovidNet-CT database.
2. The [`Setup Kaggle`](#scrollTo=wMQLloEgzPol) section:
    - is where the dataset is being acquired.
    - Explanation of various phases in the [CovidNet-CT ML code](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L415):
        - train phase is train phase
        - test phase is validation phase
        - infer phase is test phase
3. The [`Data Preprocessing`](#scrollTo=JjsNA--kG9CV) section:
    - refers to the way [CovidNet-CT preprocess its data](https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72)
    - CovidNet-CT uses TensorFlow while this notebook adapts the code to use PyTorch
    - Two highlights
        - input shape is (512, 512, 3) instead of the (224, 224, 3) used by the imagenet model
        - the image is cropped to the bounding box provided with the dataset before resize to 512x512
4. The [`Training & Validation`](#scrollTo=YqGCBwYdasI_) section:
    - refers to [how CovidNet-CT trains](https://github.com/haydengunraj/COVIDNet-CT/blob/02009821b9d063d01994cb70e61b8def0af275ab/run_covidnet_ct.py#L174)
    - This part is almost identical to the original transfer learning model notebook.

---
# Setup Kaggle data access to download dataset
---

- [dataset source from kaggle](https://www.kaggle.com/datasets/hgunraj/covidxct)
- On 02-06-2022, CT-3A datasets with 425,024 CT images are uploaded.
    - The way of how CT-3A and CT-3B are constructured can be found in [this link](https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md).
    - This notebook only uses CT-3A that is already constructed and ready for download from the kaggle source.

## Import and deterministic operations
All modules will be imported here except for modules used in [Playground](#scrollTo=_0hFKkjaLNlV) section
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function, division
import random
import numpy as np
import torch

import os
from os import path
from google.colab import files

import zipfile

from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from PIL import Image
from sklearn.model_selection import train_test_split #, StratifiedShuffleSplit

import torchvision
from torchvision import models, transforms #, datasets
import matplotlib.pyplot as plt

import torch.nn as nn

# ensure reproducibility across different executions
# https://pytorch.org/docs/stable/notes/randomness.html
# https://www.kaggle.com/mlwhiz/third-place-model-for-toxic-comments-in-pytorch
# https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html
SEED = 18
def seed_everything(seed=18):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
#torch.set_deterministic(True)
torch.use_deterministic_algorithms(True)
# %env CUBLAS_WORKSPACE_CONFIG=:4096:8

"""## Upload kaggle.json"""

CURR_DIR = os.getcwd()

# kaggle_json = path.join(CURR_DIR, "kaggle.json")
# if not path.exists(kaggle_json):
#     uploaded = files.upload()

# os.system(f"chmod 600 {kaggle_json}")

"""## Use Kaggle API to download dataset

### Solving google drive not enough capacity to store data
- [Article explaining how to access Kaggle Data in colab](https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7)
- [An old reddit post that seems to be relevant](https://old.reddit.com/r/PiratedGames/comments/uypcw2/use_google_colab_to_mass_transfer_files_from/)
- Some rclone idea
    - [rclone gui](https://rclone.org/gui/)
    - [rclone lab](https://github.com/acamposxp/RcloneLab)
    - [rclone idea 1](https://github.com/ella-tj/Any-File-2-GDrive)
    - [rclone idea 2](https://github.com/eaustin6/Rclone-Setup-on-Google-Colab)
    - [rclone idea 2](https://towardsdatascience.com/why-you-should-try-rclone-with-google-drive-and-colab-753f3ec04ba1)
    - [reddit colab thread](https://old.reddit.com/r/DataHoarder/comments/g069dx/google_colab_is_like_running_code_from_your/)
    - [socialgrep on rclone and colab](https://socialgrep.com/search?query=rclone%2Ccolab)
"""

# os.environ["KAGGLE_CONFIG_DIR"] = CURR_DIR
dataset_zip = path.join(CURR_DIR, "covidxct.zip")
dataset_dir = path.join(CURR_DIR, "data")

if not path.exists(dataset_dir) and not path.exists(dataset_zip):
    os.system("kaggle datasets download -d hgunraj/covidxct")

if not path.exists(dataset_dir):
    os.mkdir(dataset_dir)
    with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:
        zip_ref.extractall(dataset_dir)
if path.exists(dataset_zip):
    os.remove(dataset_zip)

"""---
# Data Preprocessing
---
- [how torch dataset is loaded](https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L45)
- [example custom model and custom dataset](https://github.com/ArnaudMallet/Plant_Patho/blob/master/Plant_Patho_4.ipynb)
    - [pytorch thread](https://discuss.pytorch.org/t/how-to-load-data-from-a-csv/58315/10) that mentioned this example
- [A well explained custom dataset](https://github.com/utkuozbulak/pytorch-custom-dataset-examples)

## Custom Dataset class to load CovidNet data

Various references used:
- https://colab.research.google.com/drive/187Z04CNQBVV3jmCdA2sSbMTs9BB40qh_#scrollTo=H9doKmx1TXK1
- https://drive.google.com/drive/folders/13PnDpSYUaVaKHjXjUK6bwWvJddDfbRad
- https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L72
- https://github.com/haydengunraj/COVIDNet-CT/blob/master/docs/dataset.md
- https://www.kaggle.com/datasets/hgunraj/covidxct?select=metadata.csv
- https://stackoverflow.com/questions/50544730/how-do-i-split-a-custom-dataset-into-training-and-test-datasets/50544887#50544887
- https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py
- https://github.com/pytorch/vision/blob/d4a03fc02d0566ec97341046de58160370a35bd2/torchvision/datasets/vision.py#L10
"""

class CovidNetDataset(Dataset):
    def __init__(self, img_dir, split_files, transform = None):
        # don't seem to need the csv file
        # self.df = pd.read_csv(csv_path)
        # _, self.class_to_idx  = self.find_classes(csv_path);

        self.img_dir = img_dir
        self.split_files = split_files

        self.imgs, self.targets, self.bboxes = self.get_all_split_file_data()
        # self.imgs = [entry.name for entry in os.scandir(img_dir) if entry.is_file()]
        self.transform = transform

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, index):
        # filename = self.df[index, "FILENAME"]
        # label = self.class_to_idx [self.df[index, "LABEL"]]
        # image = Image.open(os.path.join(self.img_dir, filename))

        label = self.targets[index]
        with open(self.imgs[index], "rb") as f:
            image = Image.open(f)
            image = image.crop(self.bboxes[index])
            image = image.copy()

        if self.transform is not None:
            image = self.transform(image)


        return image, label

    # from https://github.com/pytorch/vision/blob/main/torchvision/datasets/folder.py#L36
    def find_classes(self, csv_path=None):
        """Returns class name array and class_to_idx.
        See :class:`CovidNetDataset` for details.
        """
        # class_col = "finding"
        # classes = sorted(self.df[class_col].unique())
        # if not classes:
        #     raise FileNotFoundError(f"Couldn't find any class from '{class_col}' column in {csv_path}.")

        # hard code classes as the order are not alphabetic
        classes = ['Normal', 'Pneumonia', 'COVID-19']

        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}
        return classes, class_to_idx

    def get_all_split_file_data(self):
        files, classes, bboxes = [], [], []
        for split_file in self.split_files:
            f, cls, bb = self.get_data_from_split_file(split_file)
            files.extend(f)
            classes.extend(cls)
            bboxes.extend(bb)
        return files, classes, bboxes

    # from https://github.com/haydengunraj/COVIDNet-CT/blob/d1c6be5202a78d5f8802e40ff6b5b9d57189c797/dataset.py#L108
    def get_data_from_split_file(self, split_file):
        """Gets image filenames, classes and bboxes"""
        files, classes, bboxes = [], [], []
        with open(split_file, 'r') as f:
            for line in f.readlines():
                fname, cls, xmin, ymin, xmax, ymax = line.strip('\n').split()
                files.append(path.join(self.img_dir, fname))
                classes.append(int(cls))
                bboxes.append([int(xmin), int(ymin), int(xmax), int(ymax)])
        return files, classes, bboxes

"""## Spliting dataset into train, val, test

- [SO QA on spliting using sklearn](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)
    - [Train test split example](https://stackoverflow.com/questions/38250710/how-to-split-data-into-3-sets-train-validation-and-test)
    - [train test split example with indices](https://stackoverflow.com/questions/31521170/scikit-learn-train-test-split-with-indices)
- [Pytorch stratified split example](https://discuss.pytorch.org/t/how-to-do-a-stratified-split/62290)
- [sklearn StratifiedShuffleSplit doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html)
- [StratifiedShuffleSplit example](https://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn)
- [another StratifiedShuffleSplit example](https://stackoverflow.com/questions/40829137/stratified-train-validation-test-split-in-scikit-learn)
"""

# Specify all the filepath of the dataset
DATA_DIR = path.join(CURR_DIR, "data")
dirs = [entry.name for entry in os.scandir(DATA_DIR) if entry.is_dir()]
assert len(dirs) == 1 # expects to only have 1 folder that contains all the images

DATASET_DIR = path.join(DATA_DIR, dirs[0])
METADATA_CSV = path.join(DATA_DIR, "metadata.csv")

DATASET_NAME = "COVIDx_CT-3A"
TRAIN_SPLIT_FILE = path.join(DATA_DIR, f"train_{DATASET_NAME}.txt")
VAL_SPLIT_FILE = path.join(DATA_DIR, f"val_{DATASET_NAME}.txt")
TEST_SPLIT_FILE = path.join(DATA_DIR, f"test_{DATASET_NAME}.txt")
SPLIT_FILES = [TRAIN_SPLIT_FILE, VAL_SPLIT_FILE, TEST_SPLIT_FILE]

full_dataset = CovidNetDataset(DATASET_DIR, SPLIT_FILES)
full_data_len = len(full_dataset)
print(f"Length of full dataset: {full_data_len}")

# # Defines ratios, w.r.t. whole dataset.
ratio_train = 0.8
ratio_val = 0.1
ratio_test = 0.1
dummy_X = np.zeros(full_data_len)
indexes = np.arange(full_data_len)

# Produces test split. Uses train_test_split instead of StratifiedShuffleSplit to get x_remaining & y_remaining
# to be used in the next step.
# Note that an additional indexes array is provided
x_remaining, _, y_remaining, _, temp_train_index, test_index = train_test_split(
    dummy_X, full_dataset.targets, indexes, test_size=ratio_test, stratify=full_dataset.targets, random_state=SEED)

# Adjusts val ratio, w.r.t. remaining dataset.
ratio_remaining = 1 - ratio_test
ratio_val_adjusted = ratio_val / ratio_remaining

# Produces train and val splits.
_, _, _, _, train_index, val_index = train_test_split(
    x_remaining, y_remaining, temp_train_index, test_size=ratio_val_adjusted, stratify=y_remaining, random_state=SEED)

print(f"First 10 train_index: {train_index[:10]}")
print(f"length of train_index: {len(train_index)}\n")
print(f"First 10 val_index: {val_index[:10]}")
print(f"length of val_index: {len(val_index)}\n")
print(f"First 10 test_index: {test_index[:10]}")
print(f"length of test_index: {len(test_index)}\n")

"""## Applying transforms to dataset

### Define a wrapper dataset
This is to have the flexibility of applying different transforms to each of the splitted dataset
- References
    - [wrapper dataset source](https://stackoverflow.com/questions/57539567/augmenting-only-the-training-set-in-k-folds-cross-validation/57539790#57539790)
    - [pytorch dataset lazy loading idea](https://discuss.pytorch.org/t/split-dataset-into-training-and-validation-without-applying-training-transform/115429/3)
    - [individual transform using torchdata](https://stackoverflow.com/questions/61811946/train-valid-test-split-for-custom-dataset-using-pytorch-and-torchvision)
"""

class WrapperDataset:
    def __init__(self, dataset, transform=None, target_transform=None):
        self.dataset = dataset
        self.transform = transform
        self.target_transform = target_transform

    def __getitem__(self, index):
        image, label = self.dataset[index]
        if self.transform is not None:
            image = self.transform(image)
        if self.target_transform is not None:
            label = self.target_transform(label)
        return image, label

    def __len__(self):
        return len(self.dataset)

"""### Defining the transforms
References for mean and std of images
- [pytorch forum thread](https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/27?u=kharshit)
- [how the mean and std of imagenet transform being calculated](https://stackoverflow.com/questions/57532661/how-do-they-know-mean-and-std-the-input-value-of-transforms-normalize?noredirect=1&lq=1)
    - [another similar SO question](https://stackoverflow.com/questions/58151507/why-pytorch-officially-use-mean-0-485-0-456-0-406-and-std-0-229-0-224-0-2)
- [grayscale vs RGB images in ML training](https://towardsdatascience.com/transfer-learning-on-greyscale-images-how-to-fine-tune-pretrained-models-on-black-and-white-9a5150755c7a)
- Bounding box causing issue when batching as stacking don't work with different size
    - [easiest solution is to use tuple as the parameter](https://discuss.pytorch.org/t/runtimeerror-stack-expects-each-tensor-to-be-equal-size-but-got-3-224-224-at-entry-0-and-3-224-336-at-entry-3/87211/10) when calling `transform.resize()`
    - [another solution is to override `collate_fn()`](https://discuss.pytorch.org/t/dataloader-gives-stack-expects-each-tensor-to-be-equal-size-due-to-different-image-has-different-objects-number/91941) when contructing `Dataloader`
"""

covidnet_std_transform = transforms.Compose([
    transforms.Grayscale(num_output_channels=3),
    transforms.Resize((512, 512)), # this is important or else batching will have error due to bbox
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # POTENTIAL_FINE_TUNE
])

covidnet_train_transform = transforms.Compose([
    transforms.RandomChoice(transforms=[
        transforms.RandomHorizontalFlip(),
        transforms.RandomVerticalFlip(),
        transforms.RandomRotation(10),
        transforms.ColorJitter(brightness=.3, hue=.3),
        transforms.RandomPerspective(distortion_scale=0.4),
        transforms.RandomAffine(degrees=(0, 0), translate=(0.05, 0.1), scale=(0.85, 0.95))])
    ])

image_transforms = {
    'train': transforms.Compose([
        covidnet_train_transform,
        covidnet_std_transform
    ]),
    'val': transforms.Compose([
        covidnet_std_transform
    ]),
    'test': transforms.Compose([
        covidnet_std_transform
    ]),
    'playground': transforms.Compose([
        covidnet_train_transform,
        covidnet_std_transform
    ])
}

"""### Creating the Dataset Loader"""

seed_everything(SEED)
BATCH_SIZE = 64

train_sampler = SubsetRandomSampler(train_index)
val_sampler = SubsetRandomSampler(val_index)
test_sampler = SubsetRandomSampler(test_index)

train_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['train']), batch_size=BATCH_SIZE, sampler=train_sampler)
val_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['val']), batch_size=BATCH_SIZE, sampler=val_sampler)
test_loader = DataLoader(WrapperDataset(full_dataset, image_transforms['test']), batch_size=BATCH_SIZE, sampler=test_sampler)
